[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advancing International Business Research through Artificial Intelligence and Machine Learning Applications",
    "section": "",
    "text": "This is the online appendix to Advancing International Business Research through Artificial Intelligence and Machine Learning Applications.\nHere, we provide code snippets to demonstrate textual analysis techniques on abstracts from the Journal of World Business downloaded from Scopus.",
    "crumbs": [
      "Home",
      "Advancing International Business Research through Artificial Intelligence and Machine Learning Applications"
    ]
  },
  {
    "objectID": "models/BERTsentiment.html",
    "href": "models/BERTsentiment.html",
    "title": "Machine learning perspective paper",
    "section": "",
    "text": "import pandas as pd\ndata = pd.read_csv('../data/jwb-articles.csv')\ndata = data[data['Abstract'].notna()] # Keep nonempty abstracts\ndata.head()\n\n\n\n\n\n\n\n\nAuthors\nAuthor full names\nAuthor(s) ID\nTitle\nYear\nSource title\nVolume\nIssue\nArt. No.\nPage start\n...\nISSN\nISBN\nCODEN\nPubMed ID\nLanguage of Original Document\nDocument Type\nPublication Stage\nOpen Access\nSource\nEID\n\n\n\n\n0\nAl Asady, A.; Anokhin, S.\nAl Asady, Ahmad (57219984746); Anokhin, Sergey...\n57219984746; 24482882200\nThe Trojan horse of international entrepreneur...\n2025\nJournal of World Business\n60\n6\n101677.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nNaN\nScopus\n2-s2.0-105014957115\n\n\n1\nThams, Y.; Dau, L.A.; Doh, J.; Kostova, T.; Ne...\nThams, Yannick (55357149800); Dau, Luis Alfons...\n55357149800; 35147597100; 7003920280; 66037741...\nPolitical ideology and the multinational enter...\n2025\nJournal of World Business\n60\n6\n101678.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nNaN\nScopus\n2-s2.0-105014844629\n\n\n2\nLindner, T.; Puck, J.; Puhr, H.\nLindner, Thomas (57159151000); Puck, Jonas (85...\n57159151000; 8563161700; 57223389639\nArtificial intelligence in international busin...\n2025\nJournal of World Business\n60\n6\n101676.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105014595041\n\n\n3\nBruton, G.D.; Mejía-Morelos, J.H.; Ahlstrom, D.\nBruton, Garry D. (6603867202); Mejía-Morelos, ...\n6603867202; 55748855800; 56525447800\nMultinational corporations and inclusive suppl...\n2025\nJournal of World Business\n60\n6\n101663.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013512235\n\n\n4\nLiang, Y.; Giroud, A.; Rygh, A.; Chen, Z.\nLiang, Yanze (57223851564); Giroud, Axèle L.A....\n57223851564; 7003496253; 37117826800; 58631386600\nPolitical embeddedness and post-acquisition in...\n2025\nJournal of World Business\n60\n6\n101665.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013485759\n\n\n\n\n5 rows × 41 columns\n\n\n\n\nfrom datasets import Dataset\n# Only retain the first 512 terms\ndata['Abstract_200'] = data['Abstract'].apply(lambda x: ' '.join(x.split(' ')[:200]))\n\n# Convert the Pandas DataFrame to a Hugging Face Dataset\ndataset = Dataset.from_pandas(data)\n\n\nfrom transformers import pipeline\n# Create the sentiment analysis pipeline\nnlp = pipeline('sentiment-analysis', model='hw2942/bert-base-chinese-finetuning-financial-news-sentiment-v2')\n\ndef get_sentiment(examples):\n    # Initialize lists to store results\n    sentiments = []\n    scores = []\n    final_sentiment_scores = []\n\n    # Process each entry in the batch\n    for text in examples['Abstract_200']:\n        if isinstance(text, str):\n            try:\n                result = nlp(text)\n                sentiment = result[0]['label']\n                score = result[0]['score']\n                sentiment_label = 1 if sentiment == 'Positive' else -1 if sentiment == 'Negative' else 0\n                final_sentiment_score = sentiment_label * score\n\n                sentiments.append(sentiment)\n                scores.append(score)\n                final_sentiment_scores.append(final_sentiment_score)\n            except Exception as e:\n                print(f'Error processing text: {text}. Error: {e}', flush=True)\n                # Append default values in case of an error\n                sentiments.append(None)\n                scores.append(None)\n                final_sentiment_scores.append(None)\n        else:\n            print(f'Non-string entry found: {text}', flush=True)\n            # Append default values for non-string entries\n            sentiments.append(None)\n            scores.append(None)\n            final_sentiment_scores.append(None)\n\n    # Ensure the output lists are of the same length as the batch size\n    batch_size = len(examples['Abstract_200'])\n    while len(sentiments) &lt; batch_size:\n        sentiments.append(None)\n        scores.append(None)\n        final_sentiment_scores.append(None)\n\n    return {'sentiment': sentiments, 'score': scores, 'final_sentiment_score': final_sentiment_scores}\n\ndataset = dataset.map(get_sentiment, batched=True, batch_size=64)\n\nDevice set to use mps:0\n\n\n\n\n\n\nimport pickle \n\nwith open('BERTsentiment.pkl', 'wb') as f:\n    pickle.dump(dataset, f)"
  },
  {
    "objectID": "methods/01-topic-modeling.html",
    "href": "methods/01-topic-modeling.html",
    "title": "Topic modeling",
    "section": "",
    "text": "This is a snapshot of the data we will be working with.\n\nimport pandas as pd\ndata = pd.read_csv('../data/jwb-articles.csv')\ndata = data[data['Abstract'].notna()] # Keep nonempty abstracts\ndata.head()\n\n\n\n\n\n\n\n\nAuthors\nAuthor full names\nAuthor(s) ID\nTitle\nYear\nSource title\nVolume\nIssue\nArt. No.\nPage start\n...\nISSN\nISBN\nCODEN\nPubMed ID\nLanguage of Original Document\nDocument Type\nPublication Stage\nOpen Access\nSource\nEID\n\n\n\n\n0\nAl Asady, A.; Anokhin, S.\nAl Asady, Ahmad (57219984746); Anokhin, Sergey...\n57219984746; 24482882200\nThe Trojan horse of international entrepreneur...\n2025\nJournal of World Business\n60\n6\n101677.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nNaN\nScopus\n2-s2.0-105014957115\n\n\n1\nThams, Y.; Dau, L.A.; Doh, J.; Kostova, T.; Ne...\nThams, Yannick (55357149800); Dau, Luis Alfons...\n55357149800; 35147597100; 7003920280; 66037741...\nPolitical ideology and the multinational enter...\n2025\nJournal of World Business\n60\n6\n101678.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nNaN\nScopus\n2-s2.0-105014844629\n\n\n2\nLindner, T.; Puck, J.; Puhr, H.\nLindner, Thomas (57159151000); Puck, Jonas (85...\n57159151000; 8563161700; 57223389639\nArtificial intelligence in international busin...\n2025\nJournal of World Business\n60\n6\n101676.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105014595041\n\n\n3\nBruton, G.D.; Mejía-Morelos, J.H.; Ahlstrom, D.\nBruton, Garry D. (6603867202); Mejía-Morelos, ...\n6603867202; 55748855800; 56525447800\nMultinational corporations and inclusive suppl...\n2025\nJournal of World Business\n60\n6\n101663.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013512235\n\n\n4\nLiang, Y.; Giroud, A.; Rygh, A.; Chen, Z.\nLiang, Yanze (57223851564); Giroud, Axèle L.A....\n57223851564; 7003496253; 37117826800; 58631386600\nPolitical embeddedness and post-acquisition in...\n2025\nJournal of World Business\n60\n6\n101665.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013485759\n\n\n\n\n5 rows × 41 columns",
    "crumbs": [
      "Home",
      "Methods",
      "Topic modeling"
    ]
  },
  {
    "objectID": "methods/01-topic-modeling.html#data",
    "href": "methods/01-topic-modeling.html#data",
    "title": "Topic modeling",
    "section": "",
    "text": "This is a snapshot of the data we will be working with.\n\nimport pandas as pd\ndata = pd.read_csv('../data/jwb-articles.csv')\ndata = data[data['Abstract'].notna()] # Keep nonempty abstracts\ndata.head()\n\n\n\n\n\n\n\n\nAuthors\nAuthor full names\nAuthor(s) ID\nTitle\nYear\nSource title\nVolume\nIssue\nArt. No.\nPage start\n...\nISSN\nISBN\nCODEN\nPubMed ID\nLanguage of Original Document\nDocument Type\nPublication Stage\nOpen Access\nSource\nEID\n\n\n\n\n0\nAl Asady, A.; Anokhin, S.\nAl Asady, Ahmad (57219984746); Anokhin, Sergey...\n57219984746; 24482882200\nThe Trojan horse of international entrepreneur...\n2025\nJournal of World Business\n60\n6\n101677.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nNaN\nScopus\n2-s2.0-105014957115\n\n\n1\nThams, Y.; Dau, L.A.; Doh, J.; Kostova, T.; Ne...\nThams, Yannick (55357149800); Dau, Luis Alfons...\n55357149800; 35147597100; 7003920280; 66037741...\nPolitical ideology and the multinational enter...\n2025\nJournal of World Business\n60\n6\n101678.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nNaN\nScopus\n2-s2.0-105014844629\n\n\n2\nLindner, T.; Puck, J.; Puhr, H.\nLindner, Thomas (57159151000); Puck, Jonas (85...\n57159151000; 8563161700; 57223389639\nArtificial intelligence in international busin...\n2025\nJournal of World Business\n60\n6\n101676.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105014595041\n\n\n3\nBruton, G.D.; Mejía-Morelos, J.H.; Ahlstrom, D.\nBruton, Garry D. (6603867202); Mejía-Morelos, ...\n6603867202; 55748855800; 56525447800\nMultinational corporations and inclusive suppl...\n2025\nJournal of World Business\n60\n6\n101663.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013512235\n\n\n4\nLiang, Y.; Giroud, A.; Rygh, A.; Chen, Z.\nLiang, Yanze (57223851564); Giroud, Axèle L.A....\n57223851564; 7003496253; 37117826800; 58631386600\nPolitical embeddedness and post-acquisition in...\n2025\nJournal of World Business\n60\n6\n101665.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013485759\n\n\n\n\n5 rows × 41 columns",
    "crumbs": [
      "Home",
      "Methods",
      "Topic modeling"
    ]
  },
  {
    "objectID": "methods/01-topic-modeling.html#latent-dirichlet-allocation-lda",
    "href": "methods/01-topic-modeling.html#latent-dirichlet-allocation-lda",
    "title": "Topic modeling",
    "section": "Latent Dirichlet Allocation (LDA)",
    "text": "Latent Dirichlet Allocation (LDA)\nWe first remove stop words and lemmatize the text.\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import RegexpTokenizer\n\ntokenizer = RegexpTokenizer(r'\\w+')\nstop_words = stopwords.words('english')\n\ndocs = []\nfor abstract in data['Abstract']:\n    tokens = word_tokenize(abstract.lower())\n    tokens = tokenizer.tokenize(' '.join(tokens))\n    rm_stop_words = [word for word in tokens if word not in stop_words]\n    docs.append(rm_stop_words)\n\nBefore we fit the LDA model, we construct a dictionary and convert our text to a bag of words.\n\nimport gensim\nfrom gensim.models.ldamodel import LdaModel\nfrom gensim import corpora\n\nlda_dict = corpora.Dictionary(docs)\nprint('The number of unique words:', len(lda_dict))\nprint(lda_dict)\n\nThe number of unique words: 8944\nDictionary&lt;8944 unique tokens: ['activities', 'affect', 'aims', 'also', 'argues']...&gt;\n\n\n\nlda_doc_corpus = [lda_dict.doc2bow(word) for word in docs]\nprint(lda_doc_corpus[0])\n\n[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2), (6, 1), (7, 2), (8, 3), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 2), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 3), (25, 2), (26, 2), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 2), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1)]\n\n\nNow we run LDA.\nThe LDA model hyperparameters alpha and eta are important for outcomes.\n\nAlpha is a parameter that controls the prior distribution over topic weights in each document.\nEta is a parameter for the prior distribution over word weights in each topic. Griffiths and Steyvers (2004) suggest a value of 50/k (where k is number of topics) for alpha and 0.1 for eta. Griffiths TL, Steyvers M (2004). “Finding Scientific Topics.” Proceedings of the National Academy of Sciences of the United States of America, 101, 5228–5235.\n\n\nlda = LdaModel(corpus=lda_doc_corpus, id2word=lda_dict, num_topics=5,\n              random_state=42, passes=10, alpha=50/5, eta=0.1)\n\nWe can examine the results of the LDA.\n\nlda.show_topics()\n\n[(0,\n  '0.008*\"international\" + 0.007*\"firms\" + 0.007*\"study\" + 0.006*\"foreign\" + 0.006*\"business\" + 0.006*\"performance\" + 0.005*\"firm\" + 0.005*\"knowledge\" + 0.005*\"internationalization\" + 0.005*\"institutional\"'),\n (1,\n  '0.013*\"international\" + 0.008*\"research\" + 0.007*\"firms\" + 0.007*\"business\" + 0.006*\"study\" + 0.006*\"global\" + 0.005*\"knowledge\" + 0.005*\"performance\" + 0.005*\"countries\" + 0.005*\"based\"'),\n (2,\n  '0.009*\"firms\" + 0.007*\"international\" + 0.007*\"subsidiary\" + 0.006*\"firm\" + 0.006*\"research\" + 0.006*\"global\" + 0.005*\"foreign\" + 0.005*\"business\" + 0.005*\"performance\" + 0.005*\"knowledge\"'),\n (3,\n  '0.008*\"research\" + 0.007*\"study\" + 0.006*\"firms\" + 0.006*\"performance\" + 0.006*\"international\" + 0.005*\"country\" + 0.005*\"foreign\" + 0.005*\"institutional\" + 0.005*\"business\" + 0.005*\"based\"'),\n (4,\n  '0.016*\"firms\" + 0.009*\"international\" + 0.007*\"study\" + 0.007*\"management\" + 0.006*\"performance\" + 0.006*\"firm\" + 0.005*\"research\" + 0.005*\"market\" + 0.005*\"based\" + 0.005*\"business\"')]\n\n\nAnd to visualise the topic probabilities for the first 15 abstracts, we can run the following code.\n\nimport matplotlib.pyplot as plt\nget_document_topics = lda.get_document_topics(lda_doc_corpus)\n\nall_probs = []\n\nfor doc_i in range(15):\n    doc_probs = get_document_topics[doc_i]\n    print(doc_probs)\n    probs = []\n    for (topic, prob) in doc_probs:\n        probs.append(prob)\n    all_probs.append(probs)\n\nplt.imshow(all_probs)\nplt.colorbar()\n\n[(0, 0.18502577), (1, 0.27372837), (2, 0.1858613), (3, 0.17344333), (4, 0.18194126)]\n[(0, 0.123563975), (1, 0.2688862), (2, 0.18882376), (3, 0.29686803), (4, 0.121858045)]\n[(0, 0.16409734), (1, 0.20209381), (2, 0.14940387), (3, 0.24056785), (4, 0.24383715)]\n[(0, 0.16398305), (1, 0.18669577), (2, 0.15503661), (3, 0.15883045), (4, 0.33545414)]\n[(0, 0.22544077), (1, 0.18374273), (2, 0.18284072), (3, 0.17819183), (4, 0.22978392)]\n[(0, 0.15645616), (1, 0.16588609), (2, 0.2596964), (3, 0.23863135), (4, 0.17933)]\n[(0, 0.13678451), (1, 0.20867777), (2, 0.12985952), (3, 0.16370597), (4, 0.3609723)]\n[(0, 0.19213931), (1, 0.13729951), (2, 0.14692119), (3, 0.16687186), (4, 0.35676813)]\n[(0, 0.23286998), (1, 0.2070405), (2, 0.15955229), (3, 0.19480501), (4, 0.20573217)]\n[(0, 0.27796832), (1, 0.19017023), (2, 0.16207576), (3, 0.18041292), (4, 0.1893728)]\n[(0, 0.16712207), (1, 0.1799058), (2, 0.15676005), (3, 0.16918756), (4, 0.3270245)]\n[(0, 0.267809), (1, 0.15632342), (2, 0.19364476), (3, 0.23301464), (4, 0.1492082)]\n[(0, 0.20694813), (1, 0.17432462), (2, 0.20037563), (3, 0.23672153), (4, 0.18163006)]\n[(0, 0.2521271), (1, 0.21680596), (2, 0.16364759), (3, 0.15310885), (4, 0.21431047)]\n[(0, 0.15407574), (1, 0.1787669), (2, 0.16378628), (3, 0.33469108), (4, 0.16867998)]",
    "crumbs": [
      "Home",
      "Methods",
      "Topic modeling"
    ]
  },
  {
    "objectID": "methods/01-topic-modeling.html#bertopic",
    "href": "methods/01-topic-modeling.html#bertopic",
    "title": "Topic modeling",
    "section": "BERTopic",
    "text": "BERTopic\n\nfrom bertopic import BERTopic\n\n# Get the abstracts from the 'Abstract' column\ndocs = df['Abstract'].tolist()\ndocs = [str(doc) for doc in docs]\n\n# Initialize BERTopic model\ntopic_model = BERTopic(language='english', calculate_probabilities=True, verbose=True)\n# Fit BERTopic to your data\ntopic_model.fit(docs)\n\n\n# Document-topic matrix is directly available in the 'topics' variable\ndoc_topic = topic_model.topics_\nprint('Document-topic matrix:')\n# Print the document-topic matrix\nprint(doc_topic)\n\nDocument-topic matrix:\n[-1, 30, -1, -1, 24, -1, 1, 5, -1, -1, -1, -1, -1, 5, 0, 8, -1, 5, 9, 10, 9, 3, -1, 30, -1, -1, -1, -1, 19, -1, -1, 8, 8, -1, -1, -1, -1, 9, 1, 9, 30, 5, -1, 29, 12, -1, -1, -1, 1, -1, 8, 3, 1, 17, -1, 10, 4, 10, -1, 1, 1, 18, -1, 19, -1, 11, 3, 8, 0, 18, 18, 18, 30, 17, -1, -1, 18, 2, 9, 8, 30, 9, 18, 8, -1, 0, -1, 9, 5, 5, 28, 8, -1, -1, 2, -1, 19, -1, -1, 0, 18, -1, -1, -1, -1, 23, 8, 17, 18, -1, -1, -1, 9, -1, 9, 1, 30, 5, -1, 5, 28, -1, 2, 8, -1, -1, 8, 8, 9, -1, -1, -1, 12, -1, -1, 16, 8, -1, 24, -1, 6, -1, 9, 22, 10, -1, 10, 6, 9, -1, -1, 3, -1, -1, -1, 9, -1, -1, 3, -1, 10, 0, 0, 0, 23, 0, 1, -1, 24, -1, -1, 27, 4, 3, -1, -1, 27, 24, -1, 5, 9, -1, -1, 0, 1, -1, 18, 8, -1, 1, 28, -1, 9, 13, -1, -1, -1, 5, -1, 21, 17, 1, 18, 27, 1, 8, 9, 1, -1, -1, 0, 9, 2, -1, -1, 1, -1, -1, -1, -1, 24, 14, -1, -1, 5, 23, 6, 5, 1, -1, -1, 3, -1, -1, -1, 27, 17, -1, -1, -1, -1, 1, -1, 18, -1, -1, 1, 10, 10, -1, -1, -1, -1, 10, 9, 8, 1, 17, 23, 17, 29, -1, -1, 1, 5, -1, 5, -1, -1, -1, -1, -1, 13, 19, 17, -1, -1, -1, -1, -1, 29, 30, -1, 18, 1, 10, -1, 5, 27, 3, 9, 27, -1, 10, 9, -1, -1, 0, 0, -1, -1, -1, 29, 9, -1, -1, -1, 16, 5, 16, 1, 1, 27, -1, 1, 20, 3, -1, -1, 23, 26, 27, -1, 3, 8, -1, -1, 10, 20, -1, 11, 19, 11, 11, -1, -1, -1, 3, 17, 0, 10, 0, 6, 23, 3, 5, -1, 11, 12, -1, -1, 0, -1, 5, 28, -1, -1, -1, -1, -1, 16, -1, 13, 0, -1, 8, -1, 10, 26, -1, 5, 0, 3, -1, 10, 10, 20, 11, 10, -1, 10, -1, 6, 0, 24, 26, -1, 3, 11, 26, -1, 4, 0, 0, 8, -1, -1, 8, -1, 23, -1, -1, -1, 3, -1, 13, 18, -1, -1, 19, 8, 6, 27, -1, -1, -1, 11, 10, 5, -1, -1, -1, -1, -1, -1, 6, 8, 21, -1, -1, -1, 11, 24, -1, -1, -1, -1, 30, 8, 4, 5, 5, 2, 5, 11, -1, -1, -1, -1, -1, 4, -1, -1, 30, -1, -1, 18, -1, 0, -1, 0, 20, 17, 6, 1, 17, -1, 27, 6, 3, -1, -1, 11, 25, 2, -1, 17, 16, 24, 20, -1, -1, -1, -1, -1, -1, 11, -1, 11, 11, -1, 5, 10, -1, 0, -1, -1, 24, 10, -1, -1, -1, 25, -1, -1, 24, 11, 5, -1, 0, -1, 10, -1, 18, 4, 20, 8, 18, -1, -1, -1, 2, -1, -1, 10, -1, 3, 5, 1, -1, 16, 7, 12, -1, 3, -1, 6, 27, 23, 24, 11, 16, -1, 10, -1, 8, 12, 8, 2, 8, -1, 4, 11, 8, -1, 8, 1, 16, -1, 16, -1, -1, 9, -1, 11, 0, 3, -1, -1, -1, -1, 1, 3, 1, 16, -1, -1, 13, 0, -1, 6, 1, -1, 20, 24, 24, 29, -1, -1, -1, 1, 30, -1, 0, -1, 4, 16, 20, 28, 0, 19, -1, -1, -1, 2, -1, 13, -1, -1, 13, -1, 30, 25, 0, -1, -1, 0, 2, 30, 0, 26, -1, 17, 4, -1, 11, -1, 17, 20, 10, 8, 20, -1, 3, 19, 5, -1, 20, -1, -1, 13, -1, 2, 3, -1, 0, 7, 12, 12, 12, 12, 12, 12, 12, -1, 12, 12, 12, 12, 2, 3, -1, 6, 11, -1, -1, 29, -1, 29, 29, 9, -1, 0, 29, 26, -1, 0, -1, -1, 5, 10, -1, 10, -1, 0, -1, -1, 1, -1, -1, 6, 1, 19, 0, 16, -1, -1, -1, -1, 18, 13, -1, 16, 13, 4, 18, -1, -1, 20, 13, 22, -1, 3, 7, 2, 7, 7, 2, 7, 2, 7, 12, 5, 0, 17, 3, 6, 4, 5, -1, -1, 0, 9, -1, -1, 3, -1, -1, 2, -1, 4, 6, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, -1, 1, 4, 16, 4, 4, 26, 0, 11, 18, -1, -1, 23, 0, -1, -1, 3, -1, -1, -1, 7, -1, -1, 7, -1, 20, -1, -1, 1, 28, -1, 25, 7, 25, 26, -1, -1, 0, 6, 16, 3, 7, -1, 20, -1, -1, 22, -1, 30, 16, -1, 16, 26, -1, -1, 12, 6, 6, 9, -1, -1, -1, 27, 9, -1, 17, 17, -1, 1, 1, 17, -1, -1, 17, 17, -1, 17, 20, 19, 22, 6, 0, -1, 4, -1, -1, -1, 0, 9, 16, -1, 7, 8, 2, 29, -1, -1, 4, 12, 14, 0, 23, 7, -1, 6, 13, -1, 13, -1, 13, 13, 13, 13, 13, -1, -1, -1, 4, 5, 19, 1, 19, 5, -1, -1, 26, 13, -1, 12, 12, 12, 12, 12, 12, 12, -1, 12, 11, 7, 13, 22, 15, 6, 0, 2, 10, -1, -1, 0, -1, -1, -1, -1, -1, -1, 21, 4, -1, 24, 20, -1, 0, -1, 20, -1, 25, -1, -1, 26, -1, -1, -1, 19, -1, -1, 0, 19, -1, 25, 0, 20, 6, -1, 31, -1, 3, 7, -1, -1, -1, 18, -1, 4, 29, 19, -1, -1, 6, -1, -1, -1, 21, 1, 3, 1, 20, 1, 1, -1, 13, 1, -1, 23, 11, 2, 25, -1, -1, 9, 2, -1, 5, -1, 22, 28, 0, -1, 0, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 2, -1, -1, 25, -1, 25, -1, -1, 2, -1, 31, 11, 21, 7, -1, 7, 5, -1, 25, -1, -1, 6, 10, 22, -1, 26, -1, 21, 3, -1, -1, -1, -1, -1, 25, 2, -1, -1, -1, -1, 6, -1, -1, -1, 2, 2, 2, 2, 0, 2, -1, 23, 26, -1, -1, -1, 4, 6, 7, 23, 8, -1, -1, -1, 0, -1, 3, -1, 21, 2, 2, -1, 7, 7, 7, -1, -1, -1, 17, 7, 7, -1, 7, -1, 29, -1, 0, 16, -1, -1, -1, -1, 21, 0, -1, -1, -1, 7, -1, 7, -1, 6, 11, 31, 15, 31, 11, 31, 31, -1, -1, -1, 9, 11, 28, -1, -1, -1, -1, 0, -1, 0, -1, 2, 0, 25, 0, 28, -1, 0, -1, 0, 15, -1, 2, 29, -1, -1, 28, -1, -1, 2, -1, -1, 2, -1, 21, 13, 4, -1, 4, -1, -1, -1, -1, -1, 2, 2, -1, 7, 7, 6, 25, -1, -1, -1, -1, 9, 21, 14, 7, 22, -1, 3, 3, 3, 3, -1, 6, 19, 3, 22, -1, 4, -1, -1, -1, 22, 22, 19, 1, 1, -1, -1, -1, 31, -1, 23, 31, -1, 16, -1, -1, 22, -1, 13, 21, -1, -1, 7, 14, -1, 21, 21, -1, -1, 7, 21, -1, 22, 21, -1, 7, -1, 24, -1, -1, -1, -1, 23, -1, 16, -1, -1, -1, 4, -1, 16, 6, -1, 2, -1, 11, 21, 2, 6, 21, 23, 2, 2, 31, -1, -1, -1, -1, -1, 6, -1, 2, 7, 21, -1, 2, 6, -1, -1, 6, 28, -1, -1, -1, -1, 14, -1, -1, -1, -1, 14, -1, -1, 14, -1, 14, 13, 22, 5, -1, -1, -1, -1, 7, -1, -1, -1, 19, -1, -1, 9, -1, 15, -1, 28, 15, 13, 14, -1, 14, 8, 9, 14, 15, -1, 14, -1, 14, -1, 31, 14, 14, 9, 14, 6, -1, -1, -1, 14, 14, -1, 14, -1, 27, 14, 14, 14, 16, 14, 14, 14, 3, 15, 15, 28, 15, -1, -1, 15, 15, 15, 15, -1, -1, 15, 15, 15, 15, 15, 15, 15, -1, 15, 19, -1, 15, 15, 22, 22, 22]\n\n\n\n# Get probabilities for each topic\nprobs = topic_model.probabilities_\nprint('Topic probabilities for the first document:')\nprint(probs[0].round(2))\nprint()\n# Print topic probabilities for the first 15 documents\nfor i in range(min(15, len(docs))):\n    print(f'Document {i + 1} is in topic {doc_topic[i]}')\n    print(f'Topic probabilities for Document {i + 1}:')\n    print(probs[i].round(3))\n    print()\n\nTopic probabilities for the first document:\n[0.   0.   0.   0.   0.   0.   0.   0.   0.   0.01 0.01 0.01 0.   0.\n 0.   0.   0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.01\n 0.   0.   0.01 0.01]\n\nDocument 1 is in topic -1\nTopic probabilities for Document 1:\n[0.003 0.003 0.003 0.004 0.002 0.003 0.003 0.003 0.003 0.006 0.007 0.007\n 0.003 0.004 0.005 0.004 0.005 0.002 0.004 0.004 0.002 0.004 0.003 0.003\n 0.003 0.003 0.004 0.008 0.003 0.004 0.006 0.005]\n\nDocument 2 is in topic 30\nTopic probabilities for Document 2:\n[0.012 0.014 0.008 0.013 0.007 0.017 0.011 0.011 0.02  0.037 0.024 0.017\n 0.008 0.016 0.015 0.01  0.03  0.006 0.009 0.016 0.007 0.011 0.017 0.009\n 0.014 0.009 0.019 0.025 0.012 0.015 0.55  0.012]\n\nDocument 3 is in topic -1\nTopic probabilities for Document 3:\n[0.004 0.004 0.002 0.003 0.002 0.004 0.002 0.003 0.005 0.004 0.003 0.002\n 0.002 0.003 0.002 0.002 0.006 0.002 0.002 0.003 0.002 0.003 0.004 0.002\n 0.005 0.002 0.006 0.004 0.003 0.005 0.006 0.002]\n\nDocument 4 is in topic -1\nTopic probabilities for Document 4:\n[0.003 0.003 0.002 0.004 0.002 0.004 0.003 0.002 0.003 0.007 0.018 0.009\n 0.002 0.005 0.006 0.003 0.007 0.001 0.003 0.005 0.002 0.003 0.004 0.002\n 0.003 0.002 0.004 0.006 0.003 0.003 0.007 0.005]\n\nDocument 5 is in topic 24\nTopic probabilities for Document 5:\n[0.077 0.035 0.014 0.057 0.013 0.023 0.02  0.015 0.017 0.019 0.02  0.016\n 0.012 0.016 0.014 0.011 0.028 0.012 0.012 0.026 0.014 0.027 0.021 0.026\n 0.151 0.016 0.049 0.024 0.033 0.027 0.022 0.012]\n\nDocument 6 is in topic -1\nTopic probabilities for Document 6:\n[0.02  0.021 0.014 0.023 0.012 0.024 0.017 0.018 0.027 0.069 0.047 0.034\n 0.014 0.024 0.028 0.019 0.05  0.01  0.016 0.028 0.011 0.019 0.024 0.016\n 0.022 0.015 0.032 0.057 0.019 0.022 0.124 0.023]\n\nDocument 7 is in topic 1\nTopic probabilities for Document 7:\n[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0.]\n\nDocument 8 is in topic 5\nTopic probabilities for Document 8:\n[0.015 0.042 0.01  0.015 0.009 0.088 0.018 0.01  0.036 0.019 0.018 0.013\n 0.009 0.02  0.013 0.009 0.024 0.009 0.011 0.019 0.01  0.012 0.031 0.012\n 0.024 0.01  0.018 0.016 0.014 0.017 0.024 0.011]\n\nDocument 9 is in topic -1\nTopic probabilities for Document 9:\n[0.028 0.122 0.014 0.031 0.014 0.056 0.027 0.015 0.028 0.025 0.025 0.019\n 0.013 0.025 0.016 0.012 0.034 0.013 0.015 0.026 0.012 0.021 0.033 0.02\n 0.066 0.015 0.032 0.024 0.024 0.03  0.03  0.014]\n\nDocument 10 is in topic -1\nTopic probabilities for Document 10:\n[0.013 0.015 0.007 0.019 0.007 0.017 0.016 0.009 0.014 0.026 0.125 0.052\n 0.007 0.02  0.028 0.016 0.038 0.006 0.012 0.03  0.009 0.013 0.02  0.011\n 0.016 0.008 0.018 0.031 0.014 0.013 0.028 0.023]\n\nDocument 11 is in topic -1\nTopic probabilities for Document 11:\n[0.01  0.016 0.006 0.012 0.005 0.016 0.01  0.007 0.016 0.029 0.021 0.012\n 0.006 0.012 0.011 0.007 0.04  0.005 0.007 0.015 0.005 0.008 0.014 0.007\n 0.014 0.006 0.015 0.018 0.009 0.01  0.023 0.008]\n\nDocument 12 is in topic -1\nTopic probabilities for Document 12:\n[0.014 0.012 0.006 0.05  0.006 0.01  0.011 0.007 0.009 0.012 0.018 0.014\n 0.006 0.01  0.01  0.008 0.019 0.005 0.007 0.018 0.006 0.014 0.011 0.011\n 0.017 0.007 0.018 0.023 0.015 0.011 0.014 0.01 ]\n\nDocument 13 is in topic -1\nTopic probabilities for Document 13:\n[0.026 0.027 0.012 0.031 0.011 0.027 0.019 0.015 0.026 0.041 0.04  0.026\n 0.012 0.021 0.022 0.014 0.151 0.01  0.012 0.043 0.012 0.02  0.027 0.017\n 0.035 0.014 0.055 0.054 0.023 0.024 0.049 0.017]\n\nDocument 14 is in topic 5\nTopic probabilities for Document 14:\n[0.016 0.048 0.011 0.017 0.01  0.108 0.02  0.012 0.039 0.021 0.02  0.015\n 0.01  0.022 0.014 0.01  0.026 0.01  0.012 0.021 0.011 0.014 0.034 0.013\n 0.027 0.011 0.02  0.017 0.015 0.018 0.026 0.012]\n\nDocument 15 is in topic 0\nTopic probabilities for Document 15:\n[0.055 0.017 0.017 0.023 0.014 0.014 0.013 0.02  0.013 0.013 0.013 0.011\n 0.014 0.012 0.01  0.008 0.018 0.012 0.009 0.017 0.015 0.026 0.014 0.026\n 0.028 0.021 0.04  0.017 0.032 0.031 0.016 0.009]\n\n\n\n\n# Get the lists of keywords under each topic\ntopic_keywords = topic_model.get_topics()\n\n# Print the lists of keywords for each topic\nfor topic_id, keywords in topic_keywords.items():\n    keywords = [(u, round(v, 3)) for u, v in keywords]\n    print(f'Topic {topic_id}: {keywords}')\n\nWe can also examine the topics more closely.\n\n# To see the first 5 topics\nfreq = topic_model.get_topic_info()\nfreq.head(5)\n\n\n\n\n\n\n\n\nTopic\nCount\nName\nRepresentation\nRepresentative_Docs\n\n\n\n\n0\n-1\n544\n-1_the_of_and_in\n[the, of, and, in, to, that, we, on, this, with]\n[In recent years, there has been an increasing...\n\n\n1\n0\n61\n0_knowledge_transfer_and_the\n[knowledge, transfer, and, the, of, on, to, th...\n[This paper proposes a conceptual framework de...\n\n\n2\n1\n52\n1_international_entrepreneurial_internationali...\n[international, entrepreneurial, international...\n[Grounded in the resource-based view of the fi...\n\n\n3\n2\n43\n2_career_expatriates_expatriate_assignments\n[career, expatriates, expatriate, assignments,...\n[Creating organizational processes which nurtu...\n\n\n4\n3\n38\n3_acquisitions_acquisition_crossborder_acquirers\n[acquisitions, acquisition, crossborder, acqui...\n[This study develops and tests a framework abo...\n\n\n\n\n\n\n\nNote that topic -1 indicates that the dociment has not been grouped into a topic.",
    "crumbs": [
      "Home",
      "Methods",
      "Topic modeling"
    ]
  },
  {
    "objectID": "methods/03-sentence-similarity-analysis.html",
    "href": "methods/03-sentence-similarity-analysis.html",
    "title": "Sentence similarity analysis",
    "section": "",
    "text": "This is a snapshot of the data we will be working with.\n\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv('../data/jwb-articles.csv')\ndata = data[data['Abstract'].notna()] # Keep nonempty abstracts\ndata.head()\n\n\n\n\n\n\n\n\nAuthors\nAuthor full names\nAuthor(s) ID\nTitle\nYear\nSource title\nVolume\nIssue\nArt. No.\nPage start\n...\nISSN\nISBN\nCODEN\nPubMed ID\nLanguage of Original Document\nDocument Type\nPublication Stage\nOpen Access\nSource\nEID\n\n\n\n\n0\nAl Asady, A.; Anokhin, S.\nAl Asady, Ahmad (57219984746); Anokhin, Sergey...\n57219984746; 24482882200\nThe Trojan horse of international entrepreneur...\n2025\nJournal of World Business\n60\n6\n101677.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nNaN\nScopus\n2-s2.0-105014957115\n\n\n1\nThams, Y.; Dau, L.A.; Doh, J.; Kostova, T.; Ne...\nThams, Yannick (55357149800); Dau, Luis Alfons...\n55357149800; 35147597100; 7003920280; 66037741...\nPolitical ideology and the multinational enter...\n2025\nJournal of World Business\n60\n6\n101678.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nNaN\nScopus\n2-s2.0-105014844629\n\n\n2\nLindner, T.; Puck, J.; Puhr, H.\nLindner, Thomas (57159151000); Puck, Jonas (85...\n57159151000; 8563161700; 57223389639\nArtificial intelligence in international busin...\n2025\nJournal of World Business\n60\n6\n101676.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105014595041\n\n\n3\nBruton, G.D.; Mejía-Morelos, J.H.; Ahlstrom, D.\nBruton, Garry D. (6603867202); Mejía-Morelos, ...\n6603867202; 55748855800; 56525447800\nMultinational corporations and inclusive suppl...\n2025\nJournal of World Business\n60\n6\n101663.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013512235\n\n\n4\nLiang, Y.; Giroud, A.; Rygh, A.; Chen, Z.\nLiang, Yanze (57223851564); Giroud, Axèle L.A....\n57223851564; 7003496253; 37117826800; 58631386600\nPolitical embeddedness and post-acquisition in...\n2025\nJournal of World Business\n60\n6\n101665.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013485759\n\n\n\n\n5 rows × 41 columns",
    "crumbs": [
      "Home",
      "Methods",
      "Sentence similarity analysis"
    ]
  },
  {
    "objectID": "methods/03-sentence-similarity-analysis.html#data",
    "href": "methods/03-sentence-similarity-analysis.html#data",
    "title": "Sentence similarity analysis",
    "section": "",
    "text": "This is a snapshot of the data we will be working with.\n\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv('../data/jwb-articles.csv')\ndata = data[data['Abstract'].notna()] # Keep nonempty abstracts\ndata.head()\n\n\n\n\n\n\n\n\nAuthors\nAuthor full names\nAuthor(s) ID\nTitle\nYear\nSource title\nVolume\nIssue\nArt. No.\nPage start\n...\nISSN\nISBN\nCODEN\nPubMed ID\nLanguage of Original Document\nDocument Type\nPublication Stage\nOpen Access\nSource\nEID\n\n\n\n\n0\nAl Asady, A.; Anokhin, S.\nAl Asady, Ahmad (57219984746); Anokhin, Sergey...\n57219984746; 24482882200\nThe Trojan horse of international entrepreneur...\n2025\nJournal of World Business\n60\n6\n101677.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nNaN\nScopus\n2-s2.0-105014957115\n\n\n1\nThams, Y.; Dau, L.A.; Doh, J.; Kostova, T.; Ne...\nThams, Yannick (55357149800); Dau, Luis Alfons...\n55357149800; 35147597100; 7003920280; 66037741...\nPolitical ideology and the multinational enter...\n2025\nJournal of World Business\n60\n6\n101678.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nNaN\nScopus\n2-s2.0-105014844629\n\n\n2\nLindner, T.; Puck, J.; Puhr, H.\nLindner, Thomas (57159151000); Puck, Jonas (85...\n57159151000; 8563161700; 57223389639\nArtificial intelligence in international busin...\n2025\nJournal of World Business\n60\n6\n101676.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105014595041\n\n\n3\nBruton, G.D.; Mejía-Morelos, J.H.; Ahlstrom, D.\nBruton, Garry D. (6603867202); Mejía-Morelos, ...\n6603867202; 55748855800; 56525447800\nMultinational corporations and inclusive suppl...\n2025\nJournal of World Business\n60\n6\n101663.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013512235\n\n\n4\nLiang, Y.; Giroud, A.; Rygh, A.; Chen, Z.\nLiang, Yanze (57223851564); Giroud, Axèle L.A....\n57223851564; 7003496253; 37117826800; 58631386600\nPolitical embeddedness and post-acquisition in...\n2025\nJournal of World Business\n60\n6\n101665.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013485759\n\n\n\n\n5 rows × 41 columns",
    "crumbs": [
      "Home",
      "Methods",
      "Sentence similarity analysis"
    ]
  },
  {
    "objectID": "methods/03-sentence-similarity-analysis.html#cosine-similarity",
    "href": "methods/03-sentence-similarity-analysis.html#cosine-similarity",
    "title": "Sentence similarity analysis",
    "section": "Cosine similarity",
    "text": "Cosine similarity\nWe first preprocess our text by tokenizing, remove stop words and stem the abstracts.\n\nimport re \nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\ndef preprocess_text(text):\n    # lowercasing\n    lowercased_text = text.lower()\n\n    # cleaning \n    remove_punctuation = re.sub(r'[^\\w\\s]', '', lowercased_text)\n    remove_white_space = remove_punctuation.strip()\n\n    # Tokenization = Breaking down each sentence into an array\n    tokenized_text = word_tokenize(remove_white_space)\n\n    # Stop Words/filtering = Removing irrelevant words\n    stop_words = set(stopwords.words('english'))\n    stopwords_removed = [word for word in tokenized_text if word not in stop_words]\n\n    # Stemming = Transforming words into their base form\n    ps = PorterStemmer()\n    stemmed_text = [ps.stem(word) for word in stopwords_removed]\n    \n    # Return the stemmed text as a list\n    return stemmed_text\n\ndata['clean_abstract'] = data['Abstract'].apply(preprocess_text)\ndata[['Abstract', 'clean_abstract']].head(5)\n\n\n\n\n\n\n\n\nAbstract\nclean_abstract\n\n\n\n\n0\nThis study explores the under-theorized relati...\n[studi, explor, undertheor, relationship, inte...\n\n\n1\nWhile politics and political issues such as ri...\n[polit, polit, issu, risk, domin, agenda, inte...\n\n\n2\nThis paper discusses the impact of artificial ...\n[paper, discuss, impact, artifici, intellig, a...\n\n\n3\nAn institutional logic represents the way a pa...\n[institut, logic, repres, way, particular, soc...\n\n\n4\nPolitical embeddedness has been shown to influ...\n[polit, embedded, shown, influenc, firm, innov...\n\n\n\n\n\n\n\nWe now compute the term frequency-inverse document frequency (TF-IDF) scores of each abstract.\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Join each list of words into a string\ntexts = data['clean_abstract'].apply(lambda x: ' '.join(x))\n\n# Compute TF-IDF\nvectorizer = TfidfVectorizer()\ntfidf_matrix = vectorizer.fit_transform(texts)\n\n# Get feature names (words)\nfeature_names = vectorizer.get_feature_names_out()\n\n# Create TF-IDF DataFrame\ndf_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n\n# Optionally, include the original data (Abstract + clean_abstract)\ndata_tfidf = pd.concat([data.reset_index(drop=True), df_tfidf], axis=1)\n\n\ndata_tfidf.head(5)\n\n\n\n\n\n\n\n\nAuthors\nAuthor full names\nAuthor(s) ID\nTitle\nYear\nSource title\nVolume\nIssue\nArt. No.\nPage start\n...\nzeroinfl\nzeroshot\nzhirinovski\nzimbabw\nzizhu\nzone\nzoom\nzurawicki\nﬁrm\nﬁrmlevel\n\n\n\n\n0\nAl Asady, A.; Anokhin, S.\nAl Asady, Ahmad (57219984746); Anokhin, Sergey...\n57219984746; 24482882200\nThe Trojan horse of international entrepreneur...\n2025\nJournal of World Business\n60\n6\n101677.0\nNaN\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\nThams, Y.; Dau, L.A.; Doh, J.; Kostova, T.; Ne...\nThams, Yannick (55357149800); Dau, Luis Alfons...\n55357149800; 35147597100; 7003920280; 66037741...\nPolitical ideology and the multinational enter...\n2025\nJournal of World Business\n60\n6\n101678.0\nNaN\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\nLindner, T.; Puck, J.; Puhr, H.\nLindner, Thomas (57159151000); Puck, Jonas (85...\n57159151000; 8563161700; 57223389639\nArtificial intelligence in international busin...\n2025\nJournal of World Business\n60\n6\n101676.0\nNaN\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\nBruton, G.D.; Mejía-Morelos, J.H.; Ahlstrom, D.\nBruton, Garry D. (6603867202); Mejía-Morelos, ...\n6603867202; 55748855800; 56525447800\nMultinational corporations and inclusive suppl...\n2025\nJournal of World Business\n60\n6\n101663.0\nNaN\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\nLiang, Y.; Giroud, A.; Rygh, A.; Chen, Z.\nLiang, Yanze (57223851564); Giroud, Axèle L.A....\n57223851564; 7003496253; 37117826800; 58631386600\nPolitical embeddedness and post-acquisition in...\n2025\nJournal of World Business\n60\n6\n101665.0\nNaN\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n5 rows × 6645 columns\n\n\n\nCompute pairwise similarity scores and show a heat map.\n\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport matplotlib.pyplot as plt\n\nsim_scores = np.zeros((df_tfidf.shape[0], df_tfidf.shape[0]))\n\ntfidf_scores = df_tfidf.values\n\ncosim = cosine_similarity(tfidf_scores)\n\nplt.imshow(cosim)\nplt.colorbar()\n\n\n\n\n\n\n\n\nFor a closer look, we examine the first 100 abstracts. The brighter the cell, the more similar the abstracts are.\n\nplt.imshow(cosim[:100, :100])\nplt.colorbar()",
    "crumbs": [
      "Home",
      "Methods",
      "Sentence similarity analysis"
    ]
  },
  {
    "objectID": "methods/02-sentiment-analysis.html",
    "href": "methods/02-sentiment-analysis.html",
    "title": "Sentiment analysis",
    "section": "",
    "text": "This is a snapshot of the data we will be working with.\n\nimport pandas as pd\ndata = pd.read_csv('../data/jwb-articles.csv')\ndata = data[data['Abstract'].notna()] # Keep nonempty abstracts\ndata.head()\n\n\n\n\n\n\n\n\nAuthors\nAuthor full names\nAuthor(s) ID\nTitle\nYear\nSource title\nVolume\nIssue\nArt. No.\nPage start\n...\nISSN\nISBN\nCODEN\nPubMed ID\nLanguage of Original Document\nDocument Type\nPublication Stage\nOpen Access\nSource\nEID\n\n\n\n\n0\nAl Asady, A.; Anokhin, S.\nAl Asady, Ahmad (57219984746); Anokhin, Sergey...\n57219984746; 24482882200\nThe Trojan horse of international entrepreneur...\n2025\nJournal of World Business\n60\n6\n101677.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nNaN\nScopus\n2-s2.0-105014957115\n\n\n1\nThams, Y.; Dau, L.A.; Doh, J.; Kostova, T.; Ne...\nThams, Yannick (55357149800); Dau, Luis Alfons...\n55357149800; 35147597100; 7003920280; 66037741...\nPolitical ideology and the multinational enter...\n2025\nJournal of World Business\n60\n6\n101678.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nNaN\nScopus\n2-s2.0-105014844629\n\n\n2\nLindner, T.; Puck, J.; Puhr, H.\nLindner, Thomas (57159151000); Puck, Jonas (85...\n57159151000; 8563161700; 57223389639\nArtificial intelligence in international busin...\n2025\nJournal of World Business\n60\n6\n101676.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105014595041\n\n\n3\nBruton, G.D.; Mejía-Morelos, J.H.; Ahlstrom, D.\nBruton, Garry D. (6603867202); Mejía-Morelos, ...\n6603867202; 55748855800; 56525447800\nMultinational corporations and inclusive suppl...\n2025\nJournal of World Business\n60\n6\n101663.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013512235\n\n\n4\nLiang, Y.; Giroud, A.; Rygh, A.; Chen, Z.\nLiang, Yanze (57223851564); Giroud, Axèle L.A....\n57223851564; 7003496253; 37117826800; 58631386600\nPolitical embeddedness and post-acquisition in...\n2025\nJournal of World Business\n60\n6\n101665.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013485759\n\n\n\n\n5 rows × 41 columns",
    "crumbs": [
      "Home",
      "Methods",
      "Sentiment analysis"
    ]
  },
  {
    "objectID": "methods/02-sentiment-analysis.html#data",
    "href": "methods/02-sentiment-analysis.html#data",
    "title": "Sentiment analysis",
    "section": "",
    "text": "This is a snapshot of the data we will be working with.\n\nimport pandas as pd\ndata = pd.read_csv('../data/jwb-articles.csv')\ndata = data[data['Abstract'].notna()] # Keep nonempty abstracts\ndata.head()\n\n\n\n\n\n\n\n\nAuthors\nAuthor full names\nAuthor(s) ID\nTitle\nYear\nSource title\nVolume\nIssue\nArt. No.\nPage start\n...\nISSN\nISBN\nCODEN\nPubMed ID\nLanguage of Original Document\nDocument Type\nPublication Stage\nOpen Access\nSource\nEID\n\n\n\n\n0\nAl Asady, A.; Anokhin, S.\nAl Asady, Ahmad (57219984746); Anokhin, Sergey...\n57219984746; 24482882200\nThe Trojan horse of international entrepreneur...\n2025\nJournal of World Business\n60\n6\n101677.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nNaN\nScopus\n2-s2.0-105014957115\n\n\n1\nThams, Y.; Dau, L.A.; Doh, J.; Kostova, T.; Ne...\nThams, Yannick (55357149800); Dau, Luis Alfons...\n55357149800; 35147597100; 7003920280; 66037741...\nPolitical ideology and the multinational enter...\n2025\nJournal of World Business\n60\n6\n101678.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nNaN\nScopus\n2-s2.0-105014844629\n\n\n2\nLindner, T.; Puck, J.; Puhr, H.\nLindner, Thomas (57159151000); Puck, Jonas (85...\n57159151000; 8563161700; 57223389639\nArtificial intelligence in international busin...\n2025\nJournal of World Business\n60\n6\n101676.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105014595041\n\n\n3\nBruton, G.D.; Mejía-Morelos, J.H.; Ahlstrom, D.\nBruton, Garry D. (6603867202); Mejía-Morelos, ...\n6603867202; 55748855800; 56525447800\nMultinational corporations and inclusive suppl...\n2025\nJournal of World Business\n60\n6\n101663.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013512235\n\n\n4\nLiang, Y.; Giroud, A.; Rygh, A.; Chen, Z.\nLiang, Yanze (57223851564); Giroud, Axèle L.A....\n57223851564; 7003496253; 37117826800; 58631386600\nPolitical embeddedness and post-acquisition in...\n2025\nJournal of World Business\n60\n6\n101665.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013485759\n\n\n\n\n5 rows × 41 columns",
    "crumbs": [
      "Home",
      "Methods",
      "Sentiment analysis"
    ]
  },
  {
    "objectID": "methods/02-sentiment-analysis.html#bertsentiment",
    "href": "methods/02-sentiment-analysis.html#bertsentiment",
    "title": "Sentiment analysis",
    "section": "BERTsentiment",
    "text": "BERTsentiment\n\nfrom datasets import Dataset\n# Only retain the first 200 terms in the sentence\ndata['Abstract_200'] = data['Abstract'].apply(lambda x: ' '.join(x.split(' ')[:200]))\n\n# Convert the Pandas DataFrame to a Hugging Face Dataset\ndataset = Dataset.from_pandas(data)\n\n\nfrom transformers import pipeline\n# Create the sentiment analysis pipeline\nnlp = pipeline('sentiment-analysis', model='hw2942/bert-base-chinese-finetuning-financial-news-sentiment-v2')\n\ndef get_sentiment(examples):\n    # Initialize lists to store results\n    sentiments = []\n    scores = []\n    final_sentiment_scores = []\n\n    # Process each entry in the batch\n    for text in examples['Abstract_200']:\n        if isinstance(text, str):\n            try:\n                result = nlp(text)\n                sentiment = result[0]['label']\n                score = result[0]['score']\n                sentiment_label = 1 if sentiment == 'Positive' else -1 if sentiment == 'Negative' else 0\n                final_sentiment_score = sentiment_label * score\n\n                sentiments.append(sentiment)\n                scores.append(score)\n                final_sentiment_scores.append(final_sentiment_score)\n            except Exception as e:\n                print(f'Error processing text: {text}. Error: {e}', flush=True)\n                # Append default values in case of an error\n                sentiments.append(None)\n                scores.append(None)\n                final_sentiment_scores.append(None)\n        else:\n            print(f'Non-string entry found: {text}', flush=True)\n            # Append default values for non-string entries\n            sentiments.append(None)\n            scores.append(None)\n            final_sentiment_scores.append(None)\n\n    # Ensure the output lists are of the same length as the batch size\n    batch_size = len(examples['Abstract_200'])\n    while len(sentiments) &lt; batch_size:\n        sentiments.append(None)\n        scores.append(None)\n        final_sentiment_scores.append(None)\n\n    return {'sentiment': sentiments, 'score': scores, 'final_sentiment_score': final_sentiment_scores}\n\ndataset = dataset.map(get_sentiment, batched=True, batch_size=64)\n\nExamine the first 5 sentiment scores.\n\ndf = pd.DataFrame(dataset)\ndf.loc[:5, ['Abstract_200', 'sentiment', 'score', 'final_sentiment_score']]\n\n\n\n\n\n\n\n\nAbstract_200\nsentiment\nscore\nfinal_sentiment_score\n\n\n\n\n0\nThis study explores the under-theorized relati...\nNeutral\n0.999846\n0.0\n\n\n1\nWhile politics and political issues such as ri...\nNeutral\n0.999837\n0.0\n\n\n2\nThis paper discusses the impact of artificial ...\nNeutral\n0.999845\n0.0\n\n\n3\nAn institutional logic represents the way a pa...\nNeutral\n0.999828\n0.0\n\n\n4\nPolitical embeddedness has been shown to influ...\nNeutral\n0.999827\n0.0\n\n\n5\nHow do MNE subsidiaries respond and perform af...\nNeutral\n0.999835\n0.0",
    "crumbs": [
      "Home",
      "Methods",
      "Sentiment analysis"
    ]
  },
  {
    "objectID": "models/BERTopic.html",
    "href": "models/BERTopic.html",
    "title": "Machine learning perspective paper",
    "section": "",
    "text": "import pandas as pd\ndata = pd.read_csv('../data/jwb-articles.csv')\ndata = data[data['Abstract'].notna()] # Keep nonempty abstracts\ndata.head()\n\n\n\n\n\n\n\n\nAuthors\nAuthor full names\nAuthor(s) ID\nTitle\nYear\nSource title\nVolume\nIssue\nArt. No.\nPage start\n...\nISSN\nISBN\nCODEN\nPubMed ID\nLanguage of Original Document\nDocument Type\nPublication Stage\nOpen Access\nSource\nEID\n\n\n\n\n0\nAl Asady, A.; Anokhin, S.\nAl Asady, Ahmad (57219984746); Anokhin, Sergey...\n57219984746; 24482882200\nThe Trojan horse of international entrepreneur...\n2025\nJournal of World Business\n60\n6\n101677.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nNaN\nScopus\n2-s2.0-105014957115\n\n\n1\nThams, Y.; Dau, L.A.; Doh, J.; Kostova, T.; Ne...\nThams, Yannick (55357149800); Dau, Luis Alfons...\n55357149800; 35147597100; 7003920280; 66037741...\nPolitical ideology and the multinational enter...\n2025\nJournal of World Business\n60\n6\n101678.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nNaN\nScopus\n2-s2.0-105014844629\n\n\n2\nLindner, T.; Puck, J.; Puhr, H.\nLindner, Thomas (57159151000); Puck, Jonas (85...\n57159151000; 8563161700; 57223389639\nArtificial intelligence in international busin...\n2025\nJournal of World Business\n60\n6\n101676.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105014595041\n\n\n3\nBruton, G.D.; Mejía-Morelos, J.H.; Ahlstrom, D.\nBruton, Garry D. (6603867202); Mejía-Morelos, ...\n6603867202; 55748855800; 56525447800\nMultinational corporations and inclusive suppl...\n2025\nJournal of World Business\n60\n6\n101663.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013512235\n\n\n4\nLiang, Y.; Giroud, A.; Rygh, A.; Chen, Z.\nLiang, Yanze (57223851564); Giroud, Axèle L.A....\n57223851564; 7003496253; 37117826800; 58631386600\nPolitical embeddedness and post-acquisition in...\n2025\nJournal of World Business\n60\n6\n101665.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013485759\n\n\n\n\n5 rows × 41 columns\n\n\n\n\nfrom bertopic import BERTopic\n\ntopic_model = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True)\ntopics, probs = topic_model.fit_transform(data['Abstract'].astype(str))\n\n2025-10-23 17:11:53,360 - BERTopic - Embedding - Transforming documents to embeddings.\n\n\n\n\n\n2025-10-23 17:12:00,134 - BERTopic - Embedding - Completed ✓\n2025-10-23 17:12:00,134 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n2025-10-23 17:12:01,178 - BERTopic - Dimensionality - Completed ✓\n2025-10-23 17:12:01,178 - BERTopic - Cluster - Start clustering the reduced embeddings\n2025-10-23 17:12:01,241 - BERTopic - Cluster - Completed ✓\n2025-10-23 17:12:01,247 - BERTopic - Representation - Fine-tuning topics using representation models.\n2025-10-23 17:12:01,341 - BERTopic - Representation - Completed ✓\n\n\n\nimport pickle \n\n# with open('BERTopic.pkl', 'wb') as f:\n#     pickle.dump(topic_model, f)\n\n\ntopic_model.probabilities_\n\narray([[3.23082954e-003, 2.91713947e-003, 2.50626535e-003, ...,\n        3.58725931e-003, 6.39792817e-003, 5.32129467e-003],\n       [1.18520916e-002, 1.41792954e-002, 8.30114966e-003, ...,\n        1.46853968e-002, 5.49580148e-001, 1.15432380e-002],\n       [3.72792191e-003, 4.03285592e-003, 2.30451978e-003, ...,\n        4.67870971e-003, 5.85715197e-003, 1.79971795e-003],\n       ...,\n       [5.10759927e-308, 8.63793925e-308, 3.58580602e-308, ...,\n        7.47138405e-308, 1.12136664e-307, 4.51913763e-308],\n       [5.00693961e-308, 8.18526070e-308, 3.54750880e-308, ...,\n        7.20819145e-308, 1.08634144e-307, 4.50757083e-308],\n       [1.29076806e-002, 2.03127312e-002, 9.26260920e-003, ...,\n        1.85100705e-002, 3.01571833e-002, 1.22287019e-002]])\n\n\n\nprobs\n\narray([[3.23082954e-003, 2.91713947e-003, 2.50626535e-003, ...,\n        3.58725931e-003, 6.39792817e-003, 5.32129467e-003],\n       [1.18520916e-002, 1.41792954e-002, 8.30114966e-003, ...,\n        1.46853968e-002, 5.49580148e-001, 1.15432380e-002],\n       [3.72792191e-003, 4.03285592e-003, 2.30451978e-003, ...,\n        4.67870971e-003, 5.85715197e-003, 1.79971795e-003],\n       ...,\n       [5.10759927e-308, 8.63793925e-308, 3.58580602e-308, ...,\n        7.47138405e-308, 1.12136664e-307, 4.51913763e-308],\n       [5.00693961e-308, 8.18526070e-308, 3.54750880e-308, ...,\n        7.20819145e-308, 1.08634144e-307, 4.50757083e-308],\n       [1.29076806e-002, 2.03127312e-002, 9.26260920e-003, ...,\n        1.85100705e-002, 3.01571833e-002, 1.22287019e-002]])\n\n\n\ntopics\n\n[-1,\n 30,\n -1,\n -1,\n 24,\n -1,\n 1,\n 5,\n -1,\n -1,\n -1,\n -1,\n -1,\n 5,\n 0,\n 8,\n -1,\n 5,\n 9,\n 10,\n 9,\n 3,\n -1,\n 30,\n -1,\n -1,\n -1,\n -1,\n 19,\n -1,\n -1,\n 8,\n 8,\n -1,\n -1,\n -1,\n -1,\n 9,\n 1,\n 9,\n 30,\n 5,\n -1,\n 29,\n 12,\n -1,\n -1,\n -1,\n 1,\n -1,\n 8,\n 3,\n 1,\n 17,\n -1,\n 10,\n 4,\n 10,\n -1,\n 1,\n 1,\n 18,\n -1,\n 19,\n -1,\n 11,\n 3,\n 8,\n 0,\n 18,\n 18,\n 18,\n 30,\n 17,\n -1,\n -1,\n 18,\n 2,\n 9,\n 8,\n 30,\n 9,\n 18,\n 8,\n -1,\n 0,\n -1,\n 9,\n 5,\n 5,\n 28,\n 8,\n -1,\n -1,\n 2,\n -1,\n 19,\n -1,\n -1,\n 0,\n 18,\n -1,\n -1,\n -1,\n -1,\n 23,\n 8,\n 17,\n 18,\n -1,\n -1,\n -1,\n 9,\n -1,\n 9,\n 1,\n 30,\n 5,\n -1,\n 5,\n 28,\n -1,\n 2,\n 8,\n -1,\n -1,\n 8,\n 8,\n 9,\n -1,\n -1,\n -1,\n 12,\n -1,\n -1,\n 16,\n 8,\n -1,\n 24,\n -1,\n 6,\n -1,\n 9,\n 22,\n 10,\n -1,\n 10,\n 6,\n 9,\n -1,\n -1,\n 3,\n -1,\n -1,\n -1,\n 9,\n -1,\n -1,\n 3,\n -1,\n 10,\n 0,\n 0,\n 0,\n 23,\n 0,\n 1,\n -1,\n 24,\n -1,\n -1,\n 27,\n 4,\n 3,\n -1,\n -1,\n 27,\n 24,\n -1,\n 5,\n 9,\n -1,\n -1,\n 0,\n 1,\n -1,\n 18,\n 8,\n -1,\n 1,\n 28,\n -1,\n 9,\n 13,\n -1,\n -1,\n -1,\n 5,\n -1,\n 21,\n 17,\n 1,\n 18,\n 27,\n 1,\n 8,\n 9,\n 1,\n -1,\n -1,\n 0,\n 9,\n 2,\n -1,\n -1,\n 1,\n -1,\n -1,\n -1,\n -1,\n 24,\n 14,\n -1,\n -1,\n 5,\n 23,\n 6,\n 5,\n 1,\n -1,\n -1,\n 3,\n -1,\n -1,\n -1,\n 27,\n 17,\n -1,\n -1,\n -1,\n -1,\n 1,\n -1,\n 18,\n -1,\n -1,\n 1,\n 10,\n 10,\n -1,\n -1,\n -1,\n -1,\n 10,\n 9,\n 8,\n 1,\n 17,\n 23,\n 17,\n 29,\n -1,\n -1,\n 1,\n 5,\n -1,\n 5,\n -1,\n -1,\n -1,\n -1,\n -1,\n 13,\n 19,\n 17,\n -1,\n -1,\n -1,\n -1,\n -1,\n 29,\n 30,\n -1,\n 18,\n 1,\n 10,\n -1,\n 5,\n 27,\n 3,\n 9,\n 27,\n -1,\n 10,\n 9,\n -1,\n -1,\n 0,\n 0,\n -1,\n -1,\n -1,\n 29,\n 9,\n -1,\n -1,\n -1,\n 16,\n 5,\n 16,\n 1,\n 1,\n 27,\n -1,\n 1,\n 20,\n 3,\n -1,\n -1,\n 23,\n 26,\n 27,\n -1,\n 3,\n 8,\n -1,\n -1,\n 10,\n 20,\n -1,\n 11,\n 19,\n 11,\n 11,\n -1,\n -1,\n -1,\n 3,\n 17,\n 0,\n 10,\n 0,\n 6,\n 23,\n 3,\n 5,\n -1,\n 11,\n 12,\n -1,\n -1,\n 0,\n -1,\n 5,\n 28,\n -1,\n -1,\n -1,\n -1,\n -1,\n 16,\n -1,\n 13,\n 0,\n -1,\n 8,\n -1,\n 10,\n 26,\n -1,\n 5,\n 0,\n 3,\n -1,\n 10,\n 10,\n 20,\n 11,\n 10,\n -1,\n 10,\n -1,\n 6,\n 0,\n 24,\n 26,\n -1,\n 3,\n 11,\n 26,\n -1,\n 4,\n 0,\n 0,\n 8,\n -1,\n -1,\n 8,\n -1,\n 23,\n -1,\n -1,\n -1,\n 3,\n -1,\n 13,\n 18,\n -1,\n -1,\n 19,\n 8,\n 6,\n 27,\n -1,\n -1,\n -1,\n 11,\n 10,\n 5,\n -1,\n -1,\n -1,\n -1,\n -1,\n -1,\n 6,\n 8,\n 21,\n -1,\n -1,\n -1,\n 11,\n 24,\n -1,\n -1,\n -1,\n -1,\n 30,\n 8,\n 4,\n 5,\n 5,\n 2,\n 5,\n 11,\n -1,\n -1,\n -1,\n -1,\n -1,\n 4,\n -1,\n -1,\n 30,\n -1,\n -1,\n 18,\n -1,\n 0,\n -1,\n 0,\n 20,\n 17,\n 6,\n 1,\n 17,\n -1,\n 27,\n 6,\n 3,\n -1,\n -1,\n 11,\n 25,\n 2,\n -1,\n 17,\n 16,\n 24,\n 20,\n -1,\n -1,\n -1,\n -1,\n -1,\n -1,\n 11,\n -1,\n 11,\n 11,\n -1,\n 5,\n 10,\n -1,\n 0,\n -1,\n -1,\n 24,\n 10,\n -1,\n -1,\n -1,\n 25,\n -1,\n -1,\n 24,\n 11,\n 5,\n -1,\n 0,\n -1,\n 10,\n -1,\n 18,\n 4,\n 20,\n 8,\n 18,\n -1,\n -1,\n -1,\n 2,\n -1,\n -1,\n 10,\n -1,\n 3,\n 5,\n 1,\n -1,\n 16,\n 7,\n 12,\n -1,\n 3,\n -1,\n 6,\n 27,\n 23,\n 24,\n 11,\n 16,\n -1,\n 10,\n -1,\n 8,\n 12,\n 8,\n 2,\n 8,\n -1,\n 4,\n 11,\n 8,\n -1,\n 8,\n 1,\n 16,\n -1,\n 16,\n -1,\n -1,\n 9,\n -1,\n 11,\n 0,\n 3,\n -1,\n -1,\n -1,\n -1,\n 1,\n 3,\n 1,\n 16,\n -1,\n -1,\n 13,\n 0,\n -1,\n 6,\n 1,\n -1,\n 20,\n 24,\n 24,\n 29,\n -1,\n -1,\n -1,\n 1,\n 30,\n -1,\n 0,\n -1,\n 4,\n 16,\n 20,\n 28,\n 0,\n 19,\n -1,\n -1,\n -1,\n 2,\n -1,\n 13,\n -1,\n -1,\n 13,\n -1,\n 30,\n 25,\n 0,\n -1,\n -1,\n 0,\n 2,\n 30,\n 0,\n 26,\n -1,\n 17,\n 4,\n -1,\n 11,\n -1,\n 17,\n 20,\n 10,\n 8,\n 20,\n -1,\n 3,\n 19,\n 5,\n -1,\n 20,\n -1,\n -1,\n 13,\n -1,\n 2,\n 3,\n -1,\n 0,\n 7,\n 12,\n 12,\n 12,\n 12,\n 12,\n 12,\n 12,\n -1,\n 12,\n 12,\n 12,\n 12,\n 2,\n 3,\n -1,\n 6,\n 11,\n -1,\n -1,\n 29,\n -1,\n 29,\n 29,\n 9,\n -1,\n 0,\n 29,\n 26,\n -1,\n 0,\n -1,\n -1,\n 5,\n 10,\n -1,\n 10,\n -1,\n 0,\n -1,\n -1,\n 1,\n -1,\n -1,\n 6,\n 1,\n 19,\n 0,\n 16,\n -1,\n -1,\n -1,\n -1,\n 18,\n 13,\n -1,\n 16,\n 13,\n 4,\n 18,\n -1,\n -1,\n 20,\n 13,\n 22,\n -1,\n 3,\n 7,\n 2,\n 7,\n 7,\n 2,\n 7,\n 2,\n 7,\n 12,\n 5,\n 0,\n 17,\n 3,\n 6,\n 4,\n 5,\n -1,\n -1,\n 0,\n 9,\n -1,\n -1,\n 3,\n -1,\n -1,\n 2,\n -1,\n 4,\n 6,\n 4,\n 2,\n 4,\n 4,\n 4,\n 4,\n 4,\n 4,\n 4,\n 4,\n 4,\n -1,\n 1,\n 4,\n 16,\n 4,\n 4,\n 26,\n 0,\n 11,\n 18,\n -1,\n -1,\n 23,\n 0,\n -1,\n -1,\n 3,\n -1,\n -1,\n -1,\n 7,\n -1,\n -1,\n 7,\n -1,\n 20,\n -1,\n -1,\n 1,\n 28,\n -1,\n 25,\n 7,\n 25,\n 26,\n -1,\n -1,\n 0,\n 6,\n 16,\n 3,\n 7,\n -1,\n 20,\n -1,\n -1,\n 22,\n -1,\n 30,\n 16,\n -1,\n 16,\n 26,\n -1,\n -1,\n 12,\n 6,\n 6,\n 9,\n -1,\n -1,\n -1,\n 27,\n 9,\n -1,\n 17,\n 17,\n -1,\n 1,\n 1,\n 17,\n -1,\n -1,\n 17,\n 17,\n -1,\n 17,\n 20,\n 19,\n 22,\n 6,\n 0,\n -1,\n 4,\n -1,\n -1,\n -1,\n 0,\n 9,\n 16,\n -1,\n 7,\n 8,\n 2,\n 29,\n -1,\n -1,\n 4,\n 12,\n 14,\n 0,\n 23,\n 7,\n -1,\n 6,\n 13,\n -1,\n 13,\n -1,\n 13,\n 13,\n 13,\n 13,\n 13,\n -1,\n -1,\n -1,\n 4,\n 5,\n 19,\n 1,\n 19,\n 5,\n -1,\n -1,\n 26,\n 13,\n -1,\n 12,\n 12,\n 12,\n 12,\n 12,\n 12,\n 12,\n -1,\n 12,\n 11,\n 7,\n 13,\n 22,\n 15,\n 6,\n 0,\n 2,\n 10,\n -1,\n -1,\n 0,\n -1,\n -1,\n -1,\n -1,\n -1,\n -1,\n 21,\n 4,\n -1,\n 24,\n 20,\n -1,\n 0,\n -1,\n 20,\n -1,\n 25,\n -1,\n -1,\n 26,\n -1,\n -1,\n -1,\n 19,\n -1,\n -1,\n 0,\n 19,\n -1,\n 25,\n 0,\n 20,\n 6,\n -1,\n 31,\n -1,\n 3,\n 7,\n -1,\n -1,\n -1,\n 18,\n -1,\n 4,\n 29,\n 19,\n -1,\n -1,\n 6,\n -1,\n -1,\n -1,\n 21,\n 1,\n 3,\n 1,\n 20,\n 1,\n 1,\n -1,\n 13,\n 1,\n -1,\n 23,\n 11,\n 2,\n 25,\n -1,\n -1,\n 9,\n 2,\n -1,\n 5,\n -1,\n 22,\n 28,\n 0,\n -1,\n 0,\n -1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n -1,\n 2,\n -1,\n -1,\n 25,\n -1,\n 25,\n -1,\n -1,\n 2,\n ...]\n\n\n\nfreq = topic_model.get_topic_info(); freq.head(5)\n\n\n\n\n\n\n\n\nTopic\nCount\nName\nRepresentation\nRepresentative_Docs\n\n\n\n\n0\n-1\n544\n-1_the_of_and_in\n[the, of, and, in, to, that, we, on, this, with]\n[In recent years, there has been an increasing...\n\n\n1\n0\n61\n0_knowledge_transfer_and_the\n[knowledge, transfer, and, the, of, on, to, th...\n[This paper proposes a conceptual framework de...\n\n\n2\n1\n52\n1_international_entrepreneurial_internationali...\n[international, entrepreneurial, international...\n[Grounded in the resource-based view of the fi...\n\n\n3\n2\n43\n2_career_expatriates_expatriate_assignments\n[career, expatriates, expatriate, assignments,...\n[Creating organizational processes which nurtu...\n\n\n4\n3\n38\n3_acquisitions_acquisition_crossborder_acquirers\n[acquisitions, acquisition, crossborder, acqui...\n[This study develops and tests a framework abo..."
  }
]