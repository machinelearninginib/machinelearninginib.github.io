[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Perspective Paper",
    "section": "",
    "text": "This is the online appendix to PAPER NAME.",
    "crumbs": [
      "Home",
      "Machine Learning Perspective Paper"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site",
    "crumbs": [
      "Home",
      "Introduction",
      "About"
    ]
  },
  {
    "objectID": "methods/01-topic-modeling.html",
    "href": "methods/01-topic-modeling.html",
    "title": "Topic modeling",
    "section": "",
    "text": "This is a snapshot of the data we will be working with.\n\nimport pandas as pd\ndata = pd.read_csv('../data/jwb-articles.csv')\ndata = data[data['Abstract'].notna()] # Keep nonempty abstracts\ndata.head()\n\n\n\n\n\n\n\n\nAuthors\nAuthor full names\nAuthor(s) ID\nTitle\nYear\nSource title\nVolume\nIssue\nArt. No.\nPage start\n...\nISSN\nISBN\nCODEN\nPubMed ID\nLanguage of Original Document\nDocument Type\nPublication Stage\nOpen Access\nSource\nEID\n\n\n\n\n0\nAl Asady, A.; Anokhin, S.\nAl Asady, Ahmad (57219984746); Anokhin, Sergey...\n57219984746; 24482882200\nThe Trojan horse of international entrepreneur...\n2025\nJournal of World Business\n60\n6\n101677.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nNaN\nScopus\n2-s2.0-105014957115\n\n\n1\nThams, Y.; Dau, L.A.; Doh, J.; Kostova, T.; Ne...\nThams, Yannick (55357149800); Dau, Luis Alfons...\n55357149800; 35147597100; 7003920280; 66037741...\nPolitical ideology and the multinational enter...\n2025\nJournal of World Business\n60\n6\n101678.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nNaN\nScopus\n2-s2.0-105014844629\n\n\n2\nLindner, T.; Puck, J.; Puhr, H.\nLindner, Thomas (57159151000); Puck, Jonas (85...\n57159151000; 8563161700; 57223389639\nArtificial intelligence in international busin...\n2025\nJournal of World Business\n60\n6\n101676.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105014595041\n\n\n3\nBruton, G.D.; Mejía-Morelos, J.H.; Ahlstrom, D.\nBruton, Garry D. (6603867202); Mejía-Morelos, ...\n6603867202; 55748855800; 56525447800\nMultinational corporations and inclusive suppl...\n2025\nJournal of World Business\n60\n6\n101663.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013512235\n\n\n4\nLiang, Y.; Giroud, A.; Rygh, A.; Chen, Z.\nLiang, Yanze (57223851564); Giroud, Axèle L.A....\n57223851564; 7003496253; 37117826800; 58631386600\nPolitical embeddedness and post-acquisition in...\n2025\nJournal of World Business\n60\n6\n101665.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013485759\n\n\n\n\n5 rows × 41 columns",
    "crumbs": [
      "Home",
      "Methods",
      "Topic modeling"
    ]
  },
  {
    "objectID": "methods/01-topic-modeling.html#data",
    "href": "methods/01-topic-modeling.html#data",
    "title": "Topic modeling",
    "section": "",
    "text": "This is a snapshot of the data we will be working with.\n\nimport pandas as pd\ndata = pd.read_csv('../data/jwb-articles.csv')\ndata = data[data['Abstract'].notna()] # Keep nonempty abstracts\ndata.head()\n\n\n\n\n\n\n\n\nAuthors\nAuthor full names\nAuthor(s) ID\nTitle\nYear\nSource title\nVolume\nIssue\nArt. No.\nPage start\n...\nISSN\nISBN\nCODEN\nPubMed ID\nLanguage of Original Document\nDocument Type\nPublication Stage\nOpen Access\nSource\nEID\n\n\n\n\n0\nAl Asady, A.; Anokhin, S.\nAl Asady, Ahmad (57219984746); Anokhin, Sergey...\n57219984746; 24482882200\nThe Trojan horse of international entrepreneur...\n2025\nJournal of World Business\n60\n6\n101677.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nNaN\nScopus\n2-s2.0-105014957115\n\n\n1\nThams, Y.; Dau, L.A.; Doh, J.; Kostova, T.; Ne...\nThams, Yannick (55357149800); Dau, Luis Alfons...\n55357149800; 35147597100; 7003920280; 66037741...\nPolitical ideology and the multinational enter...\n2025\nJournal of World Business\n60\n6\n101678.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nNaN\nScopus\n2-s2.0-105014844629\n\n\n2\nLindner, T.; Puck, J.; Puhr, H.\nLindner, Thomas (57159151000); Puck, Jonas (85...\n57159151000; 8563161700; 57223389639\nArtificial intelligence in international busin...\n2025\nJournal of World Business\n60\n6\n101676.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105014595041\n\n\n3\nBruton, G.D.; Mejía-Morelos, J.H.; Ahlstrom, D.\nBruton, Garry D. (6603867202); Mejía-Morelos, ...\n6603867202; 55748855800; 56525447800\nMultinational corporations and inclusive suppl...\n2025\nJournal of World Business\n60\n6\n101663.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013512235\n\n\n4\nLiang, Y.; Giroud, A.; Rygh, A.; Chen, Z.\nLiang, Yanze (57223851564); Giroud, Axèle L.A....\n57223851564; 7003496253; 37117826800; 58631386600\nPolitical embeddedness and post-acquisition in...\n2025\nJournal of World Business\n60\n6\n101665.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013485759\n\n\n\n\n5 rows × 41 columns",
    "crumbs": [
      "Home",
      "Methods",
      "Topic modeling"
    ]
  },
  {
    "objectID": "data/jwb-scrape.html",
    "href": "data/jwb-scrape.html",
    "title": "Machine Learning Perspective Paper",
    "section": "",
    "text": "from bs4 import BeautifulSoup\nimport pandas as pd\n\n\nresults = {}\nfor issue in range(1, 7):\n    site = open(f'jwb-data/{issue:02d}.html', 'r', encoding='utf-8').read()\n    soup = BeautifulSoup(site)\n    articles = soup.find_all('dl')[1:]\n    for paper in range(len(articles)):\n        article_info = {}\n        article_info['title'] = articles[paper].find('span', {'class': 'js-article-title text-l'}).text\n        article_info['abstract'] = articles[paper].find_all('p')[-1].text\n        results[f'{issue}-{paper}'] = article_info\n\n\npd.DataFrame(results).T.to_csv('jwb-vol60.csv', index=False)"
  },
  {
    "objectID": "methods/01-topic-modeling.html#latent-dirichlet-allocation-lda",
    "href": "methods/01-topic-modeling.html#latent-dirichlet-allocation-lda",
    "title": "Topic modeling",
    "section": "Latent Dirichlet Allocation (LDA)",
    "text": "Latent Dirichlet Allocation (LDA)\nWe first remove stop words and lemmatize the text.\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import RegexpTokenizer\n\ntokenizer = RegexpTokenizer(r'\\w+')\nstop_words = stopwords.words('english')\n\ndocs = []\nfor abstract in data['Abstract']:\n    tokens = word_tokenize(abstract.lower())\n    tokens = tokenizer.tokenize(' '.join(tokens))\n    rm_stop_words = [word for word in tokens if word not in stop_words]\n    docs.append(rm_stop_words)\n\nBefore we fit the LDA model, we construct a dictionary and convert our text to a bag of words.\n\nimport gensim\nfrom gensim.models.ldamodel import LdaModel\nfrom gensim import corpora\n\nlda_dict = corpora.Dictionary(docs)\nprint('The number of unique words:', len(lda_dict))\nprint(lda_dict)\n\nThe number of unique words: 8944\nDictionary&lt;8944 unique tokens: ['activities', 'affect', 'aims', 'also', 'argues']...&gt;\n\n\n\nlda_doc_corpus = [lda_dict.doc2bow(word) for word in docs]\nprint(lda_doc_corpus[0])\n\n[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2), (6, 1), (7, 2), (8, 3), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 2), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 3), (25, 2), (26, 2), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 2), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1)]\n\n\nNow we run LDA.\nThe LDA model hyperparameters alpha and eta are important for outcomes.\n\nAlpha is a parameter that controls the prior distribution over topic weights in each document.\nEta is a parameter for the prior distribution over word weights in each topic. Griffiths and Steyvers (2004) suggest a value of 50/k (where k is number of topics) for alpha and 0.1 for eta. Griffiths TL, Steyvers M (2004). “Finding Scientific Topics.” Proceedings of the National Academy of Sciences of the United States of America, 101, 5228–5235.\n\n\nlda = LdaModel(corpus=lda_doc_corpus, id2word=lda_dict, num_topics=5,\n              random_state=42, passes=10, alpha=50/5, eta=0.1)\n\nWe can examine the results of the LDA.\n\nlda.show_topics()\n\n[(0,\n  '0.008*\"international\" + 0.007*\"firms\" + 0.007*\"study\" + 0.006*\"foreign\" + 0.006*\"business\" + 0.006*\"performance\" + 0.005*\"firm\" + 0.005*\"knowledge\" + 0.005*\"internationalization\" + 0.005*\"institutional\"'),\n (1,\n  '0.013*\"international\" + 0.008*\"research\" + 0.007*\"firms\" + 0.007*\"business\" + 0.006*\"study\" + 0.006*\"global\" + 0.005*\"knowledge\" + 0.005*\"performance\" + 0.005*\"countries\" + 0.005*\"based\"'),\n (2,\n  '0.009*\"firms\" + 0.007*\"international\" + 0.007*\"subsidiary\" + 0.006*\"firm\" + 0.006*\"research\" + 0.006*\"global\" + 0.005*\"foreign\" + 0.005*\"business\" + 0.005*\"performance\" + 0.005*\"knowledge\"'),\n (3,\n  '0.008*\"research\" + 0.007*\"study\" + 0.006*\"firms\" + 0.006*\"performance\" + 0.006*\"international\" + 0.005*\"country\" + 0.005*\"foreign\" + 0.005*\"institutional\" + 0.005*\"business\" + 0.005*\"based\"'),\n (4,\n  '0.016*\"firms\" + 0.009*\"international\" + 0.007*\"study\" + 0.007*\"management\" + 0.006*\"performance\" + 0.006*\"firm\" + 0.005*\"research\" + 0.005*\"market\" + 0.005*\"based\" + 0.005*\"business\"')]\n\n\nAnd to visualise the topic probabilities for the first 15 abstracts, we can run the following code.\n\nimport matplotlib.pyplot as plt\nget_document_topics = lda.get_document_topics(lda_doc_corpus)\n\nall_probs = []\n\nfor doc_i in range(15):\n    doc_probs = get_document_topics[doc_i]\n    print(doc_probs)\n    probs = []\n    for (topic, prob) in doc_probs:\n        probs.append(prob)\n    all_probs.append(probs)\n\nplt.imshow(all_probs)\nplt.colorbar()\n\n[(0, 0.18502577), (1, 0.27372837), (2, 0.1858613), (3, 0.17344333), (4, 0.18194126)]\n[(0, 0.123563975), (1, 0.2688862), (2, 0.18882376), (3, 0.29686803), (4, 0.121858045)]\n[(0, 0.16409734), (1, 0.20209381), (2, 0.14940387), (3, 0.24056785), (4, 0.24383715)]\n[(0, 0.16398305), (1, 0.18669577), (2, 0.15503661), (3, 0.15883045), (4, 0.33545414)]\n[(0, 0.22544077), (1, 0.18374273), (2, 0.18284072), (3, 0.17819183), (4, 0.22978392)]\n[(0, 0.15645616), (1, 0.16588609), (2, 0.2596964), (3, 0.23863135), (4, 0.17933)]\n[(0, 0.13678451), (1, 0.20867777), (2, 0.12985952), (3, 0.16370597), (4, 0.3609723)]\n[(0, 0.19213931), (1, 0.13729951), (2, 0.14692119), (3, 0.16687186), (4, 0.35676813)]\n[(0, 0.23286998), (1, 0.2070405), (2, 0.15955229), (3, 0.19480501), (4, 0.20573217)]\n[(0, 0.27796832), (1, 0.19017023), (2, 0.16207576), (3, 0.18041292), (4, 0.1893728)]\n[(0, 0.16712207), (1, 0.1799058), (2, 0.15676005), (3, 0.16918756), (4, 0.3270245)]\n[(0, 0.267809), (1, 0.15632342), (2, 0.19364476), (3, 0.23301464), (4, 0.1492082)]\n[(0, 0.20694813), (1, 0.17432462), (2, 0.20037563), (3, 0.23672153), (4, 0.18163006)]\n[(0, 0.2521271), (1, 0.21680596), (2, 0.16364759), (3, 0.15310885), (4, 0.21431047)]\n[(0, 0.15407574), (1, 0.1787669), (2, 0.16378628), (3, 0.33469108), (4, 0.16867998)]",
    "crumbs": [
      "Home",
      "Methods",
      "Topic modeling"
    ]
  },
  {
    "objectID": "methods/01-topic-modeling.html#dynamic-topic-modeling",
    "href": "methods/01-topic-modeling.html#dynamic-topic-modeling",
    "title": "Topic modeling",
    "section": "Dynamic Topic Modeling",
    "text": "Dynamic Topic Modeling",
    "crumbs": [
      "Home",
      "Methods",
      "Topic modeling"
    ]
  },
  {
    "objectID": "methods/01-topic-modeling.html#bertopic",
    "href": "methods/01-topic-modeling.html#bertopic",
    "title": "Topic modeling",
    "section": "BERTopic",
    "text": "BERTopic\n\nfrom bertopic import BERTopic\n\n# Get the abstracts from the 'Abstract' column\ndocs = df['Abstract'].tolist()\ndocs = [str(doc) for doc in docs]\n\n# Initialize BERTopic model\ntopic_model = BERTopic(language='english', calculate_probabilities=True, verbose=True)\n# Fit BERTopic to your data\ntopic_model.fit(docs)\n\n\n# Document-topic matrix is directly available in the 'topics' variable\ndoc_topic = topic_model.topics_\nprint('Document-topic matrix:')\n# Print the document-topic matrix\nprint(doc_topic)\n\nDocument-topic matrix:\n[-1, 30, -1, -1, 24, -1, 1, 5, -1, -1, -1, -1, -1, 5, 0, 8, -1, 5, 9, 10, 9, 3, -1, 30, -1, -1, -1, -1, 19, -1, -1, 8, 8, -1, -1, -1, -1, 9, 1, 9, 30, 5, -1, 29, 12, -1, -1, -1, 1, -1, 8, 3, 1, 17, -1, 10, 4, 10, -1, 1, 1, 18, -1, 19, -1, 11, 3, 8, 0, 18, 18, 18, 30, 17, -1, -1, 18, 2, 9, 8, 30, 9, 18, 8, -1, 0, -1, 9, 5, 5, 28, 8, -1, -1, 2, -1, 19, -1, -1, 0, 18, -1, -1, -1, -1, 23, 8, 17, 18, -1, -1, -1, 9, -1, 9, 1, 30, 5, -1, 5, 28, -1, 2, 8, -1, -1, 8, 8, 9, -1, -1, -1, 12, -1, -1, 16, 8, -1, 24, -1, 6, -1, 9, 22, 10, -1, 10, 6, 9, -1, -1, 3, -1, -1, -1, 9, -1, -1, 3, -1, 10, 0, 0, 0, 23, 0, 1, -1, 24, -1, -1, 27, 4, 3, -1, -1, 27, 24, -1, 5, 9, -1, -1, 0, 1, -1, 18, 8, -1, 1, 28, -1, 9, 13, -1, -1, -1, 5, -1, 21, 17, 1, 18, 27, 1, 8, 9, 1, -1, -1, 0, 9, 2, -1, -1, 1, -1, -1, -1, -1, 24, 14, -1, -1, 5, 23, 6, 5, 1, -1, -1, 3, -1, -1, -1, 27, 17, -1, -1, -1, -1, 1, -1, 18, -1, -1, 1, 10, 10, -1, -1, -1, -1, 10, 9, 8, 1, 17, 23, 17, 29, -1, -1, 1, 5, -1, 5, -1, -1, -1, -1, -1, 13, 19, 17, -1, -1, -1, -1, -1, 29, 30, -1, 18, 1, 10, -1, 5, 27, 3, 9, 27, -1, 10, 9, -1, -1, 0, 0, -1, -1, -1, 29, 9, -1, -1, -1, 16, 5, 16, 1, 1, 27, -1, 1, 20, 3, -1, -1, 23, 26, 27, -1, 3, 8, -1, -1, 10, 20, -1, 11, 19, 11, 11, -1, -1, -1, 3, 17, 0, 10, 0, 6, 23, 3, 5, -1, 11, 12, -1, -1, 0, -1, 5, 28, -1, -1, -1, -1, -1, 16, -1, 13, 0, -1, 8, -1, 10, 26, -1, 5, 0, 3, -1, 10, 10, 20, 11, 10, -1, 10, -1, 6, 0, 24, 26, -1, 3, 11, 26, -1, 4, 0, 0, 8, -1, -1, 8, -1, 23, -1, -1, -1, 3, -1, 13, 18, -1, -1, 19, 8, 6, 27, -1, -1, -1, 11, 10, 5, -1, -1, -1, -1, -1, -1, 6, 8, 21, -1, -1, -1, 11, 24, -1, -1, -1, -1, 30, 8, 4, 5, 5, 2, 5, 11, -1, -1, -1, -1, -1, 4, -1, -1, 30, -1, -1, 18, -1, 0, -1, 0, 20, 17, 6, 1, 17, -1, 27, 6, 3, -1, -1, 11, 25, 2, -1, 17, 16, 24, 20, -1, -1, -1, -1, -1, -1, 11, -1, 11, 11, -1, 5, 10, -1, 0, -1, -1, 24, 10, -1, -1, -1, 25, -1, -1, 24, 11, 5, -1, 0, -1, 10, -1, 18, 4, 20, 8, 18, -1, -1, -1, 2, -1, -1, 10, -1, 3, 5, 1, -1, 16, 7, 12, -1, 3, -1, 6, 27, 23, 24, 11, 16, -1, 10, -1, 8, 12, 8, 2, 8, -1, 4, 11, 8, -1, 8, 1, 16, -1, 16, -1, -1, 9, -1, 11, 0, 3, -1, -1, -1, -1, 1, 3, 1, 16, -1, -1, 13, 0, -1, 6, 1, -1, 20, 24, 24, 29, -1, -1, -1, 1, 30, -1, 0, -1, 4, 16, 20, 28, 0, 19, -1, -1, -1, 2, -1, 13, -1, -1, 13, -1, 30, 25, 0, -1, -1, 0, 2, 30, 0, 26, -1, 17, 4, -1, 11, -1, 17, 20, 10, 8, 20, -1, 3, 19, 5, -1, 20, -1, -1, 13, -1, 2, 3, -1, 0, 7, 12, 12, 12, 12, 12, 12, 12, -1, 12, 12, 12, 12, 2, 3, -1, 6, 11, -1, -1, 29, -1, 29, 29, 9, -1, 0, 29, 26, -1, 0, -1, -1, 5, 10, -1, 10, -1, 0, -1, -1, 1, -1, -1, 6, 1, 19, 0, 16, -1, -1, -1, -1, 18, 13, -1, 16, 13, 4, 18, -1, -1, 20, 13, 22, -1, 3, 7, 2, 7, 7, 2, 7, 2, 7, 12, 5, 0, 17, 3, 6, 4, 5, -1, -1, 0, 9, -1, -1, 3, -1, -1, 2, -1, 4, 6, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, -1, 1, 4, 16, 4, 4, 26, 0, 11, 18, -1, -1, 23, 0, -1, -1, 3, -1, -1, -1, 7, -1, -1, 7, -1, 20, -1, -1, 1, 28, -1, 25, 7, 25, 26, -1, -1, 0, 6, 16, 3, 7, -1, 20, -1, -1, 22, -1, 30, 16, -1, 16, 26, -1, -1, 12, 6, 6, 9, -1, -1, -1, 27, 9, -1, 17, 17, -1, 1, 1, 17, -1, -1, 17, 17, -1, 17, 20, 19, 22, 6, 0, -1, 4, -1, -1, -1, 0, 9, 16, -1, 7, 8, 2, 29, -1, -1, 4, 12, 14, 0, 23, 7, -1, 6, 13, -1, 13, -1, 13, 13, 13, 13, 13, -1, -1, -1, 4, 5, 19, 1, 19, 5, -1, -1, 26, 13, -1, 12, 12, 12, 12, 12, 12, 12, -1, 12, 11, 7, 13, 22, 15, 6, 0, 2, 10, -1, -1, 0, -1, -1, -1, -1, -1, -1, 21, 4, -1, 24, 20, -1, 0, -1, 20, -1, 25, -1, -1, 26, -1, -1, -1, 19, -1, -1, 0, 19, -1, 25, 0, 20, 6, -1, 31, -1, 3, 7, -1, -1, -1, 18, -1, 4, 29, 19, -1, -1, 6, -1, -1, -1, 21, 1, 3, 1, 20, 1, 1, -1, 13, 1, -1, 23, 11, 2, 25, -1, -1, 9, 2, -1, 5, -1, 22, 28, 0, -1, 0, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 2, -1, -1, 25, -1, 25, -1, -1, 2, -1, 31, 11, 21, 7, -1, 7, 5, -1, 25, -1, -1, 6, 10, 22, -1, 26, -1, 21, 3, -1, -1, -1, -1, -1, 25, 2, -1, -1, -1, -1, 6, -1, -1, -1, 2, 2, 2, 2, 0, 2, -1, 23, 26, -1, -1, -1, 4, 6, 7, 23, 8, -1, -1, -1, 0, -1, 3, -1, 21, 2, 2, -1, 7, 7, 7, -1, -1, -1, 17, 7, 7, -1, 7, -1, 29, -1, 0, 16, -1, -1, -1, -1, 21, 0, -1, -1, -1, 7, -1, 7, -1, 6, 11, 31, 15, 31, 11, 31, 31, -1, -1, -1, 9, 11, 28, -1, -1, -1, -1, 0, -1, 0, -1, 2, 0, 25, 0, 28, -1, 0, -1, 0, 15, -1, 2, 29, -1, -1, 28, -1, -1, 2, -1, -1, 2, -1, 21, 13, 4, -1, 4, -1, -1, -1, -1, -1, 2, 2, -1, 7, 7, 6, 25, -1, -1, -1, -1, 9, 21, 14, 7, 22, -1, 3, 3, 3, 3, -1, 6, 19, 3, 22, -1, 4, -1, -1, -1, 22, 22, 19, 1, 1, -1, -1, -1, 31, -1, 23, 31, -1, 16, -1, -1, 22, -1, 13, 21, -1, -1, 7, 14, -1, 21, 21, -1, -1, 7, 21, -1, 22, 21, -1, 7, -1, 24, -1, -1, -1, -1, 23, -1, 16, -1, -1, -1, 4, -1, 16, 6, -1, 2, -1, 11, 21, 2, 6, 21, 23, 2, 2, 31, -1, -1, -1, -1, -1, 6, -1, 2, 7, 21, -1, 2, 6, -1, -1, 6, 28, -1, -1, -1, -1, 14, -1, -1, -1, -1, 14, -1, -1, 14, -1, 14, 13, 22, 5, -1, -1, -1, -1, 7, -1, -1, -1, 19, -1, -1, 9, -1, 15, -1, 28, 15, 13, 14, -1, 14, 8, 9, 14, 15, -1, 14, -1, 14, -1, 31, 14, 14, 9, 14, 6, -1, -1, -1, 14, 14, -1, 14, -1, 27, 14, 14, 14, 16, 14, 14, 14, 3, 15, 15, 28, 15, -1, -1, 15, 15, 15, 15, -1, -1, 15, 15, 15, 15, 15, 15, 15, -1, 15, 19, -1, 15, 15, 22, 22, 22]\n\n\n\n# Get probabilities for each topic\nprobs = topic_model.probabilities_\nprint('Topic probabilities for the first document:')\nprint(probs[0].round(2))\nprint()\n# Print topic probabilities for the first 15 documents\nfor i in range(min(15, len(docs))):\n    print(f'Document {i + 1} is in topic {doc_topic[i]}')\n    print(f'Topic probabilities for Document {i + 1}:')\n    print(probs[i].round(3))\n    print()\n\nTopic probabilities for the first document:\n[0.   0.   0.   0.   0.   0.   0.   0.   0.   0.01 0.01 0.01 0.   0.\n 0.   0.   0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.01\n 0.   0.   0.01 0.01]\n\nDocument 1 is in topic -1\nTopic probabilities for Document 1:\n[0.003 0.003 0.003 0.004 0.002 0.003 0.003 0.003 0.003 0.006 0.007 0.007\n 0.003 0.004 0.005 0.004 0.005 0.002 0.004 0.004 0.002 0.004 0.003 0.003\n 0.003 0.003 0.004 0.008 0.003 0.004 0.006 0.005]\n\nDocument 2 is in topic 30\nTopic probabilities for Document 2:\n[0.012 0.014 0.008 0.013 0.007 0.017 0.011 0.011 0.02  0.037 0.024 0.017\n 0.008 0.016 0.015 0.01  0.03  0.006 0.009 0.016 0.007 0.011 0.017 0.009\n 0.014 0.009 0.019 0.025 0.012 0.015 0.55  0.012]\n\nDocument 3 is in topic -1\nTopic probabilities for Document 3:\n[0.004 0.004 0.002 0.003 0.002 0.004 0.002 0.003 0.005 0.004 0.003 0.002\n 0.002 0.003 0.002 0.002 0.006 0.002 0.002 0.003 0.002 0.003 0.004 0.002\n 0.005 0.002 0.006 0.004 0.003 0.005 0.006 0.002]\n\nDocument 4 is in topic -1\nTopic probabilities for Document 4:\n[0.003 0.003 0.002 0.004 0.002 0.004 0.003 0.002 0.003 0.007 0.018 0.009\n 0.002 0.005 0.006 0.003 0.007 0.001 0.003 0.005 0.002 0.003 0.004 0.002\n 0.003 0.002 0.004 0.006 0.003 0.003 0.007 0.005]\n\nDocument 5 is in topic 24\nTopic probabilities for Document 5:\n[0.077 0.035 0.014 0.057 0.013 0.023 0.02  0.015 0.017 0.019 0.02  0.016\n 0.012 0.016 0.014 0.011 0.028 0.012 0.012 0.026 0.014 0.027 0.021 0.026\n 0.151 0.016 0.049 0.024 0.033 0.027 0.022 0.012]\n\nDocument 6 is in topic -1\nTopic probabilities for Document 6:\n[0.02  0.021 0.014 0.023 0.012 0.024 0.017 0.018 0.027 0.069 0.047 0.034\n 0.014 0.024 0.028 0.019 0.05  0.01  0.016 0.028 0.011 0.019 0.024 0.016\n 0.022 0.015 0.032 0.057 0.019 0.022 0.124 0.023]\n\nDocument 7 is in topic 1\nTopic probabilities for Document 7:\n[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0.]\n\nDocument 8 is in topic 5\nTopic probabilities for Document 8:\n[0.015 0.042 0.01  0.015 0.009 0.088 0.018 0.01  0.036 0.019 0.018 0.013\n 0.009 0.02  0.013 0.009 0.024 0.009 0.011 0.019 0.01  0.012 0.031 0.012\n 0.024 0.01  0.018 0.016 0.014 0.017 0.024 0.011]\n\nDocument 9 is in topic -1\nTopic probabilities for Document 9:\n[0.028 0.122 0.014 0.031 0.014 0.056 0.027 0.015 0.028 0.025 0.025 0.019\n 0.013 0.025 0.016 0.012 0.034 0.013 0.015 0.026 0.012 0.021 0.033 0.02\n 0.066 0.015 0.032 0.024 0.024 0.03  0.03  0.014]\n\nDocument 10 is in topic -1\nTopic probabilities for Document 10:\n[0.013 0.015 0.007 0.019 0.007 0.017 0.016 0.009 0.014 0.026 0.125 0.052\n 0.007 0.02  0.028 0.016 0.038 0.006 0.012 0.03  0.009 0.013 0.02  0.011\n 0.016 0.008 0.018 0.031 0.014 0.013 0.028 0.023]\n\nDocument 11 is in topic -1\nTopic probabilities for Document 11:\n[0.01  0.016 0.006 0.012 0.005 0.016 0.01  0.007 0.016 0.029 0.021 0.012\n 0.006 0.012 0.011 0.007 0.04  0.005 0.007 0.015 0.005 0.008 0.014 0.007\n 0.014 0.006 0.015 0.018 0.009 0.01  0.023 0.008]\n\nDocument 12 is in topic -1\nTopic probabilities for Document 12:\n[0.014 0.012 0.006 0.05  0.006 0.01  0.011 0.007 0.009 0.012 0.018 0.014\n 0.006 0.01  0.01  0.008 0.019 0.005 0.007 0.018 0.006 0.014 0.011 0.011\n 0.017 0.007 0.018 0.023 0.015 0.011 0.014 0.01 ]\n\nDocument 13 is in topic -1\nTopic probabilities for Document 13:\n[0.026 0.027 0.012 0.031 0.011 0.027 0.019 0.015 0.026 0.041 0.04  0.026\n 0.012 0.021 0.022 0.014 0.151 0.01  0.012 0.043 0.012 0.02  0.027 0.017\n 0.035 0.014 0.055 0.054 0.023 0.024 0.049 0.017]\n\nDocument 14 is in topic 5\nTopic probabilities for Document 14:\n[0.016 0.048 0.011 0.017 0.01  0.108 0.02  0.012 0.039 0.021 0.02  0.015\n 0.01  0.022 0.014 0.01  0.026 0.01  0.012 0.021 0.011 0.014 0.034 0.013\n 0.027 0.011 0.02  0.017 0.015 0.018 0.026 0.012]\n\nDocument 15 is in topic 0\nTopic probabilities for Document 15:\n[0.055 0.017 0.017 0.023 0.014 0.014 0.013 0.02  0.013 0.013 0.013 0.011\n 0.014 0.012 0.01  0.008 0.018 0.012 0.009 0.017 0.015 0.026 0.014 0.026\n 0.028 0.021 0.04  0.017 0.032 0.031 0.016 0.009]\n\n\n\n\n# Get the lists of keywords under each topic\ntopic_keywords = topic_model.get_topics()\n\n# Print the lists of keywords for each topic\nfor topic_id, keywords in topic_keywords.items():\n    keywords = [(u, round(v, 3)) for u, v in keywords]\n    print(f'Topic {topic_id}: {keywords}')\n\nWe can also examine the topics more closely.\n\nfreq = topic_model.get_topic_info()\nfreq.head(5) # To see the first 5 topics\n\n\n\n\n\n\n\n\nTopic\nCount\nName\nRepresentation\nRepresentative_Docs\n\n\n\n\n0\n-1\n544\n-1_the_of_and_in\n[the, of, and, in, to, that, we, on, this, with]\n[In recent years, there has been an increasing...\n\n\n1\n0\n61\n0_knowledge_transfer_and_the\n[knowledge, transfer, and, the, of, on, to, th...\n[This paper proposes a conceptual framework de...\n\n\n2\n1\n52\n1_international_entrepreneurial_internationali...\n[international, entrepreneurial, international...\n[Grounded in the resource-based view of the fi...\n\n\n3\n2\n43\n2_career_expatriates_expatriate_assignments\n[career, expatriates, expatriate, assignments,...\n[Creating organizational processes which nurtu...\n\n\n4\n3\n38\n3_acquisitions_acquisition_crossborder_acquirers\n[acquisitions, acquisition, crossborder, acqui...\n[This study develops and tests a framework abo...",
    "crumbs": [
      "Home",
      "Methods",
      "Topic modeling"
    ]
  },
  {
    "objectID": "models/BERTopic.html",
    "href": "models/BERTopic.html",
    "title": "Machine Learning Perspective Paper",
    "section": "",
    "text": "import pandas as pd\ndata = pd.read_csv('../data/jwb-articles.csv')\ndata = data[data['Abstract'].notna()] # Keep nonempty abstracts\ndata.head()\n\n\n\n\n\n\n\n\nAuthors\nAuthor full names\nAuthor(s) ID\nTitle\nYear\nSource title\nVolume\nIssue\nArt. No.\nPage start\n...\nISSN\nISBN\nCODEN\nPubMed ID\nLanguage of Original Document\nDocument Type\nPublication Stage\nOpen Access\nSource\nEID\n\n\n\n\n0\nAl Asady, A.; Anokhin, S.\nAl Asady, Ahmad (57219984746); Anokhin, Sergey...\n57219984746; 24482882200\nThe Trojan horse of international entrepreneur...\n2025\nJournal of World Business\n60\n6\n101677.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nNaN\nScopus\n2-s2.0-105014957115\n\n\n1\nThams, Y.; Dau, L.A.; Doh, J.; Kostova, T.; Ne...\nThams, Yannick (55357149800); Dau, Luis Alfons...\n55357149800; 35147597100; 7003920280; 66037741...\nPolitical ideology and the multinational enter...\n2025\nJournal of World Business\n60\n6\n101678.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nNaN\nScopus\n2-s2.0-105014844629\n\n\n2\nLindner, T.; Puck, J.; Puhr, H.\nLindner, Thomas (57159151000); Puck, Jonas (85...\n57159151000; 8563161700; 57223389639\nArtificial intelligence in international busin...\n2025\nJournal of World Business\n60\n6\n101676.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105014595041\n\n\n3\nBruton, G.D.; Mejía-Morelos, J.H.; Ahlstrom, D.\nBruton, Garry D. (6603867202); Mejía-Morelos, ...\n6603867202; 55748855800; 56525447800\nMultinational corporations and inclusive suppl...\n2025\nJournal of World Business\n60\n6\n101663.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013512235\n\n\n4\nLiang, Y.; Giroud, A.; Rygh, A.; Chen, Z.\nLiang, Yanze (57223851564); Giroud, Axèle L.A....\n57223851564; 7003496253; 37117826800; 58631386600\nPolitical embeddedness and post-acquisition in...\n2025\nJournal of World Business\n60\n6\n101665.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013485759\n\n\n\n\n5 rows × 41 columns\n\n\n\n\nfrom bertopic import BERTopic\n\ntopic_model = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True)\ntopics, probs = topic_model.fit_transform(data['Abstract'].astype(str))\n\n2025-10-23 15:46:57,896 - BERTopic - Embedding - Transforming documents to embeddings.\n\n\n\n\n\n2025-10-23 15:47:04,651 - BERTopic - Embedding - Completed ✓\n2025-10-23 15:47:04,652 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n2025-10-23 15:47:06,455 - BERTopic - Dimensionality - Completed ✓\n2025-10-23 15:47:06,456 - BERTopic - Cluster - Start clustering the reduced embeddings\n2025-10-23 15:47:06,530 - BERTopic - Cluster - Completed ✓\n2025-10-23 15:47:06,533 - BERTopic - Representation - Fine-tuning topics using representation models.\n2025-10-23 15:47:06,636 - BERTopic - Representation - Completed ✓\n\n\n\nimport pickle \n\nwith open('BERTopic.pkl', 'wb') as f:  # open a text file\n    pickle.dump(topic_model, f)\n\n\ntopic_model.probabilities_\n\narray([[2.67803981e-003, 2.98547797e-003, 2.61826865e-003, ...,\n        2.76175479e-003, 4.16144820e-003, 2.90433965e-003],\n       [7.55761220e-308, 6.51005917e-308, 6.21077628e-308, ...,\n        5.14305040e-308, 6.52787771e-308, 6.89930869e-308],\n       [3.03304179e-002, 2.49166112e-002, 2.71876804e-002, ...,\n        1.84549213e-002, 1.27129652e-002, 3.10650347e-002],\n       ...,\n       [9.45390270e-308, 8.08774380e-308, 5.49599301e-308, ...,\n        4.82213333e-308, 4.61407231e-308, 6.22077585e-308],\n       [9.63044933e-308, 8.08463621e-308, 5.54356682e-308, ...,\n        4.82296960e-308, 4.54998805e-308, 6.26946127e-308],\n       [9.61516319e-308, 8.01029548e-308, 5.52445380e-308, ...,\n        4.81240638e-308, 4.56614100e-308, 6.24411575e-308]])\n\n\n\nprobs\n\narray([[2.67803981e-003, 2.98547797e-003, 2.61826865e-003, ...,\n        2.76175479e-003, 4.16144820e-003, 2.90433965e-003],\n       [7.55761220e-308, 6.51005917e-308, 6.21077628e-308, ...,\n        5.14305040e-308, 6.52787771e-308, 6.89930869e-308],\n       [3.03304179e-002, 2.49166112e-002, 2.71876804e-002, ...,\n        1.84549213e-002, 1.27129652e-002, 3.10650347e-002],\n       ...,\n       [9.45390270e-308, 8.08774380e-308, 5.49599301e-308, ...,\n        4.82213333e-308, 4.61407231e-308, 6.22077585e-308],\n       [9.63044933e-308, 8.08463621e-308, 5.54356682e-308, ...,\n        4.82296960e-308, 4.54998805e-308, 6.26946127e-308],\n       [9.61516319e-308, 8.01029548e-308, 5.52445380e-308, ...,\n        4.81240638e-308, 4.56614100e-308, 6.24411575e-308]])\n\n\n\ntopics\n\n[-1,\n 12,\n -1,\n -1,\n 24,\n 12,\n 0,\n 7,\n 0,\n 10,\n -1,\n 3,\n -1,\n 7,\n 30,\n 9,\n -1,\n 7,\n -1,\n 10,\n 11,\n 3,\n 12,\n 12,\n -1,\n -1,\n -1,\n -1,\n 21,\n -1,\n -1,\n 9,\n 9,\n -1,\n -1,\n -1,\n 14,\n 12,\n 0,\n 12,\n 12,\n 7,\n -1,\n 26,\n 13,\n -1,\n -1,\n -1,\n 0,\n -1,\n 9,\n 3,\n 0,\n 17,\n -1,\n -1,\n 5,\n 10,\n -1,\n 0,\n 0,\n 20,\n -1,\n -1,\n 10,\n -1,\n 3,\n 9,\n 2,\n 20,\n 20,\n 20,\n 12,\n 17,\n -1,\n -1,\n 20,\n 8,\n 11,\n 9,\n 12,\n 11,\n 20,\n 9,\n -1,\n 2,\n -1,\n 11,\n 7,\n 7,\n 1,\n 9,\n -1,\n 12,\n -1,\n 12,\n 21,\n -1,\n -1,\n 2,\n 20,\n -1,\n -1,\n -1,\n 14,\n 1,\n 9,\n 17,\n 20,\n -1,\n -1,\n -1,\n 11,\n -1,\n 11,\n 0,\n -1,\n 7,\n -1,\n -1,\n 1,\n -1,\n 8,\n 9,\n -1,\n -1,\n 9,\n 9,\n 11,\n -1,\n -1,\n -1,\n 13,\n -1,\n -1,\n -1,\n 9,\n -1,\n 24,\n 12,\n 4,\n -1,\n 11,\n 23,\n 10,\n -1,\n 10,\n 4,\n -1,\n -1,\n -1,\n 3,\n -1,\n -1,\n -1,\n 11,\n -1,\n -1,\n 3,\n -1,\n 10,\n 30,\n 2,\n 30,\n 1,\n 30,\n 0,\n -1,\n 24,\n -1,\n -1,\n 25,\n 5,\n 3,\n -1,\n -1,\n -1,\n 24,\n -1,\n 7,\n 11,\n -1,\n -1,\n 2,\n 0,\n -1,\n 20,\n 9,\n -1,\n 0,\n 1,\n -1,\n 11,\n 4,\n -1,\n -1,\n -1,\n 7,\n -1,\n -1,\n 17,\n 0,\n 20,\n 25,\n 0,\n 9,\n 11,\n 0,\n -1,\n -1,\n 2,\n 11,\n 8,\n 5,\n 2,\n 0,\n -1,\n -1,\n 21,\n -1,\n 24,\n 18,\n -1,\n 10,\n 7,\n 1,\n 4,\n 7,\n 0,\n -1,\n 16,\n 3,\n -1,\n -1,\n -1,\n 25,\n 17,\n -1,\n -1,\n -1,\n -1,\n 0,\n -1,\n 20,\n 0,\n -1,\n 0,\n 10,\n -1,\n -1,\n -1,\n -1,\n 25,\n 10,\n 11,\n 9,\n 0,\n -1,\n -1,\n 17,\n 26,\n -1,\n -1,\n 0,\n 7,\n -1,\n 7,\n -1,\n -1,\n -1,\n -1,\n -1,\n 19,\n 21,\n 17,\n -1,\n -1,\n -1,\n -1,\n -1,\n 26,\n 12,\n -1,\n 20,\n 0,\n 10,\n -1,\n 7,\n 25,\n 3,\n 11,\n 25,\n -1,\n 10,\n 11,\n 11,\n -1,\n 2,\n 2,\n -1,\n -1,\n 3,\n 26,\n 11,\n -1,\n -1,\n -1,\n 14,\n 7,\n 14,\n 0,\n 0,\n 25,\n -1,\n 0,\n 22,\n 3,\n -1,\n -1,\n 1,\n 27,\n 25,\n 12,\n 3,\n 9,\n -1,\n -1,\n 10,\n 22,\n -1,\n 16,\n 21,\n 16,\n 16,\n -1,\n -1,\n -1,\n 3,\n 17,\n 30,\n 10,\n 2,\n 4,\n 1,\n 3,\n 7,\n -1,\n 16,\n 13,\n 12,\n -1,\n 2,\n -1,\n 7,\n 1,\n -1,\n -1,\n -1,\n -1,\n -1,\n -1,\n -1,\n 19,\n 2,\n -1,\n 9,\n 12,\n 10,\n -1,\n -1,\n 7,\n 2,\n 3,\n -1,\n 10,\n 10,\n 22,\n -1,\n 10,\n -1,\n 10,\n -1,\n 4,\n 2,\n 24,\n -1,\n -1,\n 3,\n -1,\n 27,\n -1,\n 5,\n 2,\n 2,\n 9,\n -1,\n -1,\n 9,\n 27,\n 1,\n -1,\n -1,\n -1,\n 3,\n -1,\n 19,\n 20,\n -1,\n 0,\n 21,\n 9,\n 4,\n 25,\n -1,\n -1,\n -1,\n 16,\n -1,\n 7,\n -1,\n -1,\n -1,\n -1,\n -1,\n -1,\n 4,\n 9,\n 1,\n -1,\n -1,\n -1,\n 16,\n 24,\n -1,\n -1,\n -1,\n -1,\n 12,\n 9,\n 5,\n 7,\n 7,\n 8,\n 7,\n 16,\n -1,\n 6,\n -1,\n -1,\n -1,\n 5,\n -1,\n -1,\n 12,\n -1,\n -1,\n 20,\n -1,\n 2,\n 0,\n 2,\n 22,\n 17,\n 4,\n 0,\n 17,\n -1,\n -1,\n 4,\n 3,\n -1,\n -1,\n 16,\n 28,\n 8,\n 12,\n 17,\n 14,\n 24,\n 22,\n -1,\n -1,\n -1,\n 27,\n -1,\n -1,\n -1,\n -1,\n -1,\n -1,\n -1,\n 7,\n -1,\n -1,\n 2,\n -1,\n -1,\n 24,\n 10,\n -1,\n -1,\n -1,\n 28,\n -1,\n -1,\n 24,\n 16,\n 7,\n 1,\n 2,\n -1,\n -1,\n -1,\n 20,\n 5,\n 22,\n 9,\n 20,\n -1,\n -1,\n -1,\n 8,\n -1,\n -1,\n 10,\n -1,\n 3,\n 7,\n 0,\n -1,\n 14,\n 6,\n 13,\n -1,\n 3,\n 10,\n 4,\n 25,\n 1,\n 24,\n -1,\n 14,\n -1,\n 10,\n -1,\n 9,\n 13,\n 9,\n 8,\n 9,\n -1,\n 5,\n 16,\n 9,\n -1,\n 9,\n 0,\n 14,\n 12,\n 14,\n 1,\n -1,\n 11,\n -1,\n 16,\n 2,\n 3,\n -1,\n 10,\n -1,\n -1,\n 0,\n 3,\n 0,\n -1,\n -1,\n -1,\n 19,\n 2,\n -1,\n 4,\n 0,\n -1,\n 22,\n 24,\n 24,\n 26,\n -1,\n -1,\n -1,\n 0,\n 12,\n -1,\n -1,\n -1,\n 5,\n -1,\n 22,\n 1,\n 2,\n 21,\n -1,\n -1,\n -1,\n 8,\n 12,\n 19,\n -1,\n 12,\n 19,\n -1,\n 12,\n -1,\n 2,\n -1,\n 24,\n 2,\n 8,\n 12,\n 2,\n 27,\n 13,\n 17,\n 5,\n -1,\n 16,\n -1,\n 17,\n 22,\n 10,\n 9,\n 22,\n -1,\n 3,\n 21,\n 7,\n -1,\n 22,\n -1,\n -1,\n 19,\n -1,\n 8,\n 3,\n 1,\n 2,\n 6,\n 13,\n 13,\n 13,\n 13,\n 13,\n 13,\n 13,\n -1,\n 13,\n 13,\n 13,\n 13,\n -1,\n 3,\n -1,\n 4,\n 16,\n 1,\n -1,\n 26,\n -1,\n 26,\n 26,\n 11,\n -1,\n 2,\n -1,\n 27,\n -1,\n 2,\n -1,\n -1,\n 7,\n -1,\n 10,\n 10,\n -1,\n 2,\n -1,\n -1,\n 0,\n -1,\n -1,\n 4,\n 0,\n 21,\n 30,\n 14,\n -1,\n -1,\n -1,\n -1,\n 20,\n 19,\n -1,\n 14,\n -1,\n 5,\n 20,\n 12,\n -1,\n 22,\n -1,\n 23,\n 17,\n 3,\n 6,\n 8,\n 6,\n 6,\n -1,\n 6,\n -1,\n 6,\n -1,\n 7,\n 2,\n 17,\n 3,\n 4,\n 5,\n 7,\n -1,\n -1,\n 2,\n 11,\n -1,\n -1,\n 3,\n -1,\n -1,\n 8,\n 30,\n 5,\n 4,\n 5,\n -1,\n 5,\n 5,\n 5,\n 5,\n 5,\n 5,\n 5,\n 5,\n 5,\n -1,\n 0,\n 5,\n 14,\n 5,\n 5,\n 27,\n 2,\n 16,\n 20,\n -1,\n -1,\n 1,\n 2,\n 2,\n -1,\n 3,\n -1,\n -1,\n -1,\n 6,\n -1,\n -1,\n 6,\n -1,\n 22,\n 6,\n 25,\n 0,\n 1,\n -1,\n -1,\n 6,\n -1,\n 27,\n -1,\n 6,\n 2,\n 4,\n 14,\n 3,\n 6,\n -1,\n 22,\n -1,\n -1,\n 23,\n 2,\n 12,\n 14,\n 14,\n 14,\n 27,\n -1,\n -1,\n 13,\n 4,\n 4,\n 11,\n 0,\n -1,\n -1,\n 25,\n 11,\n -1,\n 17,\n 17,\n -1,\n 0,\n 0,\n 17,\n -1,\n 8,\n 17,\n 17,\n 12,\n 17,\n 22,\n 21,\n 23,\n 4,\n 2,\n -1,\n 5,\n -1,\n -1,\n -1,\n 2,\n 11,\n 14,\n -1,\n 6,\n 9,\n 8,\n 26,\n -1,\n -1,\n 5,\n 13,\n -1,\n 2,\n 1,\n 6,\n -1,\n 4,\n 19,\n -1,\n 19,\n -1,\n 19,\n 19,\n 19,\n 19,\n 19,\n 12,\n -1,\n 2,\n 5,\n 7,\n 21,\n 0,\n 21,\n 7,\n -1,\n -1,\n 27,\n 4,\n -1,\n 13,\n 13,\n 13,\n 13,\n 13,\n 13,\n 13,\n -1,\n 13,\n 16,\n 6,\n 19,\n 23,\n 15,\n 4,\n 2,\n 8,\n 10,\n -1,\n -1,\n -1,\n -1,\n -1,\n -1,\n -1,\n 1,\n -1,\n 1,\n 5,\n -1,\n 24,\n 22,\n -1,\n 2,\n -1,\n 22,\n -1,\n 28,\n -1,\n -1,\n 27,\n -1,\n -1,\n -1,\n 21,\n -1,\n -1,\n 2,\n 21,\n -1,\n 28,\n 2,\n 22,\n 4,\n -1,\n 29,\n -1,\n 3,\n 6,\n -1,\n -1,\n 0,\n 20,\n -1,\n 5,\n 26,\n 21,\n -1,\n -1,\n 4,\n -1,\n -1,\n -1,\n 1,\n 0,\n 3,\n 0,\n 22,\n 0,\n 0,\n -1,\n 19,\n 0,\n -1,\n 1,\n 16,\n 8,\n 28,\n -1,\n -1,\n 11,\n 8,\n 14,\n 7,\n -1,\n 23,\n 1,\n 2,\n -1,\n 2,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n -1,\n 8,\n -1,\n -1,\n 28,\n -1,\n -1,\n -1,\n -1,\n 8,\n ...]\n\n\n\nfreq = topic_model.get_topic_info(); freq.head(5)\n\n\n\n\n\n\n\n\nTopic\nCount\nName\nRepresentation\nRepresentative_Docs\n\n\n\n\n0\n-1\n520\n-1_the_of_and_in\n[the, of, and, in, to, that, we, elsevier, on,...\n[We expand the institutional perspective of in...\n\n\n1\n0\n60\n0_international_entrepreneurial_internationali...\n[international, entrepreneurial, international...\n[Time in firm internationalization has differe...\n\n\n2\n1\n59\n1_trust_alliances_partners_in\n[trust, alliances, partners, in, of, alliance,...\n[This paper analyses the differences in the wa...\n\n\n3\n2\n55\n2_knowledge_transfer_the_and\n[knowledge, transfer, the, and, of, on, we, in...\n[This paper proposes a conceptual framework de...\n\n\n4\n3\n40\n3_acquisitions_acquisition_crossborder_of\n[acquisitions, acquisition, crossborder, of, t...\n[This paper examines the effect of national cu..."
  },
  {
    "objectID": "methods/02-sentiment-analysis.html",
    "href": "methods/02-sentiment-analysis.html",
    "title": "Sentiment Analysis",
    "section": "",
    "text": "This is a snapshot of the data we will be working with.\n\nimport pandas as pd\ndata = pd.read_csv('../data/jwb-articles.csv')\ndata = data[data['Abstract'].notna()] # Keep nonempty abstracts\ndata.head()\n\n\n\n\n\n\n\n\nAuthors\nAuthor full names\nAuthor(s) ID\nTitle\nYear\nSource title\nVolume\nIssue\nArt. No.\nPage start\n...\nISSN\nISBN\nCODEN\nPubMed ID\nLanguage of Original Document\nDocument Type\nPublication Stage\nOpen Access\nSource\nEID\n\n\n\n\n0\nAl Asady, A.; Anokhin, S.\nAl Asady, Ahmad (57219984746); Anokhin, Sergey...\n57219984746; 24482882200\nThe Trojan horse of international entrepreneur...\n2025\nJournal of World Business\n60\n6\n101677.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nNaN\nScopus\n2-s2.0-105014957115\n\n\n1\nThams, Y.; Dau, L.A.; Doh, J.; Kostova, T.; Ne...\nThams, Yannick (55357149800); Dau, Luis Alfons...\n55357149800; 35147597100; 7003920280; 66037741...\nPolitical ideology and the multinational enter...\n2025\nJournal of World Business\n60\n6\n101678.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nNaN\nScopus\n2-s2.0-105014844629\n\n\n2\nLindner, T.; Puck, J.; Puhr, H.\nLindner, Thomas (57159151000); Puck, Jonas (85...\n57159151000; 8563161700; 57223389639\nArtificial intelligence in international busin...\n2025\nJournal of World Business\n60\n6\n101676.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105014595041\n\n\n3\nBruton, G.D.; Mejía-Morelos, J.H.; Ahlstrom, D.\nBruton, Garry D. (6603867202); Mejía-Morelos, ...\n6603867202; 55748855800; 56525447800\nMultinational corporations and inclusive suppl...\n2025\nJournal of World Business\n60\n6\n101663.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013512235\n\n\n4\nLiang, Y.; Giroud, A.; Rygh, A.; Chen, Z.\nLiang, Yanze (57223851564); Giroud, Axèle L.A....\n57223851564; 7003496253; 37117826800; 58631386600\nPolitical embeddedness and post-acquisition in...\n2025\nJournal of World Business\n60\n6\n101665.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013485759\n\n\n\n\n5 rows × 41 columns",
    "crumbs": [
      "Home",
      "Methods",
      "Sentiment Analysis"
    ]
  },
  {
    "objectID": "methods/02-sentiment-analysis.html#data",
    "href": "methods/02-sentiment-analysis.html#data",
    "title": "Sentiment Analysis",
    "section": "",
    "text": "This is a snapshot of the data we will be working with.\n\nimport pandas as pd\ndata = pd.read_csv('../data/jwb-articles.csv')\ndata = data[data['Abstract'].notna()] # Keep nonempty abstracts\ndata.head()\n\n\n\n\n\n\n\n\nAuthors\nAuthor full names\nAuthor(s) ID\nTitle\nYear\nSource title\nVolume\nIssue\nArt. No.\nPage start\n...\nISSN\nISBN\nCODEN\nPubMed ID\nLanguage of Original Document\nDocument Type\nPublication Stage\nOpen Access\nSource\nEID\n\n\n\n\n0\nAl Asady, A.; Anokhin, S.\nAl Asady, Ahmad (57219984746); Anokhin, Sergey...\n57219984746; 24482882200\nThe Trojan horse of international entrepreneur...\n2025\nJournal of World Business\n60\n6\n101677.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nNaN\nScopus\n2-s2.0-105014957115\n\n\n1\nThams, Y.; Dau, L.A.; Doh, J.; Kostova, T.; Ne...\nThams, Yannick (55357149800); Dau, Luis Alfons...\n55357149800; 35147597100; 7003920280; 66037741...\nPolitical ideology and the multinational enter...\n2025\nJournal of World Business\n60\n6\n101678.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nNaN\nScopus\n2-s2.0-105014844629\n\n\n2\nLindner, T.; Puck, J.; Puhr, H.\nLindner, Thomas (57159151000); Puck, Jonas (85...\n57159151000; 8563161700; 57223389639\nArtificial intelligence in international busin...\n2025\nJournal of World Business\n60\n6\n101676.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105014595041\n\n\n3\nBruton, G.D.; Mejía-Morelos, J.H.; Ahlstrom, D.\nBruton, Garry D. (6603867202); Mejía-Morelos, ...\n6603867202; 55748855800; 56525447800\nMultinational corporations and inclusive suppl...\n2025\nJournal of World Business\n60\n6\n101663.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013512235\n\n\n4\nLiang, Y.; Giroud, A.; Rygh, A.; Chen, Z.\nLiang, Yanze (57223851564); Giroud, Axèle L.A....\n57223851564; 7003496253; 37117826800; 58631386600\nPolitical embeddedness and post-acquisition in...\n2025\nJournal of World Business\n60\n6\n101665.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013485759\n\n\n\n\n5 rows × 41 columns",
    "crumbs": [
      "Home",
      "Methods",
      "Sentiment Analysis"
    ]
  },
  {
    "objectID": "methods/02-sentiment-analysis.html#bertsentiment",
    "href": "methods/02-sentiment-analysis.html#bertsentiment",
    "title": "Sentiment Analysis",
    "section": "BERTsentiment",
    "text": "BERTsentiment\n\nfrom datasets import Dataset\n# Convert the Pandas DataFrame to a Hugging Face Dataset\ndataset = Dataset.from_pandas(data)\n\n\nfrom transformers import pipeline\n# Create the sentiment analysis pipeline\nnlp = pipeline(\"sentiment-analysis\", model=\"hw2942/bert-base-chinese-finetuning-financial-news-sentiment-v2\")\n\ndef get_sentiment(examples):\n    # Initialize lists to store results\n    sentiments = []\n    scores = []\n    final_sentiment_scores = []\n\n    # Process each entry in the batch\n    for text in examples['Abstract']:\n        if isinstance(text, str):\n            try:\n                result = nlp(text)\n                sentiment = result[0]['label']\n                score = result[0]['score']\n                sentiment_label = 1 if sentiment == \"Positive\" else -1 if sentiment == \"Negative\" else 0\n                final_sentiment_score = sentiment_label * score\n\n                sentiments.append(sentiment)\n                scores.append(score)\n                final_sentiment_scores.append(final_sentiment_score)\n            except Exception as e:\n                print(f\"Error processing text: {text}. Error: {e}\", flush=True)\n                # Append default values in case of an error\n                sentiments.append(None)\n                scores.append(None)\n                final_sentiment_scores.append(None)\n        else:\n            print(f\"Non-string entry found: {text}\", flush=True)\n            # Append default values for non-string entries\n            sentiments.append(None)\n            scores.append(None)\n            final_sentiment_scores.append(None)\n\n    # Ensure the output lists are of the same length as the batch size\n    batch_size = len(examples['Abstract'])\n    while len(sentiments) &lt; batch_size:\n        sentiments.append(None)\n        scores.append(None)\n        final_sentiment_scores.append(None)\n\n    return {'sentiment': sentiments, 'score': scores, 'final_sentiment_score': final_sentiment_scores}\n\ndataset = dataset.map(get_sentiment, batched=True, batch_size=64)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDevice set to use mps:0\n\n\n\n\n\nToken indices sequence length is longer than the specified maximum sequence length for this model (2249 &gt; 512). Running this sequence through the model will result in indexing errors\n\n\nError processing text: The authors regret that the coefficients, standard errors, significance, and hazard ratio were not correctly displayed during the publication process. Accordingly, it requires a few changes in the results section as well. However, the key conclusions and the messages of empirical results remain unchanged. The authors would also like to clarify that the entire document does not represent corrections. &lt;Original version&gt; (We underline the parts that have to be corrected) Table 3 presents standardized coefficients of the Cox proportional hazard analysis. In Model 1, we included only our control variables. As expected, subsidiary age, subsidiary profitability, subsidiary ownership, parent firm profitability, host country size and host country potential all show positive and significant impacts on subsidiary survival. The next three models consider the direct impacts of relative scope, mandate overlap, and global value-chain integration on survival, respectively. In Model 2, we included the first independent variable, the relative scope of subsidiary mandate portfolio, to test the first hypothesis. The coefficient of the portfolio's relative scope is −0.10, and it is statistically significant (p &lt; 0.05). The negative and significant coefficient supports the first hypothesis and the result is consistent throughout the remaining models. Model 5 includes the main effects of the three independent variables. Model 6 and 7 add two interaction terms, respectively. For a precise discussion pertaining to the hypotheses, we interpret our results using hazard ratio as shown in Table 4, which denotes the percentage change in the hazard with each unit change in the predictor. In Model 8, the relative scope of a mandate portfolio shows the hazard ratio of 0.90 (β = −0.11, p &lt; 0.05), suggesting that the survival likelihood of a subsidiary increases with an increase in the scope of the subsidiary's mandate portfolio in relation to the average number of mandates conducted by same-parent subsidiaries. Hypothesis 2, predicting a lower chance of survival, given an interaction between the relative scope and the subsidiary's mandate overlap, is supported (β = 0.15, p &lt; 0.05). While the relative scope of a mandate portfolio alone exhibits the hazard ratio of 0.90 for the risk of subsidiary exit, the hazard ratio of 1.16 for the interaction term suggests that a subsidiary's chances of survival significantly decrease when the scope of the mandate portfolio has overlapping mandates. As shown in Figure 1, subsidiaries with low levels of relative scope exit at similar rates regardless of overlap. With high relative scope, in line with our prediction, exit rates increase as overlap increases. Hypothesis 3, predicting higher chances of survival, given an interaction between the relative scope and the subsidiary's level of global value-chain integration, is supported (β = −0.35, p &lt; 0.01). The interaction term exhibits the hazard ratio of 0.70, which shows higher chances of survival when compared with the effect of the relative scope alone. Figure 2 shows that when relative scope is high, subsidiaries with high global orientation shows lower exit rates than those with low global orientation. Therefore, all of our hypotheses are confirmed through empirical tests. &lt;Corrected version&gt; (We underline the corrected parts) Table 3 presents standardized coefficients of the Cox proportional hazard analysis. In Model 1, we included only our control variables. As expected, subsidiary age, subsidiary profitability, subsidiary ownership, parent firm profitability, host country size and host country potential all show positive and significant impacts on subsidiary survival. The next three models consider the direct impacts of relative scope, mandate overlap, and global value-chain integration on survival, respectively. Model 5 includes the main effects of the three independent variables. Model 6 and 7 add two interaction terms, respectively. The coefficient of the portfolio's relative scope is −0.106, and it is statistically significant (p &lt; 0.05) in Model 8, indicating that subsidiaries with higher level of relative scope are less likely to exit, providing support for the first hypothesis. For a precise discussion pertaining to the hypotheses, we interpret our results using hazard ratio as shown in Table 4, which denotes the percentage change in the hazard with each unit change in the predictor. In Model 8, the relative scope of a mandate portfolio shows the hazard ratio of 0.899 (β = −0.106, p &lt; 0.05), suggesting that the survival likelihood of a subsidiary increases with an increase in the scope of the subsidiary's mandate portfolio in relation to the average number of mandates conducted by same-parent subsidiaries. Hypothesis 2, predicting a lower chance of survival, given an interaction between the relative scope and the subsidiary's mandate overlap, is supported (β = 0.149, p &lt; 0.05). While the relative scope of a mandate portfolio alone exhibits the hazard ratio of 0.899 for the risk of subsidiary exit, the hazard ratio of 1.160 for the interaction term suggests that a subsidiary's chances of survival significantly decrease when the scope of the mandate portfolio has overlapping mandates. As shown in Figure 1, subsidiaries with low levels of relative scope exit at similar rates regardless of overlap. With high relative scope, in line with our prediction, exit rates increase as overlap increases. Hypothesis 3, predicting higher chances of survival, given an interaction between the relative scope and the subsidiary's level of global value-chain integration, is supported (β = −0.352, p &lt; 0.01). The interaction term exhibits the hazard ratio of 0.703, which shows higher chances of survival when compared with the effect of the relative scope alone. Figure 2 shows that when relative scope is high, subsidiaries with high global orientation shows lower exit rates than those with low global orientation. Therefore, all of our hypotheses are confirmed through empirical tests. Table 3 Coefficient estimates of Cox proportional hazard analyses predicting divestment rate. [Table presented] Robust standard errors in brackets *** p &lt; 0.01, ** p &lt; 0.05, * p &lt; 0.1. Table 4 Hazard ratio of Cox proportional hazard analyses predicting divestment rate. [Table presented] Robust standard errors in brackets *** p &lt; 0.01, ** p &lt; 0.05, * p &lt; 0.1. The authors would like to apologise for any inconvenience caused.. Error: The size of tensor a (2249) must match the size of tensor b (512) at non-singleton dimension 1\nError processing text: While the extant literature has examined the influence of controlling and non-controlling principals on the internationalization decisions of emerging market firms, heterogeneity among non-controlling principals is largely ignored. The risk characteristics of different groups of owners, shaped by their institutional environments, could contribute to the differences in their preferences for firm internationalization. In this paper, we draw insights from institutional theory and behavioral risk perspective to examine the risk propensities and risk perceptions of various non-controlling principals, such as pressure-resistant (FIIs and mutual funds) and pressure-sensitive (banks, insurance companies and lending institutions) institutional investors. Empirical results from a sample of 2364 unique Indian firms during the 2005–2014 time-period show that, after controlling for ﬁrm-level resources and capabilities identiﬁed in prior literature, the ownership share of different types of institutional investors is associated with ﬁrms’ international investments differently. While pressure-sensitive institutional investors, such as banks and insurance companies, are not supportive of foreign investments by firms, pressure-resistant institutional investors, such as FIIs and mutual funds, are supportive of this strategic decision. Furthermore, our results show that the family ownership in a firm (measured in terms of family shareholding) further lowers the preference of pressure sensitive institutional investors for internationalization, whereas family ownership positively moderates the pressure resistant investors towards internationalization.. Error: The size of tensor a (530) must match the size of tensor b (512) at non-singleton dimension 1\nError processing text: Export intensity (EI) has been widely examined as a performance outcome of exporting firms. To date, studies on the determinants of EI have generated mixed and even contradictory results. To reconcile such inconsistencies, this study dichotomizes export strategy in emerging economies into two distinctive types, expansion-oriented vs. escape-oriented, with the former inspired by exploiting firm-specific competencies as portrayed by the RBV and the latter motivated by avoiding the domestic institutional deficiencies as informed by the institutional perspective. Different from prior findings in the International Business literature, this research finds that a firm's extremely high EI might not result from their superior competencies. Instead, high EI firms might focus on export mainly for the purpose of escaping from their home country's deficient institutional environment that places extra burdens in terms of costs of doing business. Such escape-oriented exporters are more sensitive and responsive to changes in the environment while they do not enhance their learning as much as those expansion-oriented exporters. Furthermore, institutional environment has heterogeneous impacts on firms with different ownership types. Our study helps integrate the insights from both the RBV and the institutional perspective, and our dichotomization of export strategy adds precision and sophistication to the understanding of EI and export performance. Our hypotheses are supported by an empirical study based on a sample of exporting firms in China between 1998 and 2007.. Error: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1\nError processing text: Sporadic studies on the global norm of national treatment for patent uncertainties (NTPU) urge for insights of changes as well as for clarification to discrepancy. This global norm has been a concern for policy makers and practitioners for over a century, as a socially and strategically more significant matter than before for multilateral cooperation given the active technology transfer across borders. To fill in the void and extend prior studies, we examine the global compliance of NTPU from the perspective of patent pendency and granting by addressing three relevant questions: (1) Is NTPU upheld within countries? (2) How does NTPU diverge across countries? (3) How does NTPU change, as an outcome, over time? Based on the institutional theory, lagged regression modeling and longitudinal comparison of US and Chinese patenting, our findings reveal that: (1) NTPU is overall upheld because equality in pendency is demonstrated in both countries and in US granting, and foreigners are even favored for Chinese granting. (2) NTPU is comparatively divergent between the countries in pendency and granting due to national variations. (3) Regressive and progressive changes in NTPU are evidenced since both countries provide equal or higher granting, but longer pendency than before. Our findings contribute to theories by providing new insights to the global norm of national treatment and institutional theory from the perspective of patent uncertainties. We make novel empirical contribution to address NTPU changes of the top patent filing countries and methodological contribution to the longitudinal comparative study. The results also provide implications that concern policy makers and practitioners to handle patent uncertainties across borders.. Error: The size of tensor a (554) must match the size of tensor b (512) at non-singleton dimension 1\nError processing text: This article, to date, is the first to consolidate, review, and integrate over 250 earlier studies that examine the country-specific determinants of cross-border mergers and acquisitions. Following 6Ws’ systematic review design and protocol, we survey the taxonomy of research published over the past three decades in international business, strategic management, finance, and economics. We present our syntheses in seven strands: macroeconomic and financial markets environment, institutional and regulatory environment, political environment and corruption, tax and the taxation environment, accounting standards and valuation guidelines, cultural environment, and geographical environment. Our integrative review and discussions are framed through Home–Host country, West–South, and South–West directional flows. We then show some highlights of the bibliometric analysis, provide a summary for each country-level determinant, and offer several theoretical propositions and research directions in need of future exploration. The review suggests that better the host country's institutional laws with regard to financial markets, taxation and corporate governance, then higher the number of inward acquisitions. It emphasizes that geopolitical distance, regulatory distance, and cultural distance between developed and developing economies are more likely to be moderated by the target country's market size, natural resources base, and weak institutional laws, especially corporate tax and capital gains tax. Overall, the article contributes to institutional framework and political economy view of globalized production by reviewing the crucial research question – what determines cross-border merger and acquisition transactions around the world?. Error: The size of tensor a (544) must match the size of tensor b (512) at non-singleton dimension 1\nError processing text: Talent Management (TM) has attracted increasing attention from academics and practitioners in recent years, but there are many gaps and omissions left for further theoretical and empirical development. One line of debate has been whether TM is merely a re-packaging of what already exists, not being distinct from traditional HRM practices or disciplines. The paper has three main components: (i) a review of how 'Talent' and TM has been conceptualised in the literature and the outline of a framework we have derived therefrom which identifies four main perspectives on TM: exclusive-people; exclusive-position; inclusive-people; social capital; (ii) the presentation and analysis of our research findings relating to TM perspectives and practices in seven multinational corporations (MNCs) in Beijing; (iii) a concluding discussion which compares and contrasts our findings with the extant literature and our framework. Six of the companies had adopted 'exclusive' perspectives, seeing TM as 'integrated, selective' HRM. For some, this involved an 'exclusive-people' focus on certain groups of 'high-performing' or 'high-potential' people, whilst for others it meant an 'exclusive-position' focus on certain 'key' positions in the organization. Just one organization had adopted an 'inclusive-people' approach. Two of the companies emphasized 'organizationally focussed competence development', concentrating upon smooth talent flows and development, and moving towards a 'social capital' perspective which took cognizance of networks, contexts and relationships as well as human capital. The implications of our findings for research and practice are outlined. All rights reserved.. Error: The size of tensor a (544) must match the size of tensor b (512) at non-singleton dimension 1\nError processing text: From the resource-based perspective, organization learning is the foundation of firms creating their special resources and thereby increasing their competitive advantage. Organization learning is indeed derived from individual learning within the organization. However, many firms have adopted downsizing strategies to reduce the redundancy. Nevertheless, it had a great impact both on laid-off employees and remaining ones. The remaining employees lost their trust, loyalty toward the firm and eventually left. The consequence not only affected the firms' daily operation but also impacted employees' learning motivation for improving their ability to enhance the firm's competitive advantage. In the post-downsizing era, applying appropriate human resource management practices to motivate employees would be a critical issue. The study began with two psychological constructs: job satisfaction and learning commitment to explore the content of job satisfaction which significantly influenced remaining employees' learning commitment. The study used both qualitative and quantitative methods to collect and analyze the data. The results revealed that the two criterion in job satisfaction \"the relationship with colleagues\" and \"the relationship with the family\" significantly influenced employees' learning commitment. However, this was clearly different from managers' subjective expectation. The findings provide important implications for both the research field and practical management of downsizing, employee motivation, cross-culture management and strategic HRM practices.. Error: The size of tensor a (523) must match the size of tensor b (512) at non-singleton dimension 1",
    "crumbs": [
      "Home",
      "Methods",
      "Sentiment Analysis"
    ]
  }
]