[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Online Appendix to Advancing International Business Research through Artificial Intelligence and Machine Learning Applications",
    "section": "",
    "text": "This is the online appendix to Advancing International Business Research through Artificial Intelligence and Machine Learning Applications.",
    "crumbs": [
      "Online Appendix to Advancing IB Research through AI and ML Applications"
    ]
  },
  {
    "objectID": "models/BERTsentiment.html",
    "href": "models/BERTsentiment.html",
    "title": "Machine Learning Perspective Paper",
    "section": "",
    "text": "import pandas as pd\ndata = pd.read_csv('../data/jwb-articles.csv')\ndata = data[data['Abstract'].notna()] # Keep nonempty abstracts\ndata.head()\n\n\n\n\n\n\n\n\nAuthors\nAuthor full names\nAuthor(s) ID\nTitle\nYear\nSource title\nVolume\nIssue\nArt. No.\nPage start\n...\nISSN\nISBN\nCODEN\nPubMed ID\nLanguage of Original Document\nDocument Type\nPublication Stage\nOpen Access\nSource\nEID\n\n\n\n\n0\nAl Asady, A.; Anokhin, S.\nAl Asady, Ahmad (57219984746); Anokhin, Sergey...\n57219984746; 24482882200\nThe Trojan horse of international entrepreneur...\n2025\nJournal of World Business\n60\n6\n101677.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nNaN\nScopus\n2-s2.0-105014957115\n\n\n1\nThams, Y.; Dau, L.A.; Doh, J.; Kostova, T.; Ne...\nThams, Yannick (55357149800); Dau, Luis Alfons...\n55357149800; 35147597100; 7003920280; 66037741...\nPolitical ideology and the multinational enter...\n2025\nJournal of World Business\n60\n6\n101678.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nNaN\nScopus\n2-s2.0-105014844629\n\n\n2\nLindner, T.; Puck, J.; Puhr, H.\nLindner, Thomas (57159151000); Puck, Jonas (85...\n57159151000; 8563161700; 57223389639\nArtificial intelligence in international busin...\n2025\nJournal of World Business\n60\n6\n101676.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105014595041\n\n\n3\nBruton, G.D.; Mejía-Morelos, J.H.; Ahlstrom, D.\nBruton, Garry D. (6603867202); Mejía-Morelos, ...\n6603867202; 55748855800; 56525447800\nMultinational corporations and inclusive suppl...\n2025\nJournal of World Business\n60\n6\n101663.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013512235\n\n\n4\nLiang, Y.; Giroud, A.; Rygh, A.; Chen, Z.\nLiang, Yanze (57223851564); Giroud, Axèle L.A....\n57223851564; 7003496253; 37117826800; 58631386600\nPolitical embeddedness and post-acquisition in...\n2025\nJournal of World Business\n60\n6\n101665.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013485759\n\n\n\n\n5 rows × 41 columns\n\n\n\n\nfrom datasets import Dataset\n# Only retain the first 512 terms\ndata['Abstract_200'] = data['Abstract'].apply(lambda x: ' '.join(x.split(' ')[:200]))\n\n# Convert the Pandas DataFrame to a Hugging Face Dataset\ndataset = Dataset.from_pandas(data)\n\n\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import pipeline\n\nfinbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\ntokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n\nnlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)\n\nsentences = [\"there is a shortage of capital, and we need extra financing\",  \n             \"growth is strong and we have plenty of liquidity\", \n             \"there are doubts about our finances\", \n             \"profits are flat\"]\nresults = nlp(sentences)\nprint(results)  #LABEL_0: neutral; LABEL_1: positive; LABEL_2: negative\n\nDevice set to use mps:0\n\n\n[{'label': 'Negative', 'score': 0.9966173768043518}, {'label': 'Positive', 'score': 1.0}, {'label': 'Negative', 'score': 0.5968121886253357}, {'label': 'Neutral', 'score': 0.9999551773071289}]\n\n\n\nfrom transformers import pipeline\n# Create the sentiment analysis pipeline\n\nfinbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\ntokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\nnlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)\n\n\ndef get_sentiment(examples):\n    # Initialize lists to store results\n    sentiments = []\n    scores = []\n\n    # Process each entry in the batch\n    for text in examples['Abstract_200']:\n        try:\n            result = nlp(text)\n            sentiment = result[0]['label']\n            score = result[0]['score']\n\n            sentiments.append(sentiment)\n            scores.append(score)\n        except Exception as e:\n            print(f'Error processing text: {text}. Error: {e}', flush=True)\n            # Append default values in case of an error\n            sentiments.append(None)\n            scores.append(None)\n\n    # Ensure the output lists are of the same length as the batch size\n    batch_size = len(examples['Abstract_200'])\n    while len(sentiments) &lt; batch_size:\n        sentiments.append(None)\n        scores.append(None)\n\n    return {'sentiment': sentiments, 'score': scores}\n\ndataset = dataset.map(get_sentiment, batched=True, batch_size=64)\n\nDevice set to use mps:0\n\n\n\n\n\n\nimport pickle \n\nwith open('BERTsentiment.pkl', 'wb') as f:\n    pickle.dump(dataset, f)"
  },
  {
    "objectID": "methods/01-topic-modeling.html",
    "href": "methods/01-topic-modeling.html",
    "title": "Topic Modeling",
    "section": "",
    "text": "This is a snapshot of the data (JWB article data 1967–2025 downloaded from Scopus) we will be working with.\n\nimport pandas as pd\ndata = pd.read_csv('../data/jwb-articles.csv')\ndata = data[data['Abstract'].notna()] # Keep nonempty abstracts\ndata.head()\n\n\n\n\n\n\n\n\nAuthors\nAuthor full names\nAuthor(s) ID\nTitle\nYear\nSource title\nVolume\nIssue\nArt. No.\nPage start\n...\nISSN\nISBN\nCODEN\nPubMed ID\nLanguage of Original Document\nDocument Type\nPublication Stage\nOpen Access\nSource\nEID\n\n\n\n\n0\nAl Asady, A.; Anokhin, S.\nAl Asady, Ahmad (57219984746); Anokhin, Sergey...\n57219984746; 24482882200\nThe Trojan horse of international entrepreneur...\n2025\nJournal of World Business\n60\n6\n101677.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nNaN\nScopus\n2-s2.0-105014957115\n\n\n1\nThams, Y.; Dau, L.A.; Doh, J.; Kostova, T.; Ne...\nThams, Yannick (55357149800); Dau, Luis Alfons...\n55357149800; 35147597100; 7003920280; 66037741...\nPolitical ideology and the multinational enter...\n2025\nJournal of World Business\n60\n6\n101678.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nNaN\nScopus\n2-s2.0-105014844629\n\n\n2\nLindner, T.; Puck, J.; Puhr, H.\nLindner, Thomas (57159151000); Puck, Jonas (85...\n57159151000; 8563161700; 57223389639\nArtificial intelligence in international busin...\n2025\nJournal of World Business\n60\n6\n101676.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105014595041\n\n\n3\nBruton, G.D.; Mejía-Morelos, J.H.; Ahlstrom, D.\nBruton, Garry D. (6603867202); Mejía-Morelos, ...\n6603867202; 55748855800; 56525447800\nMultinational corporations and inclusive suppl...\n2025\nJournal of World Business\n60\n6\n101663.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013512235\n\n\n4\nLiang, Y.; Giroud, A.; Rygh, A.; Chen, Z.\nLiang, Yanze (57223851564); Giroud, Axèle L.A....\n57223851564; 7003496253; 37117826800; 58631386600\nPolitical embeddedness and post-acquisition in...\n2025\nJournal of World Business\n60\n6\n101665.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013485759\n\n\n\n\n5 rows × 41 columns",
    "crumbs": [
      "Methods",
      "Topic Modeling"
    ]
  },
  {
    "objectID": "methods/01-topic-modeling.html#data",
    "href": "methods/01-topic-modeling.html#data",
    "title": "Topic Modeling",
    "section": "",
    "text": "This is a snapshot of the data (JWB article data 1967–2025 downloaded from Scopus) we will be working with.\n\nimport pandas as pd\ndata = pd.read_csv('../data/jwb-articles.csv')\ndata = data[data['Abstract'].notna()] # Keep nonempty abstracts\ndata.head()\n\n\n\n\n\n\n\n\nAuthors\nAuthor full names\nAuthor(s) ID\nTitle\nYear\nSource title\nVolume\nIssue\nArt. No.\nPage start\n...\nISSN\nISBN\nCODEN\nPubMed ID\nLanguage of Original Document\nDocument Type\nPublication Stage\nOpen Access\nSource\nEID\n\n\n\n\n0\nAl Asady, A.; Anokhin, S.\nAl Asady, Ahmad (57219984746); Anokhin, Sergey...\n57219984746; 24482882200\nThe Trojan horse of international entrepreneur...\n2025\nJournal of World Business\n60\n6\n101677.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nNaN\nScopus\n2-s2.0-105014957115\n\n\n1\nThams, Y.; Dau, L.A.; Doh, J.; Kostova, T.; Ne...\nThams, Yannick (55357149800); Dau, Luis Alfons...\n55357149800; 35147597100; 7003920280; 66037741...\nPolitical ideology and the multinational enter...\n2025\nJournal of World Business\n60\n6\n101678.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nNaN\nScopus\n2-s2.0-105014844629\n\n\n2\nLindner, T.; Puck, J.; Puhr, H.\nLindner, Thomas (57159151000); Puck, Jonas (85...\n57159151000; 8563161700; 57223389639\nArtificial intelligence in international busin...\n2025\nJournal of World Business\n60\n6\n101676.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105014595041\n\n\n3\nBruton, G.D.; Mejía-Morelos, J.H.; Ahlstrom, D.\nBruton, Garry D. (6603867202); Mejía-Morelos, ...\n6603867202; 55748855800; 56525447800\nMultinational corporations and inclusive suppl...\n2025\nJournal of World Business\n60\n6\n101663.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013512235\n\n\n4\nLiang, Y.; Giroud, A.; Rygh, A.; Chen, Z.\nLiang, Yanze (57223851564); Giroud, Axèle L.A....\n57223851564; 7003496253; 37117826800; 58631386600\nPolitical embeddedness and post-acquisition in...\n2025\nJournal of World Business\n60\n6\n101665.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013485759\n\n\n\n\n5 rows × 41 columns",
    "crumbs": [
      "Methods",
      "Topic Modeling"
    ]
  },
  {
    "objectID": "methods/01-topic-modeling.html#latent-dirichlet-allocation-lda",
    "href": "methods/01-topic-modeling.html#latent-dirichlet-allocation-lda",
    "title": "Topic Modeling",
    "section": "Latent Dirichlet Allocation (LDA)",
    "text": "Latent Dirichlet Allocation (LDA)\nText preprocessing: tokenize abstracts, remove punctuation and stop words, and store cleaned tokens.\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import RegexpTokenizer\n\ntokenizer = RegexpTokenizer(r'\\w+')\nstop_words = stopwords.words('english')\n\ndocs = []\nfor abstract in data['Abstract']:\n    tokens = word_tokenize(abstract.lower())\n    tokens = tokenizer.tokenize(' '.join(tokens))\n    rm_stop_words = [word for word in tokens if word not in stop_words]\n    docs.append(rm_stop_words)\n\nBefore we fit the LDA model, we construct a dictionary and convert our text to a bag of words.\n\nimport gensim\nfrom gensim.models.ldamodel import LdaModel\nfrom gensim import corpora\n\nlda_dict = corpora.Dictionary(docs)\nprint('The number of unique words:', len(lda_dict))\nprint(lda_dict)\n\nThe number of unique words: 8944\nDictionary&lt;8944 unique tokens: ['activities', 'affect', 'aims', 'also', 'argues']...&gt;\n\n\n\nlda_doc_corpus = [lda_dict.doc2bow(word) for word in docs]\nprint(lda_doc_corpus[0])\n\n[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2), (6, 1), (7, 2), (8, 3), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 2), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 3), (25, 2), (26, 2), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 2), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1)]\n\n\nNow we train a LDA model to identify latent topics in the abstracts.\n\nlda = LdaModel(corpus=lda_doc_corpus, id2word=lda_dict, num_topics=5,\n              random_state=42, passes=40, alpha=10, eta=0.1)\n\nWe can examine the results of the LDA and display the topics by listing the words associated with each topic.\n\nlda.show_topics()\n\n[(0,\n  '0.010*\"foreign\" + 0.009*\"institutional\" + 0.009*\"countries\" + 0.008*\"performance\" + 0.008*\"firms\" + 0.007*\"study\" + 0.007*\"chinese\" + 0.007*\"international\" + 0.007*\"relationship\" + 0.006*\"knowledge\"'),\n (1,\n  '0.019*\"international\" + 0.014*\"research\" + 0.013*\"business\" + 0.010*\"global\" + 0.007*\"ib\" + 0.006*\"knowledge\" + 0.005*\"new\" + 0.005*\"literature\" + 0.005*\"future\" + 0.004*\"study\"'),\n (2,\n  '0.013*\"subsidiary\" + 0.008*\"firm\" + 0.007*\"subsidiaries\" + 0.007*\"performance\" + 0.006*\"firms\" + 0.006*\"foreign\" + 0.006*\"results\" + 0.005*\"global\" + 0.005*\"knowledge\" + 0.005*\"family\"'),\n (3,\n  '0.009*\"cultural\" + 0.008*\"research\" + 0.007*\"study\" + 0.007*\"country\" + 0.007*\"political\" + 0.007*\"leadership\" + 0.006*\"multinational\" + 0.006*\"mncs\" + 0.005*\"performance\" + 0.005*\"institutional\"'),\n (4,\n  '0.025*\"firms\" + 0.010*\"management\" + 0.010*\"international\" + 0.009*\"market\" + 0.008*\"study\" + 0.008*\"firm\" + 0.006*\"performance\" + 0.006*\"internationalization\" + 0.005*\"markets\" + 0.005*\"talent\"')]\n\n\nAnd to visualize the topic probabilities for the first 15 abstracts in a heatmap, we can run the following code.\n\nimport matplotlib.pyplot as plt\nget_document_topics = lda.get_document_topics(lda_doc_corpus)\n\nall_probs = []\n\nfor doc_i in range(15):\n    doc_probs = get_document_topics[doc_i]\n    print(doc_probs)\n    probs = []\n    for (topic, prob) in doc_probs:\n        probs.append(prob)\n    all_probs.append(probs)\n\nplt.imshow(all_probs)\nplt.colorbar()\n\n[(0, 0.19223882), (1, 0.280424), (2, 0.18413724), (3, 0.17475025), (4, 0.1684497)]\n[(0, 0.09653838), (1, 0.35228702), (2, 0.11452496), (3, 0.3348996), (4, 0.10175001)]\n[(0, 0.14665498), (1, 0.25366387), (2, 0.13557251), (3, 0.26665145), (4, 0.19745715)]\n[(0, 0.12201825), (1, 0.20950222), (2, 0.121986), (3, 0.1319087), (4, 0.4145848)]\n[(0, 0.28674793), (1, 0.1418315), (2, 0.18685031), (3, 0.1600391), (4, 0.22453114)]\n[(0, 0.17187466), (1, 0.14167556), (2, 0.32062945), (3, 0.23193854), (4, 0.13388178)]\n[(0, 0.11212903), (1, 0.22102064), (2, 0.094602), (3, 0.1505873), (4, 0.421661)]\n[(0, 0.14603102), (1, 0.107232735), (2, 0.11303985), (3, 0.11569671), (4, 0.5179997)]\n[(0, 0.3068515), (1, 0.18931858), (2, 0.15783337), (3, 0.15932915), (4, 0.18666743)]\n[(0, 0.3844842), (1, 0.16181146), (2, 0.15116443), (3, 0.13615756), (4, 0.16638234)]\n[(0, 0.16029285), (1, 0.2216254), (2, 0.11499759), (3, 0.14059122), (4, 0.36249295)]\n[(0, 0.4805228), (1, 0.09621751), (2, 0.14083625), (3, 0.17901455), (4, 0.10340884)]\n[(0, 0.2092328), (1, 0.13705625), (2, 0.22701237), (3, 0.25130144), (4, 0.17539714)]\n[(0, 0.20451798), (1, 0.3667167), (2, 0.11523414), (3, 0.11886554), (4, 0.19466569)]\n[(0, 0.11036309), (1, 0.1859023), (2, 0.15423217), (3, 0.41059655), (4, 0.13890587)]",
    "crumbs": [
      "Methods",
      "Topic Modeling"
    ]
  },
  {
    "objectID": "methods/01-topic-modeling.html#bertopic",
    "href": "methods/01-topic-modeling.html#bertopic",
    "title": "Topic Modeling",
    "section": "BERTopic",
    "text": "BERTopic\nWe first extract the abstracts from a DataFrame and fit a BERTopic model.\n\nfrom bertopic import BERTopic\n\ndocs = df['Abstract'].tolist()\ndocs = [str(doc) for doc in docs]\n\ntopic_model = BERTopic(language='english', calculate_probabilities=True, verbose=True)\ntopic_model.fit(docs)\n\nWe can retrieve and print the document-topic matrix from the trained BERTopic model, showing which topics are associated with each document.\n\ndoc_topic = topic_model.topics_\nprint('Document-topic matrix:')\nprint(doc_topic)\n\nDocument-topic matrix:\n[-1, 30, -1, -1, 24, -1, 1, 5, -1, -1, -1, -1, -1, 5, 0, 8, -1, 5, 9, 10, 9, 3, -1, 30, -1, -1, -1, -1, 19, -1, -1, 8, 8, -1, -1, -1, -1, 9, 1, 9, 30, 5, -1, 29, 12, -1, -1, -1, 1, -1, 8, 3, 1, 17, -1, 10, 4, 10, -1, 1, 1, 18, -1, 19, -1, 11, 3, 8, 0, 18, 18, 18, 30, 17, -1, -1, 18, 2, 9, 8, 30, 9, 18, 8, -1, 0, -1, 9, 5, 5, 28, 8, -1, -1, 2, -1, 19, -1, -1, 0, 18, -1, -1, -1, -1, 23, 8, 17, 18, -1, -1, -1, 9, -1, 9, 1, 30, 5, -1, 5, 28, -1, 2, 8, -1, -1, 8, 8, 9, -1, -1, -1, 12, -1, -1, 16, 8, -1, 24, -1, 6, -1, 9, 22, 10, -1, 10, 6, 9, -1, -1, 3, -1, -1, -1, 9, -1, -1, 3, -1, 10, 0, 0, 0, 23, 0, 1, -1, 24, -1, -1, 27, 4, 3, -1, -1, 27, 24, -1, 5, 9, -1, -1, 0, 1, -1, 18, 8, -1, 1, 28, -1, 9, 13, -1, -1, -1, 5, -1, 21, 17, 1, 18, 27, 1, 8, 9, 1, -1, -1, 0, 9, 2, -1, -1, 1, -1, -1, -1, -1, 24, 14, -1, -1, 5, 23, 6, 5, 1, -1, -1, 3, -1, -1, -1, 27, 17, -1, -1, -1, -1, 1, -1, 18, -1, -1, 1, 10, 10, -1, -1, -1, -1, 10, 9, 8, 1, 17, 23, 17, 29, -1, -1, 1, 5, -1, 5, -1, -1, -1, -1, -1, 13, 19, 17, -1, -1, -1, -1, -1, 29, 30, -1, 18, 1, 10, -1, 5, 27, 3, 9, 27, -1, 10, 9, -1, -1, 0, 0, -1, -1, -1, 29, 9, -1, -1, -1, 16, 5, 16, 1, 1, 27, -1, 1, 20, 3, -1, -1, 23, 26, 27, -1, 3, 8, -1, -1, 10, 20, -1, 11, 19, 11, 11, -1, -1, -1, 3, 17, 0, 10, 0, 6, 23, 3, 5, -1, 11, 12, -1, -1, 0, -1, 5, 28, -1, -1, -1, -1, -1, 16, -1, 13, 0, -1, 8, -1, 10, 26, -1, 5, 0, 3, -1, 10, 10, 20, 11, 10, -1, 10, -1, 6, 0, 24, 26, -1, 3, 11, 26, -1, 4, 0, 0, 8, -1, -1, 8, -1, 23, -1, -1, -1, 3, -1, 13, 18, -1, -1, 19, 8, 6, 27, -1, -1, -1, 11, 10, 5, -1, -1, -1, -1, -1, -1, 6, 8, 21, -1, -1, -1, 11, 24, -1, -1, -1, -1, 30, 8, 4, 5, 5, 2, 5, 11, -1, -1, -1, -1, -1, 4, -1, -1, 30, -1, -1, 18, -1, 0, -1, 0, 20, 17, 6, 1, 17, -1, 27, 6, 3, -1, -1, 11, 25, 2, -1, 17, 16, 24, 20, -1, -1, -1, -1, -1, -1, 11, -1, 11, 11, -1, 5, 10, -1, 0, -1, -1, 24, 10, -1, -1, -1, 25, -1, -1, 24, 11, 5, -1, 0, -1, 10, -1, 18, 4, 20, 8, 18, -1, -1, -1, 2, -1, -1, 10, -1, 3, 5, 1, -1, 16, 7, 12, -1, 3, -1, 6, 27, 23, 24, 11, 16, -1, 10, -1, 8, 12, 8, 2, 8, -1, 4, 11, 8, -1, 8, 1, 16, -1, 16, -1, -1, 9, -1, 11, 0, 3, -1, -1, -1, -1, 1, 3, 1, 16, -1, -1, 13, 0, -1, 6, 1, -1, 20, 24, 24, 29, -1, -1, -1, 1, 30, -1, 0, -1, 4, 16, 20, 28, 0, 19, -1, -1, -1, 2, -1, 13, -1, -1, 13, -1, 30, 25, 0, -1, -1, 0, 2, 30, 0, 26, -1, 17, 4, -1, 11, -1, 17, 20, 10, 8, 20, -1, 3, 19, 5, -1, 20, -1, -1, 13, -1, 2, 3, -1, 0, 7, 12, 12, 12, 12, 12, 12, 12, -1, 12, 12, 12, 12, 2, 3, -1, 6, 11, -1, -1, 29, -1, 29, 29, 9, -1, 0, 29, 26, -1, 0, -1, -1, 5, 10, -1, 10, -1, 0, -1, -1, 1, -1, -1, 6, 1, 19, 0, 16, -1, -1, -1, -1, 18, 13, -1, 16, 13, 4, 18, -1, -1, 20, 13, 22, -1, 3, 7, 2, 7, 7, 2, 7, 2, 7, 12, 5, 0, 17, 3, 6, 4, 5, -1, -1, 0, 9, -1, -1, 3, -1, -1, 2, -1, 4, 6, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, -1, 1, 4, 16, 4, 4, 26, 0, 11, 18, -1, -1, 23, 0, -1, -1, 3, -1, -1, -1, 7, -1, -1, 7, -1, 20, -1, -1, 1, 28, -1, 25, 7, 25, 26, -1, -1, 0, 6, 16, 3, 7, -1, 20, -1, -1, 22, -1, 30, 16, -1, 16, 26, -1, -1, 12, 6, 6, 9, -1, -1, -1, 27, 9, -1, 17, 17, -1, 1, 1, 17, -1, -1, 17, 17, -1, 17, 20, 19, 22, 6, 0, -1, 4, -1, -1, -1, 0, 9, 16, -1, 7, 8, 2, 29, -1, -1, 4, 12, 14, 0, 23, 7, -1, 6, 13, -1, 13, -1, 13, 13, 13, 13, 13, -1, -1, -1, 4, 5, 19, 1, 19, 5, -1, -1, 26, 13, -1, 12, 12, 12, 12, 12, 12, 12, -1, 12, 11, 7, 13, 22, 15, 6, 0, 2, 10, -1, -1, 0, -1, -1, -1, -1, -1, -1, 21, 4, -1, 24, 20, -1, 0, -1, 20, -1, 25, -1, -1, 26, -1, -1, -1, 19, -1, -1, 0, 19, -1, 25, 0, 20, 6, -1, 31, -1, 3, 7, -1, -1, -1, 18, -1, 4, 29, 19, -1, -1, 6, -1, -1, -1, 21, 1, 3, 1, 20, 1, 1, -1, 13, 1, -1, 23, 11, 2, 25, -1, -1, 9, 2, -1, 5, -1, 22, 28, 0, -1, 0, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 2, -1, -1, 25, -1, 25, -1, -1, 2, -1, 31, 11, 21, 7, -1, 7, 5, -1, 25, -1, -1, 6, 10, 22, -1, 26, -1, 21, 3, -1, -1, -1, -1, -1, 25, 2, -1, -1, -1, -1, 6, -1, -1, -1, 2, 2, 2, 2, 0, 2, -1, 23, 26, -1, -1, -1, 4, 6, 7, 23, 8, -1, -1, -1, 0, -1, 3, -1, 21, 2, 2, -1, 7, 7, 7, -1, -1, -1, 17, 7, 7, -1, 7, -1, 29, -1, 0, 16, -1, -1, -1, -1, 21, 0, -1, -1, -1, 7, -1, 7, -1, 6, 11, 31, 15, 31, 11, 31, 31, -1, -1, -1, 9, 11, 28, -1, -1, -1, -1, 0, -1, 0, -1, 2, 0, 25, 0, 28, -1, 0, -1, 0, 15, -1, 2, 29, -1, -1, 28, -1, -1, 2, -1, -1, 2, -1, 21, 13, 4, -1, 4, -1, -1, -1, -1, -1, 2, 2, -1, 7, 7, 6, 25, -1, -1, -1, -1, 9, 21, 14, 7, 22, -1, 3, 3, 3, 3, -1, 6, 19, 3, 22, -1, 4, -1, -1, -1, 22, 22, 19, 1, 1, -1, -1, -1, 31, -1, 23, 31, -1, 16, -1, -1, 22, -1, 13, 21, -1, -1, 7, 14, -1, 21, 21, -1, -1, 7, 21, -1, 22, 21, -1, 7, -1, 24, -1, -1, -1, -1, 23, -1, 16, -1, -1, -1, 4, -1, 16, 6, -1, 2, -1, 11, 21, 2, 6, 21, 23, 2, 2, 31, -1, -1, -1, -1, -1, 6, -1, 2, 7, 21, -1, 2, 6, -1, -1, 6, 28, -1, -1, -1, -1, 14, -1, -1, -1, -1, 14, -1, -1, 14, -1, 14, 13, 22, 5, -1, -1, -1, -1, 7, -1, -1, -1, 19, -1, -1, 9, -1, 15, -1, 28, 15, 13, 14, -1, 14, 8, 9, 14, 15, -1, 14, -1, 14, -1, 31, 14, 14, 9, 14, 6, -1, -1, -1, 14, 14, -1, 14, -1, 27, 14, 14, 14, 16, 14, 14, 14, 3, 15, 15, 28, 15, -1, -1, 15, 15, 15, 15, -1, -1, 15, 15, 15, 15, 15, 15, 15, -1, 15, 19, -1, 15, 15, 22, 22, 22]\n\n\nWe can also retrieve and print the topic probability distributions for each document, showing the likelihood of each topic being associated with the documents.\n\n# Get probabilities for each topic\nprobs = topic_model.probabilities_\nprint('Topic probabilities for the first document:')\nprint(probs[0].round(2))\nprint()\n# Print topic probabilities for the first 15 documents\nfor i in range(min(15, len(docs))):\n    print(f'Document {i + 1} is in topic {doc_topic[i]}')\n    print(f'Topic probabilities for Document {i + 1}:')\n    print(probs[i].round(3))\n    print()\n\nTopic probabilities for the first document:\n[0.   0.   0.   0.   0.   0.   0.   0.   0.   0.01 0.01 0.01 0.   0.\n 0.   0.   0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.01\n 0.   0.   0.01 0.01]\n\nDocument 1 is in topic -1\nTopic probabilities for Document 1:\n[0.003 0.003 0.003 0.004 0.002 0.003 0.003 0.003 0.003 0.006 0.007 0.007\n 0.003 0.004 0.005 0.004 0.005 0.002 0.004 0.004 0.002 0.004 0.003 0.003\n 0.003 0.003 0.004 0.008 0.003 0.004 0.006 0.005]\n\nDocument 2 is in topic 30\nTopic probabilities for Document 2:\n[0.012 0.014 0.008 0.013 0.007 0.017 0.011 0.011 0.02  0.037 0.024 0.017\n 0.008 0.016 0.015 0.01  0.03  0.006 0.009 0.016 0.007 0.011 0.017 0.009\n 0.014 0.009 0.019 0.025 0.012 0.015 0.55  0.012]\n\nDocument 3 is in topic -1\nTopic probabilities for Document 3:\n[0.004 0.004 0.002 0.003 0.002 0.004 0.002 0.003 0.005 0.004 0.003 0.002\n 0.002 0.003 0.002 0.002 0.006 0.002 0.002 0.003 0.002 0.003 0.004 0.002\n 0.005 0.002 0.006 0.004 0.003 0.005 0.006 0.002]\n\nDocument 4 is in topic -1\nTopic probabilities for Document 4:\n[0.003 0.003 0.002 0.004 0.002 0.004 0.003 0.002 0.003 0.007 0.018 0.009\n 0.002 0.005 0.006 0.003 0.007 0.001 0.003 0.005 0.002 0.003 0.004 0.002\n 0.003 0.002 0.004 0.006 0.003 0.003 0.007 0.005]\n\nDocument 5 is in topic 24\nTopic probabilities for Document 5:\n[0.077 0.035 0.014 0.057 0.013 0.023 0.02  0.015 0.017 0.019 0.02  0.016\n 0.012 0.016 0.014 0.011 0.028 0.012 0.012 0.026 0.014 0.027 0.021 0.026\n 0.151 0.016 0.049 0.024 0.033 0.027 0.022 0.012]\n\nDocument 6 is in topic -1\nTopic probabilities for Document 6:\n[0.02  0.021 0.014 0.023 0.012 0.024 0.017 0.018 0.027 0.069 0.047 0.034\n 0.014 0.024 0.028 0.019 0.05  0.01  0.016 0.028 0.011 0.019 0.024 0.016\n 0.022 0.015 0.032 0.057 0.019 0.022 0.124 0.023]\n\nDocument 7 is in topic 1\nTopic probabilities for Document 7:\n[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0.]\n\nDocument 8 is in topic 5\nTopic probabilities for Document 8:\n[0.015 0.042 0.01  0.015 0.009 0.088 0.018 0.01  0.036 0.019 0.018 0.013\n 0.009 0.02  0.013 0.009 0.024 0.009 0.011 0.019 0.01  0.012 0.031 0.012\n 0.024 0.01  0.018 0.016 0.014 0.017 0.024 0.011]\n\nDocument 9 is in topic -1\nTopic probabilities for Document 9:\n[0.028 0.122 0.014 0.031 0.014 0.056 0.027 0.015 0.028 0.025 0.025 0.019\n 0.013 0.025 0.016 0.012 0.034 0.013 0.015 0.026 0.012 0.021 0.033 0.02\n 0.066 0.015 0.032 0.024 0.024 0.03  0.03  0.014]\n\nDocument 10 is in topic -1\nTopic probabilities for Document 10:\n[0.013 0.015 0.007 0.019 0.007 0.017 0.016 0.009 0.014 0.026 0.125 0.052\n 0.007 0.02  0.028 0.016 0.038 0.006 0.012 0.03  0.009 0.013 0.02  0.011\n 0.016 0.008 0.018 0.031 0.014 0.013 0.028 0.023]\n\nDocument 11 is in topic -1\nTopic probabilities for Document 11:\n[0.01  0.016 0.006 0.012 0.005 0.016 0.01  0.007 0.016 0.029 0.021 0.012\n 0.006 0.012 0.011 0.007 0.04  0.005 0.007 0.015 0.005 0.008 0.014 0.007\n 0.014 0.006 0.015 0.018 0.009 0.01  0.023 0.008]\n\nDocument 12 is in topic -1\nTopic probabilities for Document 12:\n[0.014 0.012 0.006 0.05  0.006 0.01  0.011 0.007 0.009 0.012 0.018 0.014\n 0.006 0.01  0.01  0.008 0.019 0.005 0.007 0.018 0.006 0.014 0.011 0.011\n 0.017 0.007 0.018 0.023 0.015 0.011 0.014 0.01 ]\n\nDocument 13 is in topic -1\nTopic probabilities for Document 13:\n[0.026 0.027 0.012 0.031 0.011 0.027 0.019 0.015 0.026 0.041 0.04  0.026\n 0.012 0.021 0.022 0.014 0.151 0.01  0.012 0.043 0.012 0.02  0.027 0.017\n 0.035 0.014 0.055 0.054 0.023 0.024 0.049 0.017]\n\nDocument 14 is in topic 5\nTopic probabilities for Document 14:\n[0.016 0.048 0.011 0.017 0.01  0.108 0.02  0.012 0.039 0.021 0.02  0.015\n 0.01  0.022 0.014 0.01  0.026 0.01  0.012 0.021 0.011 0.014 0.034 0.013\n 0.027 0.011 0.02  0.017 0.015 0.018 0.026 0.012]\n\nDocument 15 is in topic 0\nTopic probabilities for Document 15:\n[0.055 0.017 0.017 0.023 0.014 0.014 0.013 0.02  0.013 0.013 0.013 0.011\n 0.014 0.012 0.01  0.008 0.018 0.012 0.009 0.017 0.015 0.026 0.014 0.026\n 0.028 0.021 0.04  0.017 0.032 0.031 0.016 0.009]\n\n\n\n\n# Get the lists of keywords under each topic\ntopic_keywords = topic_model.get_topics()\n\n# Print the lists of keywords for each topic\nfor topic_id, keywords in topic_keywords.items():\n    keywords = [(u, round(v, 3)) for u, v in keywords]\n    print(f'Topic {topic_id}: {keywords}')\n\nWe can also examine the topics more closely.\n\n# To see the first 5 topics\nfreq = topic_model.get_topic_info()\nfreq.head(5)\n\n\n\n\n\n\n\n\nTopic\nCount\nName\nRepresentation\nRepresentative_Docs\n\n\n\n\n0\n-1\n544\n-1_the_of_and_in\n[the, of, and, in, to, that, we, on, this, with]\n[In recent years, there has been an increasing...\n\n\n1\n0\n61\n0_knowledge_transfer_and_the\n[knowledge, transfer, and, the, of, on, to, th...\n[This paper proposes a conceptual framework de...\n\n\n2\n1\n52\n1_international_entrepreneurial_internationali...\n[international, entrepreneurial, international...\n[Grounded in the resource-based view of the fi...\n\n\n3\n2\n43\n2_career_expatriates_expatriate_assignments\n[career, expatriates, expatriate, assignments,...\n[Creating organizational processes which nurtu...\n\n\n4\n3\n38\n3_acquisitions_acquisition_crossborder_acquirers\n[acquisitions, acquisition, crossborder, acqui...\n[This study develops and tests a framework abo...\n\n\n\n\n\n\n\nNote that topic -1 indicates that the dociment has not been grouped into a topic.",
    "crumbs": [
      "Methods",
      "Topic Modeling"
    ]
  },
  {
    "objectID": "methods/03-sentence-similarity-analysis.html",
    "href": "methods/03-sentence-similarity-analysis.html",
    "title": "Sentence Similarity Analysis",
    "section": "",
    "text": "This is a snapshot of the data (JWB article data 1967–2025 downloaded from Scopus) we will be working with.\n\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv('../data/jwb-articles.csv')\ndata = data[data['Abstract'].notna()] # Keep nonempty abstracts\ndata.head()\n\n\n\n\n\n\n\n\nAuthors\nAuthor full names\nAuthor(s) ID\nTitle\nYear\nSource title\nVolume\nIssue\nArt. No.\nPage start\n...\nISSN\nISBN\nCODEN\nPubMed ID\nLanguage of Original Document\nDocument Type\nPublication Stage\nOpen Access\nSource\nEID\n\n\n\n\n0\nAl Asady, A.; Anokhin, S.\nAl Asady, Ahmad (57219984746); Anokhin, Sergey...\n57219984746; 24482882200\nThe Trojan horse of international entrepreneur...\n2025\nJournal of World Business\n60\n6\n101677.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nNaN\nScopus\n2-s2.0-105014957115\n\n\n1\nThams, Y.; Dau, L.A.; Doh, J.; Kostova, T.; Ne...\nThams, Yannick (55357149800); Dau, Luis Alfons...\n55357149800; 35147597100; 7003920280; 66037741...\nPolitical ideology and the multinational enter...\n2025\nJournal of World Business\n60\n6\n101678.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nNaN\nScopus\n2-s2.0-105014844629\n\n\n2\nLindner, T.; Puck, J.; Puhr, H.\nLindner, Thomas (57159151000); Puck, Jonas (85...\n57159151000; 8563161700; 57223389639\nArtificial intelligence in international busin...\n2025\nJournal of World Business\n60\n6\n101676.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105014595041\n\n\n3\nBruton, G.D.; Mejía-Morelos, J.H.; Ahlstrom, D.\nBruton, Garry D. (6603867202); Mejía-Morelos, ...\n6603867202; 55748855800; 56525447800\nMultinational corporations and inclusive suppl...\n2025\nJournal of World Business\n60\n6\n101663.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013512235\n\n\n4\nLiang, Y.; Giroud, A.; Rygh, A.; Chen, Z.\nLiang, Yanze (57223851564); Giroud, Axèle L.A....\n57223851564; 7003496253; 37117826800; 58631386600\nPolitical embeddedness and post-acquisition in...\n2025\nJournal of World Business\n60\n6\n101665.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013485759\n\n\n\n\n5 rows × 41 columns",
    "crumbs": [
      "Methods",
      "Sentence Similarity Analysis"
    ]
  },
  {
    "objectID": "methods/03-sentence-similarity-analysis.html#data",
    "href": "methods/03-sentence-similarity-analysis.html#data",
    "title": "Sentence Similarity Analysis",
    "section": "",
    "text": "This is a snapshot of the data (JWB article data 1967–2025 downloaded from Scopus) we will be working with.\n\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv('../data/jwb-articles.csv')\ndata = data[data['Abstract'].notna()] # Keep nonempty abstracts\ndata.head()\n\n\n\n\n\n\n\n\nAuthors\nAuthor full names\nAuthor(s) ID\nTitle\nYear\nSource title\nVolume\nIssue\nArt. No.\nPage start\n...\nISSN\nISBN\nCODEN\nPubMed ID\nLanguage of Original Document\nDocument Type\nPublication Stage\nOpen Access\nSource\nEID\n\n\n\n\n0\nAl Asady, A.; Anokhin, S.\nAl Asady, Ahmad (57219984746); Anokhin, Sergey...\n57219984746; 24482882200\nThe Trojan horse of international entrepreneur...\n2025\nJournal of World Business\n60\n6\n101677.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nNaN\nScopus\n2-s2.0-105014957115\n\n\n1\nThams, Y.; Dau, L.A.; Doh, J.; Kostova, T.; Ne...\nThams, Yannick (55357149800); Dau, Luis Alfons...\n55357149800; 35147597100; 7003920280; 66037741...\nPolitical ideology and the multinational enter...\n2025\nJournal of World Business\n60\n6\n101678.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nNaN\nScopus\n2-s2.0-105014844629\n\n\n2\nLindner, T.; Puck, J.; Puhr, H.\nLindner, Thomas (57159151000); Puck, Jonas (85...\n57159151000; 8563161700; 57223389639\nArtificial intelligence in international busin...\n2025\nJournal of World Business\n60\n6\n101676.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105014595041\n\n\n3\nBruton, G.D.; Mejía-Morelos, J.H.; Ahlstrom, D.\nBruton, Garry D. (6603867202); Mejía-Morelos, ...\n6603867202; 55748855800; 56525447800\nMultinational corporations and inclusive suppl...\n2025\nJournal of World Business\n60\n6\n101663.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013512235\n\n\n4\nLiang, Y.; Giroud, A.; Rygh, A.; Chen, Z.\nLiang, Yanze (57223851564); Giroud, Axèle L.A....\n57223851564; 7003496253; 37117826800; 58631386600\nPolitical embeddedness and post-acquisition in...\n2025\nJournal of World Business\n60\n6\n101665.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013485759\n\n\n\n\n5 rows × 41 columns",
    "crumbs": [
      "Methods",
      "Sentence Similarity Analysis"
    ]
  },
  {
    "objectID": "methods/03-sentence-similarity-analysis.html#cosine-similarity",
    "href": "methods/03-sentence-similarity-analysis.html#cosine-similarity",
    "title": "Sentence Similarity Analysis",
    "section": "Cosine Similarity",
    "text": "Cosine Similarity\nWe first preprocess our text by tokenizing, remove stop words and stem the abstracts.\n\nimport re \nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\ndef preprocess_text(text):\n    # Lowercasing and remove punctuation\n    lowercased_text = text.lower()\n    remove_punctuation = re.sub(r'[^\\w\\s]', '', lowercased_text)\n    remove_white_space = remove_punctuation.strip()\n\n    # Tokenization \n    tokenized_text = word_tokenize(remove_white_space)\n\n    # Remove stop words\n    stop_words = set(stopwords.words('english'))\n    stopwords_removed = [word for word in tokenized_text if word not in stop_words]\n\n    # Stemming\n    ps = PorterStemmer()\n    stemmed_text = [ps.stem(word) for word in stopwords_removed]\n    \n    # Return the stemmed text as a list\n    return stemmed_text\n\ndata['clean_abstract'] = data['Abstract'].apply(preprocess_text)\ndata[['Abstract', 'clean_abstract']].head(5)\n\n\n\n\n\n\n\n\nAbstract\nclean_abstract\n\n\n\n\n0\nThis study explores the under-theorized relati...\n[studi, explor, undertheor, relationship, inte...\n\n\n1\nWhile politics and political issues such as ri...\n[polit, polit, issu, risk, domin, agenda, inte...\n\n\n2\nThis paper discusses the impact of artificial ...\n[paper, discuss, impact, artifici, intellig, a...\n\n\n3\nAn institutional logic represents the way a pa...\n[institut, logic, repres, way, particular, soc...\n\n\n4\nPolitical embeddedness has been shown to influ...\n[polit, embedded, shown, influenc, firm, innov...\n\n\n\n\n\n\n\nWe now compute the term frequency-inverse document frequency (TF-IDF) scores of each abstract.\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Join each list of words into a string\ntexts = data['clean_abstract'].apply(lambda x: ' '.join(x))\n\n# Compute TF-IDF\nvectorizer = TfidfVectorizer()\ntfidf_matrix = vectorizer.fit_transform(texts)\n\n# Get feature names (words)\nfeature_names = vectorizer.get_feature_names_out()\n\n# Create TF-IDF DataFrame\ndf_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n\n# Optionally, include the original data (Abstract + clean_abstract)\ndata_tfidf = pd.concat([data.reset_index(drop=True), df_tfidf], axis=1)\n\n\ndata_tfidf.head(5)\n\n\n\n\n\n\n\n\nAuthors\nAuthor full names\nAuthor(s) ID\nTitle\nYear\nSource title\nVolume\nIssue\nArt. No.\nPage start\n...\nzeroinfl\nzeroshot\nzhirinovski\nzimbabw\nzizhu\nzone\nzoom\nzurawicki\nﬁrm\nﬁrmlevel\n\n\n\n\n0\nAl Asady, A.; Anokhin, S.\nAl Asady, Ahmad (57219984746); Anokhin, Sergey...\n57219984746; 24482882200\nThe Trojan horse of international entrepreneur...\n2025\nJournal of World Business\n60\n6\n101677.0\nNaN\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\nThams, Y.; Dau, L.A.; Doh, J.; Kostova, T.; Ne...\nThams, Yannick (55357149800); Dau, Luis Alfons...\n55357149800; 35147597100; 7003920280; 66037741...\nPolitical ideology and the multinational enter...\n2025\nJournal of World Business\n60\n6\n101678.0\nNaN\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\nLindner, T.; Puck, J.; Puhr, H.\nLindner, Thomas (57159151000); Puck, Jonas (85...\n57159151000; 8563161700; 57223389639\nArtificial intelligence in international busin...\n2025\nJournal of World Business\n60\n6\n101676.0\nNaN\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\nBruton, G.D.; Mejía-Morelos, J.H.; Ahlstrom, D.\nBruton, Garry D. (6603867202); Mejía-Morelos, ...\n6603867202; 55748855800; 56525447800\nMultinational corporations and inclusive suppl...\n2025\nJournal of World Business\n60\n6\n101663.0\nNaN\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\nLiang, Y.; Giroud, A.; Rygh, A.; Chen, Z.\nLiang, Yanze (57223851564); Giroud, Axèle L.A....\n57223851564; 7003496253; 37117826800; 58631386600\nPolitical embeddedness and post-acquisition in...\n2025\nJournal of World Business\n60\n6\n101665.0\nNaN\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n5 rows × 6645 columns\n\n\n\nWe can compute the pairwise similarity scores and display it in a heat map.\n\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport matplotlib.pyplot as plt\n\nsim_scores = np.zeros((df_tfidf.shape[0], df_tfidf.shape[0]))\n\ntfidf_scores = df_tfidf.values\n\ncosim = cosine_similarity(tfidf_scores)\n\nplt.imshow(cosim)\nplt.colorbar()\n\n\n\n\n\n\n\n\nFor a closer look, we examine the first 100 abstracts. The brighter the cell, the more similar the abstracts are.\n\nplt.imshow(cosim[:100, :100])\nplt.colorbar()",
    "crumbs": [
      "Methods",
      "Sentence Similarity Analysis"
    ]
  },
  {
    "objectID": "methods/02-sentiment-analysis.html",
    "href": "methods/02-sentiment-analysis.html",
    "title": "Sentiment Analysis",
    "section": "",
    "text": "This is a snapshot of the data (JWB article data 1967–2025 downloaded from Scopus) we will be working with.\n\nimport pandas as pd\ndata = pd.read_csv('../data/jwb-articles.csv')\ndata = data[data['Abstract'].notna()] # Keep nonempty abstracts\ndata.head()\n\n\n\n\n\n\n\n\nAuthors\nAuthor full names\nAuthor(s) ID\nTitle\nYear\nSource title\nVolume\nIssue\nArt. No.\nPage start\n...\nISSN\nISBN\nCODEN\nPubMed ID\nLanguage of Original Document\nDocument Type\nPublication Stage\nOpen Access\nSource\nEID\n\n\n\n\n0\nAl Asady, A.; Anokhin, S.\nAl Asady, Ahmad (57219984746); Anokhin, Sergey...\n57219984746; 24482882200\nThe Trojan horse of international entrepreneur...\n2025\nJournal of World Business\n60\n6\n101677.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nNaN\nScopus\n2-s2.0-105014957115\n\n\n1\nThams, Y.; Dau, L.A.; Doh, J.; Kostova, T.; Ne...\nThams, Yannick (55357149800); Dau, Luis Alfons...\n55357149800; 35147597100; 7003920280; 66037741...\nPolitical ideology and the multinational enter...\n2025\nJournal of World Business\n60\n6\n101678.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nNaN\nScopus\n2-s2.0-105014844629\n\n\n2\nLindner, T.; Puck, J.; Puhr, H.\nLindner, Thomas (57159151000); Puck, Jonas (85...\n57159151000; 8563161700; 57223389639\nArtificial intelligence in international busin...\n2025\nJournal of World Business\n60\n6\n101676.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105014595041\n\n\n3\nBruton, G.D.; Mejía-Morelos, J.H.; Ahlstrom, D.\nBruton, Garry D. (6603867202); Mejía-Morelos, ...\n6603867202; 55748855800; 56525447800\nMultinational corporations and inclusive suppl...\n2025\nJournal of World Business\n60\n6\n101663.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013512235\n\n\n4\nLiang, Y.; Giroud, A.; Rygh, A.; Chen, Z.\nLiang, Yanze (57223851564); Giroud, Axèle L.A....\n57223851564; 7003496253; 37117826800; 58631386600\nPolitical embeddedness and post-acquisition in...\n2025\nJournal of World Business\n60\n6\n101665.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013485759\n\n\n\n\n5 rows × 41 columns",
    "crumbs": [
      "Methods",
      "Sentiment Analysis"
    ]
  },
  {
    "objectID": "methods/02-sentiment-analysis.html#data",
    "href": "methods/02-sentiment-analysis.html#data",
    "title": "Sentiment Analysis",
    "section": "",
    "text": "This is a snapshot of the data (JWB article data 1967–2025 downloaded from Scopus) we will be working with.\n\nimport pandas as pd\ndata = pd.read_csv('../data/jwb-articles.csv')\ndata = data[data['Abstract'].notna()] # Keep nonempty abstracts\ndata.head()\n\n\n\n\n\n\n\n\nAuthors\nAuthor full names\nAuthor(s) ID\nTitle\nYear\nSource title\nVolume\nIssue\nArt. No.\nPage start\n...\nISSN\nISBN\nCODEN\nPubMed ID\nLanguage of Original Document\nDocument Type\nPublication Stage\nOpen Access\nSource\nEID\n\n\n\n\n0\nAl Asady, A.; Anokhin, S.\nAl Asady, Ahmad (57219984746); Anokhin, Sergey...\n57219984746; 24482882200\nThe Trojan horse of international entrepreneur...\n2025\nJournal of World Business\n60\n6\n101677.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nNaN\nScopus\n2-s2.0-105014957115\n\n\n1\nThams, Y.; Dau, L.A.; Doh, J.; Kostova, T.; Ne...\nThams, Yannick (55357149800); Dau, Luis Alfons...\n55357149800; 35147597100; 7003920280; 66037741...\nPolitical ideology and the multinational enter...\n2025\nJournal of World Business\n60\n6\n101678.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nNaN\nScopus\n2-s2.0-105014844629\n\n\n2\nLindner, T.; Puck, J.; Puhr, H.\nLindner, Thomas (57159151000); Puck, Jonas (85...\n57159151000; 8563161700; 57223389639\nArtificial intelligence in international busin...\n2025\nJournal of World Business\n60\n6\n101676.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105014595041\n\n\n3\nBruton, G.D.; Mejía-Morelos, J.H.; Ahlstrom, D.\nBruton, Garry D. (6603867202); Mejía-Morelos, ...\n6603867202; 55748855800; 56525447800\nMultinational corporations and inclusive suppl...\n2025\nJournal of World Business\n60\n6\n101663.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013512235\n\n\n4\nLiang, Y.; Giroud, A.; Rygh, A.; Chen, Z.\nLiang, Yanze (57223851564); Giroud, Axèle L.A....\n57223851564; 7003496253; 37117826800; 58631386600\nPolitical embeddedness and post-acquisition in...\n2025\nJournal of World Business\n60\n6\n101665.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013485759\n\n\n\n\n5 rows × 41 columns",
    "crumbs": [
      "Methods",
      "Sentiment Analysis"
    ]
  },
  {
    "objectID": "methods/02-sentiment-analysis.html#bertsentiment",
    "href": "methods/02-sentiment-analysis.html#bertsentiment",
    "title": "Sentiment Analysis",
    "section": "BERTsentiment",
    "text": "BERTsentiment\nWe only train the model on the first 200 as there is a 512 token limitation on the input length for the default BERT model. This may not apply to other models. We then convert the Pandas DataFrame to a Hugging Face Dataset.\n\nfrom datasets import Dataset\ndata['Abstract_200'] = data['Abstract'].apply(lambda x: ' '.join(x.split(' ')[:200]))\ndataset = Dataset.from_pandas(data)\n\nWe now fit the model on the abstracts. We apply the FinBERT model as an illustration. This model was pre-trained on financial communication text.\n\nfrom transformers import pipeline\n\nfinbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\ntokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\nnlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)\n\ndef get_sentiment(examples):\n    # Initialize lists to store results\n    sentiments = []\n    scores = []\n\n    # Process each entry in the batch\n    for text in examples['Abstract_200']:\n        try:\n            # Get the sentiment and sentiment score for each article\n            result = nlp(text)\n            sentiment = result[0]['label']\n            score = result[0]['score']\n\n            sentiments.append(sentiment)\n            scores.append(score)\n        except Exception as e:\n            print(f'Error processing text: {text}. Error: {e}', flush=True)\n            # Append default values in case of an error\n            sentiments.append(None)\n            scores.append(None)\n\n    # Ensure the output lists are of the same length as the batch size\n    batch_size = len(examples['Abstract_200'])\n    while len(sentiments) &lt; batch_size:\n        sentiments.append(None)\n        scores.append(None)\n\n    return {'sentiment': sentiments, 'score': scores}\n\n# Run the sentiment analysis in batches\ndataset = dataset.map(get_sentiment, batched=True, batch_size=64)\n\nWe can examine the results for the first 15 articles.\n\ndf = pd.DataFrame(dataset)\ndf[['Abstract_200', 'sentiment', 'score']].head(15)\n\n\n\n\n\n\n\n\nAbstract_200\nsentiment\nscore\n\n\n\n\n0\nThis study explores the under-theorized relati...\nNeutral\n0.993450\n\n\n1\nWhile politics and political issues such as ri...\nNeutral\n0.999964\n\n\n2\nThis paper discusses the impact of artificial ...\nNeutral\n0.999968\n\n\n3\nAn institutional logic represents the way a pa...\nNeutral\n0.993483\n\n\n4\nPolitical embeddedness has been shown to influ...\nPositive\n0.577034\n\n\n5\nHow do MNE subsidiaries respond and perform af...\nNegative\n0.999626\n\n\n6\nThe ever-increasing internationalization and g...\nNeutral\n0.999457\n\n\n7\nWhile the impact of digital platforms on firms...\nPositive\n0.992149\n\n\n8\nFemale entrepreneurs in emerging economies enc...\nNeutral\n0.999127\n\n\n9\nThis study centers upon Vietnam's Law on Inves...\nPositive\n0.999999\n\n\n10\nBinational decoupling—especially between the U...\nNeutral\n0.922705\n\n\n11\nDrawing primarily upon institutional theory, w...\nNeutral\n0.999989\n\n\n12\nThis paper explores the factors that influence...\nNeutral\n0.999493\n\n\n13\nThe recent acceleration in the international e...\nNeutral\n0.962983\n\n\n14\nAchieving lateral collaboration benefits acros...\nNeutral\n0.967192",
    "crumbs": [
      "Methods",
      "Sentiment Analysis"
    ]
  },
  {
    "objectID": "models/BERTopic.html",
    "href": "models/BERTopic.html",
    "title": "Machine Learning Perspective Paper",
    "section": "",
    "text": "import pandas as pd\ndata = pd.read_csv('../data/jwb-articles.csv')\ndata = data[data['Abstract'].notna()] # Keep nonempty abstracts\ndata.head()\n\n\n\n\n\n\n\n\nAuthors\nAuthor full names\nAuthor(s) ID\nTitle\nYear\nSource title\nVolume\nIssue\nArt. No.\nPage start\n...\nISSN\nISBN\nCODEN\nPubMed ID\nLanguage of Original Document\nDocument Type\nPublication Stage\nOpen Access\nSource\nEID\n\n\n\n\n0\nAl Asady, A.; Anokhin, S.\nAl Asady, Ahmad (57219984746); Anokhin, Sergey...\n57219984746; 24482882200\nThe Trojan horse of international entrepreneur...\n2025\nJournal of World Business\n60\n6\n101677.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nNaN\nScopus\n2-s2.0-105014957115\n\n\n1\nThams, Y.; Dau, L.A.; Doh, J.; Kostova, T.; Ne...\nThams, Yannick (55357149800); Dau, Luis Alfons...\n55357149800; 35147597100; 7003920280; 66037741...\nPolitical ideology and the multinational enter...\n2025\nJournal of World Business\n60\n6\n101678.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nNaN\nScopus\n2-s2.0-105014844629\n\n\n2\nLindner, T.; Puck, J.; Puhr, H.\nLindner, Thomas (57159151000); Puck, Jonas (85...\n57159151000; 8563161700; 57223389639\nArtificial intelligence in international busin...\n2025\nJournal of World Business\n60\n6\n101676.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nShort survey\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105014595041\n\n\n3\nBruton, G.D.; Mejía-Morelos, J.H.; Ahlstrom, D.\nBruton, Garry D. (6603867202); Mejía-Morelos, ...\n6603867202; 55748855800; 56525447800\nMultinational corporations and inclusive suppl...\n2025\nJournal of World Business\n60\n6\n101663.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013512235\n\n\n4\nLiang, Y.; Giroud, A.; Rygh, A.; Chen, Z.\nLiang, Yanze (57223851564); Giroud, Axèle L.A....\n57223851564; 7003496253; 37117826800; 58631386600\nPolitical embeddedness and post-acquisition in...\n2025\nJournal of World Business\n60\n6\n101665.0\nNaN\n...\n10909516\nNaN\nNaN\nNaN\nEnglish\nArticle\nFinal\nAll Open Access; Hybrid Gold Open Access\nScopus\n2-s2.0-105013485759\n\n\n\n\n5 rows × 41 columns\n\n\n\n\nfrom bertopic import BERTopic\n\ntopic_model = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True)\ntopics, probs = topic_model.fit_transform(data['Abstract'].astype(str))\n\n2025-10-23 17:11:53,360 - BERTopic - Embedding - Transforming documents to embeddings.\n\n\n\n\n\n2025-10-23 17:12:00,134 - BERTopic - Embedding - Completed ✓\n2025-10-23 17:12:00,134 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n2025-10-23 17:12:01,178 - BERTopic - Dimensionality - Completed ✓\n2025-10-23 17:12:01,178 - BERTopic - Cluster - Start clustering the reduced embeddings\n2025-10-23 17:12:01,241 - BERTopic - Cluster - Completed ✓\n2025-10-23 17:12:01,247 - BERTopic - Representation - Fine-tuning topics using representation models.\n2025-10-23 17:12:01,341 - BERTopic - Representation - Completed ✓\n\n\n\nimport pickle \n\n# with open('BERTopic.pkl', 'wb') as f:\n#     pickle.dump(topic_model, f)\n\n\ntopic_model.probabilities_\n\narray([[3.23082954e-003, 2.91713947e-003, 2.50626535e-003, ...,\n        3.58725931e-003, 6.39792817e-003, 5.32129467e-003],\n       [1.18520916e-002, 1.41792954e-002, 8.30114966e-003, ...,\n        1.46853968e-002, 5.49580148e-001, 1.15432380e-002],\n       [3.72792191e-003, 4.03285592e-003, 2.30451978e-003, ...,\n        4.67870971e-003, 5.85715197e-003, 1.79971795e-003],\n       ...,\n       [5.10759927e-308, 8.63793925e-308, 3.58580602e-308, ...,\n        7.47138405e-308, 1.12136664e-307, 4.51913763e-308],\n       [5.00693961e-308, 8.18526070e-308, 3.54750880e-308, ...,\n        7.20819145e-308, 1.08634144e-307, 4.50757083e-308],\n       [1.29076806e-002, 2.03127312e-002, 9.26260920e-003, ...,\n        1.85100705e-002, 3.01571833e-002, 1.22287019e-002]])\n\n\n\nprobs\n\narray([[3.23082954e-003, 2.91713947e-003, 2.50626535e-003, ...,\n        3.58725931e-003, 6.39792817e-003, 5.32129467e-003],\n       [1.18520916e-002, 1.41792954e-002, 8.30114966e-003, ...,\n        1.46853968e-002, 5.49580148e-001, 1.15432380e-002],\n       [3.72792191e-003, 4.03285592e-003, 2.30451978e-003, ...,\n        4.67870971e-003, 5.85715197e-003, 1.79971795e-003],\n       ...,\n       [5.10759927e-308, 8.63793925e-308, 3.58580602e-308, ...,\n        7.47138405e-308, 1.12136664e-307, 4.51913763e-308],\n       [5.00693961e-308, 8.18526070e-308, 3.54750880e-308, ...,\n        7.20819145e-308, 1.08634144e-307, 4.50757083e-308],\n       [1.29076806e-002, 2.03127312e-002, 9.26260920e-003, ...,\n        1.85100705e-002, 3.01571833e-002, 1.22287019e-002]])\n\n\n\ntopics\n\n[-1,\n 30,\n -1,\n -1,\n 24,\n -1,\n 1,\n 5,\n -1,\n -1,\n -1,\n -1,\n -1,\n 5,\n 0,\n 8,\n -1,\n 5,\n 9,\n 10,\n 9,\n 3,\n -1,\n 30,\n -1,\n -1,\n -1,\n -1,\n 19,\n -1,\n -1,\n 8,\n 8,\n -1,\n -1,\n -1,\n -1,\n 9,\n 1,\n 9,\n 30,\n 5,\n -1,\n 29,\n 12,\n -1,\n -1,\n -1,\n 1,\n -1,\n 8,\n 3,\n 1,\n 17,\n -1,\n 10,\n 4,\n 10,\n -1,\n 1,\n 1,\n 18,\n -1,\n 19,\n -1,\n 11,\n 3,\n 8,\n 0,\n 18,\n 18,\n 18,\n 30,\n 17,\n -1,\n -1,\n 18,\n 2,\n 9,\n 8,\n 30,\n 9,\n 18,\n 8,\n -1,\n 0,\n -1,\n 9,\n 5,\n 5,\n 28,\n 8,\n -1,\n -1,\n 2,\n -1,\n 19,\n -1,\n -1,\n 0,\n 18,\n -1,\n -1,\n -1,\n -1,\n 23,\n 8,\n 17,\n 18,\n -1,\n -1,\n -1,\n 9,\n -1,\n 9,\n 1,\n 30,\n 5,\n -1,\n 5,\n 28,\n -1,\n 2,\n 8,\n -1,\n -1,\n 8,\n 8,\n 9,\n -1,\n -1,\n -1,\n 12,\n -1,\n -1,\n 16,\n 8,\n -1,\n 24,\n -1,\n 6,\n -1,\n 9,\n 22,\n 10,\n -1,\n 10,\n 6,\n 9,\n -1,\n -1,\n 3,\n -1,\n -1,\n -1,\n 9,\n -1,\n -1,\n 3,\n -1,\n 10,\n 0,\n 0,\n 0,\n 23,\n 0,\n 1,\n -1,\n 24,\n -1,\n -1,\n 27,\n 4,\n 3,\n -1,\n -1,\n 27,\n 24,\n -1,\n 5,\n 9,\n -1,\n -1,\n 0,\n 1,\n -1,\n 18,\n 8,\n -1,\n 1,\n 28,\n -1,\n 9,\n 13,\n -1,\n -1,\n -1,\n 5,\n -1,\n 21,\n 17,\n 1,\n 18,\n 27,\n 1,\n 8,\n 9,\n 1,\n -1,\n -1,\n 0,\n 9,\n 2,\n -1,\n -1,\n 1,\n -1,\n -1,\n -1,\n -1,\n 24,\n 14,\n -1,\n -1,\n 5,\n 23,\n 6,\n 5,\n 1,\n -1,\n -1,\n 3,\n -1,\n -1,\n -1,\n 27,\n 17,\n -1,\n -1,\n -1,\n -1,\n 1,\n -1,\n 18,\n -1,\n -1,\n 1,\n 10,\n 10,\n -1,\n -1,\n -1,\n -1,\n 10,\n 9,\n 8,\n 1,\n 17,\n 23,\n 17,\n 29,\n -1,\n -1,\n 1,\n 5,\n -1,\n 5,\n -1,\n -1,\n -1,\n -1,\n -1,\n 13,\n 19,\n 17,\n -1,\n -1,\n -1,\n -1,\n -1,\n 29,\n 30,\n -1,\n 18,\n 1,\n 10,\n -1,\n 5,\n 27,\n 3,\n 9,\n 27,\n -1,\n 10,\n 9,\n -1,\n -1,\n 0,\n 0,\n -1,\n -1,\n -1,\n 29,\n 9,\n -1,\n -1,\n -1,\n 16,\n 5,\n 16,\n 1,\n 1,\n 27,\n -1,\n 1,\n 20,\n 3,\n -1,\n -1,\n 23,\n 26,\n 27,\n -1,\n 3,\n 8,\n -1,\n -1,\n 10,\n 20,\n -1,\n 11,\n 19,\n 11,\n 11,\n -1,\n -1,\n -1,\n 3,\n 17,\n 0,\n 10,\n 0,\n 6,\n 23,\n 3,\n 5,\n -1,\n 11,\n 12,\n -1,\n -1,\n 0,\n -1,\n 5,\n 28,\n -1,\n -1,\n -1,\n -1,\n -1,\n 16,\n -1,\n 13,\n 0,\n -1,\n 8,\n -1,\n 10,\n 26,\n -1,\n 5,\n 0,\n 3,\n -1,\n 10,\n 10,\n 20,\n 11,\n 10,\n -1,\n 10,\n -1,\n 6,\n 0,\n 24,\n 26,\n -1,\n 3,\n 11,\n 26,\n -1,\n 4,\n 0,\n 0,\n 8,\n -1,\n -1,\n 8,\n -1,\n 23,\n -1,\n -1,\n -1,\n 3,\n -1,\n 13,\n 18,\n -1,\n -1,\n 19,\n 8,\n 6,\n 27,\n -1,\n -1,\n -1,\n 11,\n 10,\n 5,\n -1,\n -1,\n -1,\n -1,\n -1,\n -1,\n 6,\n 8,\n 21,\n -1,\n -1,\n -1,\n 11,\n 24,\n -1,\n -1,\n -1,\n -1,\n 30,\n 8,\n 4,\n 5,\n 5,\n 2,\n 5,\n 11,\n -1,\n -1,\n -1,\n -1,\n -1,\n 4,\n -1,\n -1,\n 30,\n -1,\n -1,\n 18,\n -1,\n 0,\n -1,\n 0,\n 20,\n 17,\n 6,\n 1,\n 17,\n -1,\n 27,\n 6,\n 3,\n -1,\n -1,\n 11,\n 25,\n 2,\n -1,\n 17,\n 16,\n 24,\n 20,\n -1,\n -1,\n -1,\n -1,\n -1,\n -1,\n 11,\n -1,\n 11,\n 11,\n -1,\n 5,\n 10,\n -1,\n 0,\n -1,\n -1,\n 24,\n 10,\n -1,\n -1,\n -1,\n 25,\n -1,\n -1,\n 24,\n 11,\n 5,\n -1,\n 0,\n -1,\n 10,\n -1,\n 18,\n 4,\n 20,\n 8,\n 18,\n -1,\n -1,\n -1,\n 2,\n -1,\n -1,\n 10,\n -1,\n 3,\n 5,\n 1,\n -1,\n 16,\n 7,\n 12,\n -1,\n 3,\n -1,\n 6,\n 27,\n 23,\n 24,\n 11,\n 16,\n -1,\n 10,\n -1,\n 8,\n 12,\n 8,\n 2,\n 8,\n -1,\n 4,\n 11,\n 8,\n -1,\n 8,\n 1,\n 16,\n -1,\n 16,\n -1,\n -1,\n 9,\n -1,\n 11,\n 0,\n 3,\n -1,\n -1,\n -1,\n -1,\n 1,\n 3,\n 1,\n 16,\n -1,\n -1,\n 13,\n 0,\n -1,\n 6,\n 1,\n -1,\n 20,\n 24,\n 24,\n 29,\n -1,\n -1,\n -1,\n 1,\n 30,\n -1,\n 0,\n -1,\n 4,\n 16,\n 20,\n 28,\n 0,\n 19,\n -1,\n -1,\n -1,\n 2,\n -1,\n 13,\n -1,\n -1,\n 13,\n -1,\n 30,\n 25,\n 0,\n -1,\n -1,\n 0,\n 2,\n 30,\n 0,\n 26,\n -1,\n 17,\n 4,\n -1,\n 11,\n -1,\n 17,\n 20,\n 10,\n 8,\n 20,\n -1,\n 3,\n 19,\n 5,\n -1,\n 20,\n -1,\n -1,\n 13,\n -1,\n 2,\n 3,\n -1,\n 0,\n 7,\n 12,\n 12,\n 12,\n 12,\n 12,\n 12,\n 12,\n -1,\n 12,\n 12,\n 12,\n 12,\n 2,\n 3,\n -1,\n 6,\n 11,\n -1,\n -1,\n 29,\n -1,\n 29,\n 29,\n 9,\n -1,\n 0,\n 29,\n 26,\n -1,\n 0,\n -1,\n -1,\n 5,\n 10,\n -1,\n 10,\n -1,\n 0,\n -1,\n -1,\n 1,\n -1,\n -1,\n 6,\n 1,\n 19,\n 0,\n 16,\n -1,\n -1,\n -1,\n -1,\n 18,\n 13,\n -1,\n 16,\n 13,\n 4,\n 18,\n -1,\n -1,\n 20,\n 13,\n 22,\n -1,\n 3,\n 7,\n 2,\n 7,\n 7,\n 2,\n 7,\n 2,\n 7,\n 12,\n 5,\n 0,\n 17,\n 3,\n 6,\n 4,\n 5,\n -1,\n -1,\n 0,\n 9,\n -1,\n -1,\n 3,\n -1,\n -1,\n 2,\n -1,\n 4,\n 6,\n 4,\n 2,\n 4,\n 4,\n 4,\n 4,\n 4,\n 4,\n 4,\n 4,\n 4,\n -1,\n 1,\n 4,\n 16,\n 4,\n 4,\n 26,\n 0,\n 11,\n 18,\n -1,\n -1,\n 23,\n 0,\n -1,\n -1,\n 3,\n -1,\n -1,\n -1,\n 7,\n -1,\n -1,\n 7,\n -1,\n 20,\n -1,\n -1,\n 1,\n 28,\n -1,\n 25,\n 7,\n 25,\n 26,\n -1,\n -1,\n 0,\n 6,\n 16,\n 3,\n 7,\n -1,\n 20,\n -1,\n -1,\n 22,\n -1,\n 30,\n 16,\n -1,\n 16,\n 26,\n -1,\n -1,\n 12,\n 6,\n 6,\n 9,\n -1,\n -1,\n -1,\n 27,\n 9,\n -1,\n 17,\n 17,\n -1,\n 1,\n 1,\n 17,\n -1,\n -1,\n 17,\n 17,\n -1,\n 17,\n 20,\n 19,\n 22,\n 6,\n 0,\n -1,\n 4,\n -1,\n -1,\n -1,\n 0,\n 9,\n 16,\n -1,\n 7,\n 8,\n 2,\n 29,\n -1,\n -1,\n 4,\n 12,\n 14,\n 0,\n 23,\n 7,\n -1,\n 6,\n 13,\n -1,\n 13,\n -1,\n 13,\n 13,\n 13,\n 13,\n 13,\n -1,\n -1,\n -1,\n 4,\n 5,\n 19,\n 1,\n 19,\n 5,\n -1,\n -1,\n 26,\n 13,\n -1,\n 12,\n 12,\n 12,\n 12,\n 12,\n 12,\n 12,\n -1,\n 12,\n 11,\n 7,\n 13,\n 22,\n 15,\n 6,\n 0,\n 2,\n 10,\n -1,\n -1,\n 0,\n -1,\n -1,\n -1,\n -1,\n -1,\n -1,\n 21,\n 4,\n -1,\n 24,\n 20,\n -1,\n 0,\n -1,\n 20,\n -1,\n 25,\n -1,\n -1,\n 26,\n -1,\n -1,\n -1,\n 19,\n -1,\n -1,\n 0,\n 19,\n -1,\n 25,\n 0,\n 20,\n 6,\n -1,\n 31,\n -1,\n 3,\n 7,\n -1,\n -1,\n -1,\n 18,\n -1,\n 4,\n 29,\n 19,\n -1,\n -1,\n 6,\n -1,\n -1,\n -1,\n 21,\n 1,\n 3,\n 1,\n 20,\n 1,\n 1,\n -1,\n 13,\n 1,\n -1,\n 23,\n 11,\n 2,\n 25,\n -1,\n -1,\n 9,\n 2,\n -1,\n 5,\n -1,\n 22,\n 28,\n 0,\n -1,\n 0,\n -1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n -1,\n 2,\n -1,\n -1,\n 25,\n -1,\n 25,\n -1,\n -1,\n 2,\n ...]\n\n\n\nfreq = topic_model.get_topic_info(); freq.head(5)\n\n\n\n\n\n\n\n\nTopic\nCount\nName\nRepresentation\nRepresentative_Docs\n\n\n\n\n0\n-1\n544\n-1_the_of_and_in\n[the, of, and, in, to, that, we, on, this, with]\n[In recent years, there has been an increasing...\n\n\n1\n0\n61\n0_knowledge_transfer_and_the\n[knowledge, transfer, and, the, of, on, to, th...\n[This paper proposes a conceptual framework de...\n\n\n2\n1\n52\n1_international_entrepreneurial_internationali...\n[international, entrepreneurial, international...\n[Grounded in the resource-based view of the fi...\n\n\n3\n2\n43\n2_career_expatriates_expatriate_assignments\n[career, expatriates, expatriate, assignments,...\n[Creating organizational processes which nurtu...\n\n\n4\n3\n38\n3_acquisitions_acquisition_crossborder_acquirers\n[acquisitions, acquisition, crossborder, acqui...\n[This study develops and tests a framework abo..."
  }
]